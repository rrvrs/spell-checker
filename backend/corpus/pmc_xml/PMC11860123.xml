<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006300</article-id><article-id pub-id-type="pmc">PMC11860123</article-id><article-id pub-id-type="doi">10.3390/s25041072</article-id><article-id pub-id-type="publisher-id">sensors-25-01072</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Few-Shot Segmentation of 3D Point Clouds Under Real-World Distributional Shifts in Railroad Infrastructure</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4139-4689</contrib-id><name><surname>Fayjie</surname><given-names>Abdur R.</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="c1-sensors-25-01072" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-4798-3555</contrib-id><name><surname>Lens</surname><given-names>Mathijs</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7106-8024</contrib-id><name><surname>Vandewalle</surname><given-names>Patrick</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Sun</surname><given-names>Guangcai</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01072">EAVISE, ESAT-PSI, KU Leuven, 3000 Leuven, Belgium; <email>mathijs.lens@kuleuven.be</email> (M.L.); <email>patrick.vandewalle@kuleuven.be</email> (P.V.)</aff><author-notes><corresp id="c1-sensors-25-01072"><label>*</label>Correspondence: <email>abdurrazzaq.fayjie@kuleuven.be</email></corresp></author-notes><pub-date pub-type="epub"><day>11</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1072</elocation-id><history><date date-type="received"><day>22</day><month>11</month><year>2024</year></date><date date-type="rev-recd"><day>27</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>06</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Industrial railway monitoring systems require precise understanding of 3D scenes, typically achieved using deep learning models for 3D point cloud segmentation. However, real-world applications demand these models to rapidly adapt to infrastructure upgrades and diverse environmental conditions across regions. Conventional deep learning models, which rely on large-scale annotated datasets for training and are evaluated on test sets that are drawn independently and identically from the training distribution, often fail to account for such real-world changes, leading to overestimated model performance. Recent advancements in few-shot learning, which aim to develop generalizable models with minimal annotations, have shown promise. Motivated by this potential, the paper investigates the application of few-shot learning to railway monitoring by formalizing three types of distributional shifts that are commonly encountered in such systems: (a) in-domain shifts caused by sensor noise, (b) in-domain out-of-distribution shifts arising from infrastructure changes, and (c) cross-domain out-of-distribution shifts driven by geographical variations. A systematic evaluation of few-shot learning&#x02019;s adaptability to these shifts is conducted using three performance metrics and a predictive uncertainty estimation metric. Extensive experimentation demonstrates that few-shot learning outperforms fine-tuning and maintains strong generalization under in-domain shifts with only ~1% performance deviation. However, it experiences a significant drop in performance under both in-domain and cross-domain out-of-distribution shifts, pronounced when dealing with previously unseen infrastructure classes. Additionally, we show that incorporating predictive uncertainty estimation enhances few-shot learning applicability by quantifying the model&#x02019;s sensitivity to distributional shifts, offering valuable insights into the model&#x02019;s reliability for safety-critical applications.</p></abstract><kwd-group><kwd>railway monitoring</kwd><kwd>distributional shifts</kwd><kwd>few-shot learning</kwd><kwd>point cloud segmentation</kwd></kwd-group><funding-group><award-group><funding-source>VLAIO</funding-source></award-group><award-group><funding-source>KU Leuven Internal Fund</funding-source></award-group><funding-statement>This research was funded by VLAIO and KU Leuven Internal Fund.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01072"><title>1. Introduction</title><p>Railways are among the busiest modes of global transportation and are essential for efficiently moving people, freight, and&#x000a0;containers across regions. They frequently require rapid and consistent maintenance to ensure uninterrupted operations, resulting in substantial maintenance costs, time investment, and&#x000a0;reliance on manual labor&#x000a0;[<xref rid="B1-sensors-25-01072" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01072" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01072" ref-type="bibr">3</xref>]. Automated monitoring systems can address these challenges by offering a viable and rapid solution while also reducing the reliance on manual labor and the associated costs. Such systems necessitate an accurate understanding of 3D scenes, which is typically achieved by utilizing deep learning models for point cloud segmentation&#x000a0;[<xref rid="B4-sensors-25-01072" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01072" ref-type="bibr">5</xref>]&#x02014;a task that involves classifying each point into its corresponding category, thereby dividing the point cloud into semantically meaningful regions. Most of these models are constrained by the requirement of large-scale annotated data for training. Creating such annotations for railway point cloud data is particularly challenging as&#x000a0;it involves cumbersome manual labor and is time-consuming, expensive, and&#x000a0;error-prone due to the intricate geometrical structures of the objects involved. Furthermore, their performance is typically evaluated on a test set that is identically and independently distributed (i.i.d.) relative to the training set, often causing overestimation of their actual effectiveness.</p><p>The i.i.d. assumption is often violated in real-world applications, potentially leading to catastrophic outcomes in safety-critical systems such as railway monitoring, where accuracy in decision-making is of paramount importance. Gawlikowski et al. [<xref rid="B6-sensors-25-01072" ref-type="bibr">6</xref>] attributed the failure of deep learning models in real-world deployment for safety-critical systems to their inability to distinguish between in-domain (ID) and out-of-distribution (OOD) samples, their sensitivity to distributional shifts, and&#x000a0;their lack of reliable uncertainty estimation. Railway monitoring presents a complex and dynamic landscape, often resulting in distributional shifts. For&#x000a0;example, maintenance activities can introduce new types of infrastructure as part of an upgrade process, leading to the emergence of novel classes not encountered during the model&#x02019;s training phase. Additionally, timely detection and removal of vegetation are crucial for ensuring smooth operations, but&#x000a0;vegetation characteristics vary significantly across regions. Furthermore, railway environments differ substantially based on geographical location, encompassing variations in infrastructure types, weather conditions, and&#x000a0;environmental settings, such as rural versus urban areas. These scenarios, characterized by significant variations between training and test conditions&#x000a0;[<xref rid="B7-sensors-25-01072" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01072" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-01072" ref-type="bibr">9</xref>], are referred to as OOD scenarios. A&#x000a0;model designed for railway monitoring is expected to generalize effectively under such OOD conditions. However, in&#x000a0;practice, this often results in unreliable predictions&#x000a0;[<xref rid="B6-sensors-25-01072" ref-type="bibr">6</xref>,<xref rid="B10-sensors-25-01072" ref-type="bibr">10</xref>]. Consequently, assessing a model&#x02019;s generalization capability&#x02014;both in terms of performance and uncertainty in its decisions&#x02014;becomes a cornerstone for model selection and their reliable deployment.</p><p>The scarcity of large-scale point-wise annotations for 3D point clouds and the critical need for effective generalization motivate this paper exploring few-shot learning (FSL)&#x000a0;[<xref rid="B11-sensors-25-01072" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01072" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01072" ref-type="bibr">13</xref>]&#x02014;a relatively new deep learning paradigm that aims to develop models that are capable of generalizing to unseen novel classes using only a minimal amount of labeled data. In&#x000a0;recent years, FSL has demonstrated significant potential in the image and text domains&#x000a0;[<xref rid="B14-sensors-25-01072" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01072" ref-type="bibr">15</xref>]. However, its application to 3D point clouds remains relatively limited. Although&#x000a0;some works have explored FSL for point cloud segmentation in indoor environments&#x000a0;[<xref rid="B16-sensors-25-01072" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01072" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01072" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-01072" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01072" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-01072" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01072" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-01072" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-01072" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-01072" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-01072" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-01072" ref-type="bibr">27</xref>], its use in railway environments is still largely under-explored. In contrast to controlled indoor environments with generally dense point clouds, railway environments present far more challenging scenarios: inherent environmental and sensor noises, unintentional occlusions, variations in lighting conditions, and&#x000a0;significant variations in geometric shapes within the same class (e.g., vegetation). Long-range LiDAR scanners used for railway monitoring often collect data over extended distances, resulting in sparse points in many regions within the point cloud compared to the shorter-range detailed high-resolution scanning in indoor environments. Additionally, the&#x000a0;railway environment has fewer features, but&#x000a0;these features are much larger and more spread out compared to the smaller, more numerous objects found indoors.</p><p>This paper investigates the generalization capability of FSL under real-world distributional shifts encountered in railway monitoring systems, contributing to a deeper understanding of model effectiveness in decision-making for safety-critical applications. To&#x000a0;achieve this, we formalize three types of distributional shifts: (a) ID shift, (b) in-domain OOD shift, and&#x000a0;(c) cross-domain OOD shift, further detailed in <xref rid="sec2dot2-sensors-25-01072" ref-type="sec">Section 2.2</xref>. A&#x000a0;systematic evaluation is conducted using three performance metrics, along with a predictive uncertainty estimation metric to assess model sensitivity by quantifying prediction uncertainties. As&#x000a0;demonstrated through experimental validation, this evaluation provides valuable insights into model development, selection, and&#x000a0;deployment for reliable railway monitoring systems. Although&#x000a0;prior studies have separately explored distributional shifts and uncertainty estimation in point cloud segmentation, to&#x000a0;the best of our knowledge, this is the first to comprehensively evaluate few-shot segmentation for railroad monitoring under real-world distributional shifts.</p><p>The structure of the paper is as follows: <xref rid="sec2-sensors-25-01072" ref-type="sec">Section 2</xref> presents a comprehensive review of the related works, providing background, context, and&#x000a0;the problem setup. <xref rid="sec3-sensors-25-01072" ref-type="sec">Section 3</xref> outlines the materials and methods used in the study. In <xref rid="sec4-sensors-25-01072" ref-type="sec">Section 4</xref>, we detail the experimental setup. <xref rid="sec5-sensors-25-01072" ref-type="sec">Section 5</xref> delves into an in-depth analysis of the results, examining the key findings. <xref rid="sec6-sensors-25-01072" ref-type="sec">Section 6</xref> offers a discussion of the implications and relevance of the study&#x02019;s outcomes. In <xref rid="sec8-sensors-25-01072" ref-type="sec">Section 8</xref>, the conclusion is presented.</p></sec><sec id="sec2-sensors-25-01072"><title>2. Background</title><sec id="sec2dot1-sensors-25-01072"><title>2.1. Related&#x000a0;Work</title><p><bold>Three-dimensional point cloud segmentation for railroad infrastructure.</bold> Earlier works focused on developing heuristic-based methods&#x000a0;[<xref rid="B28-sensors-25-01072" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-01072" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-01072" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-01072" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-01072" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-01072" ref-type="bibr">33</xref>] to detect railway infrastructure by exploiting underlying geometrical or morphological patterns in 3D point clouds. &#x000a0;S&#x000e1;nchez-Rodr&#x000ed;guez et al. [<xref rid="B34-sensors-25-01072" ref-type="bibr">34</xref>] integrated heuristics with an SVM classifier&#x02014;a classical machine learning technique to detect rail tracks, cantilever arms, power line wires, and&#x000a0;lining in railway tunnels. These methods are based on simple predefined rules derived from domain knowledge, making them computationally efficient and requiring minimal data. However, they heavily rely on handcrafted features designed by human experts, which restricts their application to simple detection tasks, typically involving regular, well-defined linear or circular objects. Furthermore, their adaptability is limited as&#x000a0;they struggle to handle variations such as environmental changes, outliers, scaling, or&#x000a0;noise.</p><p>The advancements in deep learning for point cloud segmentation prompted Soil&#x000e1;n et al. [<xref rid="B35-sensors-25-01072" ref-type="bibr">35</xref>] to explore two well-known point cloud segmentation networks, PointNet&#x000a0;[<xref rid="B36-sensors-25-01072" ref-type="bibr">36</xref>] and KP-Conv&#x000a0;[<xref rid="B37-sensors-25-01072" ref-type="bibr">37</xref>], for&#x000a0;railway tunnel detection, achieving performance on par with the heuristic-based approach proposed by S&#x000e1;nchez-Rodr&#x000ed;guez et al. [<xref rid="B34-sensors-25-01072" ref-type="bibr">34</xref>]. PointNet, a&#x000a0;pioneering work by Qi et al. [<xref rid="B36-sensors-25-01072" ref-type="bibr">36</xref>], is the first network that directly inputs unstructured point clouds into a neural network, learning per-point features through shared multi-layer perceptrons and pooling functions. KP-Conv introduces a flexible and deformable convolutional operator that applies point convolutions to kernel points and their neighbors in Euclidean space. Chen et al. [<xref rid="B38-sensors-25-01072" ref-type="bibr">38</xref>] applied a multi-scale Hierarchical Conditional Random Field (HiCRF) model to classify electrification assets in railway infrastructure by capturing spatial relationships. FarNet&#x000a0;[<xref rid="B39-sensors-25-01072" ref-type="bibr">39</xref>] introduced an attention module to aggregate spatial attention into feature information, learning from the spherical projection of point clouds. Ton et al. [<xref rid="B40-sensors-25-01072" ref-type="bibr">40</xref>] adopted PointNet++&#x000a0;[<xref rid="B41-sensors-25-01072" ref-type="bibr">41</xref>], SuperPoint Graph&#x000a0;[<xref rid="B42-sensors-25-01072" ref-type="bibr">42</xref>], and&#x000a0;Point Transformer&#x000a0;[<xref rid="B43-sensors-25-01072" ref-type="bibr">43</xref>] for the semantic segmentation of railway infrastructure in catenary arches. PointNet++ extends PointNet by incorporating a grouping function to capture richer feature information, while SuperPoint Graph leverages object contextual relationships to partition point clouds into superpoint graphs, with&#x000a0;each superpoint embedded via PointNet and processed by a graph convolutional network. Point Transformer incorporates self-attention layers for enhanced feature learning. However, it is computationally intensive, as&#x000a0;is SuperPoint Graph. Kharroubi et al. [<xref rid="B44-sensors-25-01072" ref-type="bibr">44</xref>] used KP-Conv, LightGBM, and&#x000a0;Random Forest for semantic segmentation in railway environments. These models, trained in a supervised manner, address the limitations of heuristic-based approaches by automatically extracting meaningful features from data, enabling their application to complex high-dimensional railway segmentation problems, albeit with higher computational complexity and a significant requirement for labeled data. However, as&#x000a0;typically evaluated on a test set drawn i.i.d. from the same distribution as the training data, these models do not account for the real-world variations encountered in railway environments, often resulting in an overestimation of model performance. Additionally, these works overlook uncertainty quantification in the model&#x02019;s predictions, which is crucial for accurate decision-making, especially in identifying highly uncertain samples for further review by human counterparts.</p><p><bold>Distributional shifts and OOD conditions in point clouds.</bold> Distributional shifts in point clouds can be categorized into ID shifts and OOD shifts. ID shifts arise from minor variations in the test set relative to the training set, characterized by positional, rotational, and&#x000a0;scaling disturbances, along with jitter, outliers, sparsity, and&#x000a0;corruptions. TriangleNet&#x000a0;[<xref rid="B45-sensors-25-01072" ref-type="bibr">45</xref>] addresses these shifts in point cloud classification by utilizing arbitrary SO(3) rotations, demonstrating high performance even with sparse point clouds using as few as 16&#x000a0;points sampled for each object. PointASNL&#x000a0;[<xref rid="B46-sensors-25-01072" ref-type="bibr">46</xref>] introduces adaptive sampling and a local&#x02013;nonlocal module to handle inherent noise and outliers in point clouds. RobustPointSet&#x000a0;[<xref rid="B47-sensors-25-01072" ref-type="bibr">47</xref>] proposes a benchmark for evaluating point cloud classifiers on transformations, such as noise, rotation, sparsity, and&#x000a0;translation. DUP-Net&#x000a0;[<xref rid="B48-sensors-25-01072" ref-type="bibr">48</xref>] and PointGuard&#x000a0;[<xref rid="B49-sensors-25-01072" ref-type="bibr">49</xref>] leverage adversarial point addition and deletion attacks by randomly adding and removing points in input data to design models that are robust to ID shifts. Additionally, Dong et al. [<xref rid="B50-sensors-25-01072" ref-type="bibr">50</xref>] leveraged the relative positions of local features, and&#x000a0;&#x000a0;Sun et al. [<xref rid="B51-sensors-25-01072" ref-type="bibr">51</xref>] emphasized learning local features through&#x000a0;self-supervision.</p><p>In contrast to ID shifts, OOD shifts occur when the test data distribution diverges significantly from the training data, often involving previously unseen classes or environments. Veeramacheneni and Valdenegro-Toro [<xref rid="B52-sensors-25-01072" ref-type="bibr">52</xref>] established a benchmark for semantic segmentation utilizing Semantic3D, which consists of outdoor scenes, and&#x000a0;S3DIS, comprising indoor scenes. Bhardwaj et al. [<xref rid="B53-sensors-25-01072" ref-type="bibr">53</xref>] employed knowledge distillation for object classification, while &#x000a0;Riz et al. [<xref rid="B54-sensors-25-01072" ref-type="bibr">54</xref>] applied online clustering and uncertainty quantification to generate prototypes for pseudo-labeling point clouds of previously unseen classes. The research on object detection&#x000a0;[<xref rid="B55-sensors-25-01072" ref-type="bibr">55</xref>,<xref rid="B56-sensors-25-01072" ref-type="bibr">56</xref>,<xref rid="B57-sensors-25-01072" ref-type="bibr">57</xref>], instance segmentation&#x000a0;[<xref rid="B58-sensors-25-01072" ref-type="bibr">58</xref>], and&#x000a0;semantic segmentation&#x000a0;[<xref rid="B57-sensors-25-01072" ref-type="bibr">57</xref>] in outdoor environments for autonomous driving has focused on enhancing robustness against distributional shifts caused by previously unseen and unknown objects. The&#x000a0;3DOS benchmark&#x000a0;[<xref rid="B59-sensors-25-01072" ref-type="bibr">59</xref>] evaluates models, encompassing both in-domain scenarios (synthetic-to-synthetic and real-to-real) and cross-domain scenarios (synthetic-to-real). Research dedicated to OOD shifts using FSL for 3D point clouds remains limited. The&#x000a0;BelHouse3D benchmark&#x000a0;[<xref rid="B60-sensors-25-01072" ref-type="bibr">60</xref>] examines FSL under such shifts in <italic toggle="yes">indoor scene segmentation</italic>, identifying inherent point cloud occlusion that alters the shapes and context of 3D household objects. Unlike prior works, our study focuses on investigating a wide range of shifts that are particularly encountered in railway environments.</p></sec><sec id="sec2dot2-sensors-25-01072"><title>2.2. Notation and Problem Setup</title><p>Assume a point cloud, <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which is an unordered set of <italic toggle="yes">n</italic> points in Euclidean space. Each point <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is a vector representing the <italic toggle="yes">i</italic>-th point in three-dimensional space and corresponds to one of the <italic toggle="yes">l</italic> target labels, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denote a deep neural network with learnable parameters <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula>. We train the model <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> on a labeled training set, <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, to evaluate its generalization capability on an evaluation (test) set, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Conditioned on <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we formulate distributional shifts to represent the types of changes encountered by real-world railway monitoring systems as&#x000a0;follows:<list list-type="order"><list-item><p><bold>ID shift:</bold>&#x000a0;<inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is constructed by sampling <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> point clouds, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:math></inline-formula> drawn i.i.d. from the same distribution as the training set, <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and&#x000a0;then adding random noise, <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow></mml:math></inline-formula>,<disp-formula id="FD1-sensors-25-01072"><label>(1)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow></mml:math></inline-formula> simulates sensor noise as represented by variations in object positions and orientations. We employ three types of noise in our transformations: jitter, mirroring, and&#x000a0;rotation. Jitter adds Gaussian noise to the point cloud <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD2-sensors-25-01072"><label>(2)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and&#x000a0;<inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represent a standard normal distribution sampled for each point in <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> Mirroring randomly reflects the point cloud along the x-axis and y-axis. Rotation rotates the point cloud around the z-axis by an <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> sampled from a uniform distribution, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p><bold>In-domain OOD shift:</bold>&#x000a0;<inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> consists of samples from classes that were not included in the training phase, taking into account a dataset, <italic toggle="yes">D</italic>, which encompasses <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> classes. The&#x000a0;training set, <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, contains samples from <italic toggle="yes">j</italic> classes, referred to as base classes, denoted as <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>l</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> In contrast, <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> includes samples from the remaining <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> classes, designated as novel classes, <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x02216;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> The base and novel classes are contextually distinct and mutually exclusive, ensuring that <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02205;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This situation represents an <italic toggle="yes">OOD shift within the same domain</italic>, characterized by the condition <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02282;</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02260;</mml:mo><mml:mi mathvariant="double-struck">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02282;</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. This shift is indicative of the emergence of new classes during upgrades to railway&#x000a0;infrastructure.</p></list-item><list-item><p><bold>Cross-domain OOD shift:</bold>&#x000a0;<inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is constructed by sampling test samples from a dataset, <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, that is entirely distinct from <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which was utilized to create the training set, <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> The datasets, <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, are sourced from different distributions, resulting in an OOD shift due to domain differences, expressed as <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02260;</mml:mo><mml:mi mathvariant="double-struck">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> For simplicity, our experimental setup assumed that <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> share the same set of classes, <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This shift reflects variations arising from differences in railway environments across diverse geographical regions, such as Asia and Europe, or&#x000a0;among various countries.</p></list-item></list></p><p><xref rid="sensors-25-01072-f001" ref-type="fig">Figure 1</xref> presents an overview of the distributional shifts, as&#x000a0;captured by the datasets used in our experimental&#x000a0;setup.</p></sec></sec><sec id="sec3-sensors-25-01072"><title>3. Materials and&#x000a0;Methods</title><sec id="sec3dot1-sensors-25-01072"><title>3.1. Few-Shot&#x000a0;Learning</title><p>FSL is closely related to the concept of generalization over test tasks, where a learned model is expected to adapt to new tasks, provided with minimal supervision at test time. This concept mimics the learning capabilities inherent in human intelligence. For&#x000a0;instance, a&#x000a0;child can identify a zebra in a zoo after seeing only a few pictures of zebras in a book, even without direct prior exposure to the animal. Theories from cognitive psychology suggest that humans and animals learn by capturing the regularities and commonalities among categories of objects, forming internal representations, defined as <italic toggle="yes">prototypes</italic>&#x000a0;[<xref rid="B61-sensors-25-01072" ref-type="bibr">61</xref>,<xref rid="B62-sensors-25-01072" ref-type="bibr">62</xref>,<xref rid="B63-sensors-25-01072" ref-type="bibr">63</xref>,<xref rid="B64-sensors-25-01072" ref-type="bibr">64</xref>,<xref rid="B65-sensors-25-01072" ref-type="bibr">65</xref>,<xref rid="B66-sensors-25-01072" ref-type="bibr">66</xref>,<xref rid="B67-sensors-25-01072" ref-type="bibr">67</xref>,<xref rid="B68-sensors-25-01072" ref-type="bibr">68</xref>]. These prototypes are then used to categorize an unseen object as belonging to a particular category (or not).</p><p>We define FSL on 3D point cloud segmentation tasks for railway environments under distributional shifts to&#x000a0;study a model&#x02019;s generalization capability at test time. Each task samples <italic toggle="yes">K</italic> labeled examples and <italic toggle="yes">q</italic> test examples from an example space to constitute a pair of support&#x02013;query sets <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <italic toggle="yes">N</italic> chosen classes, known as an <italic toggle="yes">N-way K-shot task</italic>. FSL aims to train the model <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> from <xref rid="sec2dot2-sensors-25-01072" ref-type="sec">Section 2.2</xref> over the distribution of tasks, <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which samples <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> pairs from <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> during the meta-training phase. The&#x000a0;goal is to generalize to a distribution of previously unseen tasks, <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, consisting of <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> pairs sampled from the test domain. Since the problem essentially becomes a transfer learning task under OOD shifts, we assume that minimal supervision is available through a few labeled samples from the test domain, which provide prior knowledge about the domain. These labeled examples are used to construct <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, while <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">Q</mml:mi></mml:mrow></mml:math></inline-formula> comprises the test samples to be evaluated. This <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">Q</mml:mi></mml:mrow></mml:math></inline-formula> is effectively equivalent to <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Under&#x000a0;this setting, we characterized the distributional shifts (see <xref rid="sec2dot2-sensors-25-01072" ref-type="sec">Section 2.2</xref>) as follows:<list list-type="bullet"><list-item><p><bold>ID shift:</bold>&#x000a0;<inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are defined over <italic toggle="yes">C</italic> classes within a dataset <italic toggle="yes">D</italic>. <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is drawn i.i.d. from the same distribution as the <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with the inclusion of random noise. Consequently, <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> share the same underlying distribution, such that <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02248;</mml:mo><mml:mi mathvariant="double-struck">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p><bold>In-domain OOD shift:</bold>&#x000a0;<inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are defined over <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> classes within a dataset <italic toggle="yes">D</italic>, respectively. To&#x000a0;ensure <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02260;</mml:mo><mml:mi mathvariant="double-struck">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are predefined rather than being chosen randomly.</p></list-item><list-item><p><bold>Cross-domain OOD shift:</bold>&#x000a0;<inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are defined for <italic toggle="yes">C</italic> shared classes from two distinct datasets, <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, respectively. The&#x000a0;difference in underlying distributions ensures that <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02260;</mml:mo><mml:mi mathvariant="double-struck">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list></p><p>To incorporate the prototypical view, we employ Prototypical Networks&#x000a0;[<xref rid="B69-sensors-25-01072" ref-type="bibr">69</xref>], a&#x000a0;commonly used metric-based method for FSL. They utilize <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> initialized with a pretrained weight to compute the feature representation <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for each point <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:math></inline-formula>,<disp-formula id="FD3-sensors-25-01072"><label>(3)</label><mml:math id="mm80" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1.em"/><mml:mo>&#x02200;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In an <italic toggle="yes">N</italic>-way <italic toggle="yes">K</italic>-shot task, the&#x000a0;prototype for each foreground class <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is computed as the mean embedding of all points within that class given <italic toggle="yes">K</italic> examples from <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:math></inline-formula>. The&#x000a0;background prototype is calculated as the mean embedding of points that do not belong to any of the <italic toggle="yes">N</italic> classes.<disp-formula id="FD4-sensors-25-01072"><label>(4)</label><mml:math id="mm83" display="block" overflow="scroll"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>&#x02223;</mml:mo><mml:mi>K</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>&#x02223;</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>and</mml:mi><mml:mspace width="1.em"/><mml:msubsup><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>&#x02223;</mml:mo><mml:mi>K</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>&#x02223;</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02209;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Given a query point cloud, <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the&#x000a0;probability for each point <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to belong to a class, <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>} is obtained by applying a softmax over the distances to each class&#x000a0;prototype:<disp-formula id="FD5-sensors-25-01072"><label>(5)</label><mml:math id="mm87" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> represents the total number of classes, including <italic toggle="yes">N</italic> foreground classes and 1 background class, <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the predicted class label for point <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and&#x000a0;<inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mo>&#x0222b;</mml:mo></mml:mrow></mml:math></inline-formula> denotes the similarity/distance function, which in our experiments is implemented as the cosine&#x000a0;distance.</p><p><xref rid="sensors-25-01072-f002" ref-type="fig">Figure 2</xref> presents our pipeline for point cloud segmentation in railway monitoring, illustrating the pretraining process and FSL. We employ DGCNN&#x000a0;[<xref rid="B70-sensors-25-01072" ref-type="bibr">70</xref>], pretrained on a subset of WHU-Railway3D dataset, to&#x000a0;extract support and query features using Equation (3). DGCNN constructs a local neighborhood graph for each point using <italic toggle="yes">k</italic>-Nearest Neighbors (<italic toggle="yes">k</italic>-NN) and applies a convolutional operation to the edges via EdgeConv layers to capture local geometric structures, crucial in railroad segmentation. The network aggregates features for each point by pooling the edge features from each subsequent EdgeConv layer. These aggregated features are then input into multi-layer perceptrons to produce global feature representations at different scales, which are combined to enhance the overall representation. The&#x000a0;graph in DGCNN is dynamic as&#x000a0;it is recomputed at every layer, enabling the network to adjust neighborhood relationships based on the learned features at each stage and&#x000a0;facilitating learning complex and non-linear relationships within point&#x000a0;clouds.</p><p>We conducted experiments using Transformer-based models, specifically Point Transformers v1, to&#x000a0;be used as the pretrained network. Observations indicate that the model tends to overfit on small datasets like those used in our experiments. Furthermore, recent work&#x000a0;[<xref rid="B71-sensors-25-01072" ref-type="bibr">71</xref>] comparing DGCNN with Transformer-based models, particularly Convolutional Point Transformer (CpT), reports closely comparable performance. This finding further reinforces our decision to prioritize the computationally efficient DGCNN architecture.</p></sec><sec id="sec3dot2-sensors-25-01072"><title>3.2. Datasets</title><p><bold>Infrabel-5 Railroad Segmentation dataset.</bold> The Infrabel-5 Railway Segmentation dataset consists of data from 8 railway tracks, each containing 8 million raw points, collected across various cities in Belgium. The&#x000a0;data collection process is administered by Infrabel (a Belgian government-owned public limited company responsible for the installation, upgrading, and&#x000a0;maintenance of railway infrastructure). The&#x000a0;point clouds, represented by <italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, and&#x000a0;<italic toggle="yes">z</italic> coordinates, are captured using a Z+F 9012 LiDAR-based MMS system. The&#x000a0;raw data were shared with the authors under a confidentiality agreement and&#x000a0;were also used in our earlier work&#x000a0;[<xref rid="B72-sensors-25-01072" ref-type="bibr">72</xref>]. The&#x000a0;data were initially filtered by removing outliers using the Statistical Outlier Removal (SOR) filter within the CloudCompare software (available at <ext-link xlink:href="https://www.danielgm.net/cc/" ext-link-type="uri">https://www.danielgm.net/cc/</ext-link>, (accessed on 20 December 2024)), followed by manual annotation using the software&#x02019;s built-in annotation tool. The&#x000a0;filtered dataset contains approximately 4 million points per railway track, classified into 5 object classes: cable, cable holder (support device), pole, vegetation, and&#x000a0;ground. Any remaining points are categorized as clutter. Although&#x000a0;this dataset accurately represents the highly imbalanced class scenarios common in real-world railway environments, it does not include RGB&#x000a0;information.</p><p><bold>WHU-Railway3D dataset.</bold> The WHU-Railway3D dataset&#x000a0;[<xref rid="B73-sensors-25-01072" ref-type="bibr">73</xref>] is a publicly available collection of 3D point clouds, represented by <italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, and&#x000a0;<italic toggle="yes">z</italic> coordinates, capturing railroad infrastructure across urban, rural, and&#x000a0;plateau environments. An&#x000a0;Optech Lynx MMS system captured 675 million points over 10.7 km in urban areas. In&#x000a0;rural environments, a&#x000a0;HiScan-Z MMS system captured 2775 million points across 10.6 km, while a rMMS system collected 362 million points over 10.4 km in plateau areas. The&#x000a0;maximum valid range for urban data is 250 m with an outlier tolerance of 0.2%, and&#x000a0;the average point density is 2000 points/m<sup>2</sup>. The&#x000a0;rural and plateau data have a maximum valid range of 119 m with an outlier tolerance of 0.1%. The&#x000a0;average point density is 9000 points/m<sup>2</sup> in rural areas and 500 points/m<sup>2</sup> in plateau regions. Across all environments, the&#x000a0;point position accuracy is 5 cm. The&#x000a0;dataset spans over 30 km with 3.9 billion points, manually annotated into 10 classes: rail, track bed, mast, support device, overhead line (cable), fence, pole, vegetation, building, and&#x000a0;ground. Notably, the&#x000a0;dataset lacks RGB&#x000a0;information.</p></sec></sec><sec id="sec4-sensors-25-01072"><title>4. Experiments</title><p>Our experiments evaluated the generalization capability of the FSL model under distributional shifts formulated in <xref rid="sec2dot2-sensors-25-01072" ref-type="sec">Section 2.2</xref> for&#x000a0;3D point cloud segmentation in railroads. The experimental setup for distributional shifts, including data splits and class configurations for training and evaluation, is presented in <xref rid="sensors-25-01072-t001" ref-type="table">Table 1</xref>.</p><p>First, we assessed the model under a <italic toggle="yes">no-shift scenario</italic>, which replicates the ID shift scenario without introducing additional noise (<inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow></mml:math></inline-formula>) to the evaluation set (<inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. This scenario serves as the baseline for comparison with distributional shifts. For&#x000a0;ID shifts, the&#x000a0;trained model is evaluated on <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for both datasets, incorporating jitter, mirroring, and&#x000a0;rotation (Equation (1)).</p><p>For the in-domain OOD shift, the&#x000a0;model was evaluated on two class-based configurations: <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="monospace">pole</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="monospace">vegetation</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="monospace">cable</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="monospace">support device</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for both datasets, while the remaining classes were used during the training phase. For&#x000a0;the cross-domain OOD shift, the&#x000a0;model was trained on the WHU-Railway3D dataset and evaluated on the Infrabel-5 Railroad Segmentation dataset and&#x000a0;vice&#x000a0;versa. Although&#x000a0;both datasets share the same classes&#x02014;<monospace>pole</monospace>, <monospace>vegetation</monospace>, <monospace>ground</monospace>, <monospace>cable</monospace>, and&#x000a0;<monospace>support device</monospace>&#x02014;in our experimental setup, they significantly differ in terms of distributions. We do not include the <monospace>clutter</monospace> class from both datasets in FSL&#x000a0;training.</p><sec id="sec4dot1-sensors-25-01072"><title>4.1. Evaluation&#x000a0;Metrics</title><p><bold>Performance metrics.</bold> Three metrics, <italic toggle="yes">mean Intersection over Union (<bold>mIoU</bold>)</italic>, <italic toggle="yes">Overall Accuracy (<bold>OA</bold>)</italic>, and&#x000a0;<italic toggle="yes">Matthews Correlation Coefficient (<bold>MCC</bold>)</italic>, are employed to assess the model&#x02019;s performance. Given the four categories of the confusion matrix&#x02014;true positive (<inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), false positive (<inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), true negative (<inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), and false negative (<inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>)&#x02014;these metrics are defined as&#x000a0;follows:<list list-type="bullet"><list-item><p><bold>mIoU:</bold>&#x000a0;<italic toggle="yes">mIoU</italic> measures how well each class is segmented by measuring the overlap between predicted and ground-truth points for each class.<disp-formula id="FD6-sensors-25-01072"><label>(6)</label><mml:math id="mm101" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Note that we ignore the background class for our&#x000a0;evaluation.</p></list-item><list-item><p><bold>OA:</bold>&#x000a0;<italic toggle="yes">OA</italic> measures the ratio of correctly classified points over the total number of points, regardless of classes, thereby reflecting the overall correctness of the model&#x02019;s predictions.<disp-formula id="FD7-sensors-25-01072"><label>(7)</label><mml:math id="mm102" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p><bold>MCC:</bold> In a class imbalance scenario, the&#x000a0;<italic toggle="yes">OA</italic> can present overly optimistic model performance evaluations. In&#x000a0;contrast, <italic toggle="yes">MCC</italic> provides a more equitable assessment of classification models. It yields a high score only when the predictions are accurate across all four categories of the confusion matrix&#x02014;<inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#x02014;while also considering the relative sizes of both positive and negative classes within the dataset. This makes the <italic toggle="yes">MCC</italic> a robust metric for performance evaluation, particularly in imbalanced datasets.<disp-formula id="FD8-sensors-25-01072"><label>(8)</label><mml:math id="mm107" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This metric ranges from &#x02212;1, indicating total disagreement, to&#x000a0;1, representing perfect prediction.</p></list-item></list></p><p><bold>Uncertainty metric.</bold> Entropy quantifies the model&#x02019;s performance in terms of probabilistic predictions, providing insights into the model&#x02019;s uncertainty in its predictions. Given a point, entropy is defined as,<disp-formula id="FD9-sensors-25-01072"><label>(9)</label><mml:math id="mm108" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the probability that a point belongs to class, <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. We report the <italic toggle="yes">mean entropy (mH)</italic>, which calculates the average entropy across all points in the evaluation set. A&#x000a0;lower mean entropy value indicates higher model confidence in its&#x000a0;predictions.</p></sec><sec id="sec4dot2-sensors-25-01072"><title>4.2. Implementation&#x000a0;Details</title><p>During the pretraining phase, the&#x000a0;model is trained for 200 epochs on the WHU-Railway3D dataset using the Adam optimizer with a learning rate of <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and L2 regularization of <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The&#x000a0;learning rate is reduced by a factor of 0.5 after 100 epochs. A&#x000a0;batch size of 16 is used. For&#x000a0;episodic training in FSL, 40K episodic tasks were sampled from <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The&#x000a0;Adam optimizer was employed with a learning rate of <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which was halved every 5000 episodes. The&#x000a0;trained model was evaluated on 1500 test tasks sampled from <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. An&#x000a0;NVIDIA Tesla V100 GPU with 32 GB of memory was used for training. The&#x000a0;code was implemented in PyTorch and, along with the pretrained model, is publicly available at <uri xlink:href="https://gitlab.kuleuven.be/eavise/point-clouds/dist_shift_railroads_fsl.git">https://gitlab.kuleuven.be/eavise/point-clouds/dist_shift_railroads_fsl.git</uri>, (accessed on 1 February 2025).</p><p>To efficiently utilize limited computational resources, we adopted the data preprocessing strategy from PointNet, which employs a 3D sliding cubic window of size <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to subdivide a large point cloud into smaller cubic blocks. This sampling approach has also been utilized in PointNet++, DGCNN, and&#x000a0;many others. In&#x000a0;our case, <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="3.33333pt"/><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="3.33333pt"/><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and&#x000a0;<italic toggle="yes">z</italic> equals the maximum height (in meters) recorded in the point cloud. This process captures points within <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="3.33333pt"/><mml:mi mathvariant="normal">m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mspace width="3.33333pt"/><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> cubic blocks, from&#x000a0;which we randomly sample 2048 points as input to the network. We also conducted preliminary experiments using 1024 and 4096 points. Using 2048 points enabled the model to capture more representative features compared to 1024. However, increasing the number of points to 4096 did not yield significant performance benefits. Thus, 2048 points provided a favorable trade-off between computational efficiency and model performance.</p></sec></sec><sec id="sec5-sensors-25-01072"><title>5. Experimental Results and&#x000a0;Analysis</title><p>We conducted a comprehensive evaluation of our model across four scenarios: no-shift, ID shift, in-domain OOD shift, and&#x000a0;cross-domain OOD shift, with no-shift serving as the baseline. The model performance is quantified using the following performance metrics: <italic toggle="yes">mIoU</italic>, <italic toggle="yes">OA</italic>, and&#x000a0;<italic toggle="yes">MCC</italic>. Additionally, the&#x000a0;model&#x02019;s predictive uncertainty is assessed using <italic toggle="yes">mH</italic>. The&#x000a0;average performance over 1500 test tasks is reported as the experimental results, implementing one-way twenty-shot tasks (i.e., <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) for no-shift and ID-shift. For&#x000a0;in-domain OOD shift and cross-domain OOD shift scenarios, the results are reported for one-way (i.e., <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) tasks with five, ten, and&#x000a0;twenty shots (i.e., <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>20</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). Additionally, in&#x000a0;the cross-domain OOD shift scenario, the results are also reported for a five-way task setting (i.e., <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) with five, ten, and twenty shots. A&#x000a0;one-way task corresponds to a binary segmentation problem involving a single foreground class, while a five-way task represents a multi-class segmentation problem involving five foreground&#x000a0;classes.</p><sec id="sec5dot1-sensors-25-01072"><title>5.1. Quantitative&#x000a0;Results</title><p><xref rid="sensors-25-01072-t002" ref-type="table">Table 2</xref> presents the experimental results for the ID shift using the Infrabel-5 Railroad Segmentation dataset and the WHU-Railway3D dataset compared against the baselines under the no-shift scenario. The&#x000a0;results indicate that the baselines achieved the highest performance and the lowest predictive uncertainty, with&#x000a0;mIoU values of 89.1% and 83% and&#x000a0;OA values of 92.4% and 89% for&#x000a0;the Infrabel-5 Railroad Segmentation dataset and the WHU-Railway3D dataset, respectively. Under&#x000a0;the ID shift scenario, with&#x000a0;the application of jitter, mirroring, and&#x000a0;rotation, the&#x000a0;model&#x02019;s performance remained comparable to the baselines, deviating by ~1% across all the metrics. These findings emphasize the strong generalization capability of FSL under minimal variations in evaluation&#x000a0;conditions.</p><p><xref rid="sensors-25-01072-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-01072-t004" ref-type="table">Table 4</xref> summarize the experimental results for the <italic toggle="yes">in-domain OOD shift</italic> using one-way tasks. Specifically, <xref rid="sensors-25-01072-t003" ref-type="table">Table 3</xref> reports the results for two evaluation settings with previously unseen classes from the Infrabel-5 Railroad Segmentation dataset: <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> = {<monospace>pole</monospace>, <monospace>vegetation</monospace>} and <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> = {<monospace>cable</monospace>, <monospace>support device</monospace>}. In&#x000a0;the <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> = {<monospace>pole</monospace>, <monospace>vegetation</monospace>} setting, the&#x000a0;model achieved its best performance on twenty-shot tasks with 64.2% mIoU and 74.4% OA, representing ~7.5% mIoU and ~5.5% OA improvements over the second-best performance in ten-shot tasks. Conversely, in&#x000a0;the <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> = {<monospace>cable</monospace>, <monospace>support device</monospace>} setting, the&#x000a0;model attained comparable performance across five- and twenty-shot tasks, with&#x000a0;the highest performance observed in ten-shot tasks, ~1% better than in both other settings. The&#x000a0;lowest predictive uncertainty was recorded in twenty-shot tasks for the <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> = {<monospace>pole</monospace>, <monospace>vegetation</monospace>} and&#x000a0;in five-shot tasks for <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> = {<monospace>cable</monospace>, <monospace>support device</monospace>}.</p><p><xref rid="sensors-25-01072-t004" ref-type="table">Table 4</xref> presents the results for the same evaluation settings as <xref rid="sensors-25-01072-t003" ref-type="table">Table 3</xref> but&#x000a0;applied to the WHU-Railway3D dataset. In&#x000a0;the <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> = {<monospace>pole</monospace>, <monospace>vegetation</monospace>} setting, the&#x000a0;model achieved its best performance on twenty-shot tasks with 76% mIoU and 78.4% OA, comparable to the results from ten- and twenty-shot tasks. For&#x000a0;<inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> = {<monospace>cable</monospace>, <monospace>support device</monospace>}, the&#x000a0;highest performance was achieved in twenty-shot tasks with 59.1% mIoU and 83.9% OA, showing ~2.5% mIoU improvement over the second-best result in ten-shot tasks. The&#x000a0;lowest predictive uncertainty occurred in five-shot tasks for the first setting and in ten-shot tasks for the&#x000a0;second.</p><p><xref rid="sensors-25-01072-t005" ref-type="table">Table 5</xref> and <xref rid="sensors-25-01072-t006" ref-type="table">Table 6</xref> provide the experimental results for the <italic toggle="yes">cross-domain OOD shift</italic>, evaluated on the Infrabel-5 Railroad Segmentation dataset and the WHU-Railway3D dataset, respectively. In&#x000a0;<xref rid="sensors-25-01072-t005" ref-type="table">Table 5</xref>, the&#x000a0;model achieved its best performance for the one-way setting in twenty-shot tasks, with&#x000a0;77.5% mIoU and 84.8% OA, comparable to the second-best performance in ten-shot tasks. The&#x000a0;lowest predictive uncertainty is observed in the ten-shot tasks. For&#x000a0;the five-way setting, the&#x000a0;model attained its highest performance in twenty-shot tasks, achieving 50% mIoU and 70.6% OA, with&#x000a0;~3.2% improvements in mIoU over ten-shot tasks (second best) and ~4% OA improvements compared to five-shot tasks. The&#x000a0;twenty-shot task setting generates the lowest predictive&#x000a0;uncertainty.</p><p>In <xref rid="sensors-25-01072-t006" ref-type="table">Table 6</xref>, the&#x000a0;model achieved its best performance for the one-way setting in ten-shot tasks, with&#x000a0;71.7% mIoU and 82.4% OA, comparable to the results (&#x0003c;1%) for five-shot and twenty-shot tasks. The&#x000a0;lowest predictive uncertainty is observed under the same shot setting. For&#x000a0;the five-way setting, the&#x000a0;model performed best in twenty-shot tasks, with&#x000a0;48% mIoU and 62.4% OA, showing ~2.5% mIoU and ~1.6% OA improvements compared to the second-best results achieved in ten-shot tasks. The&#x000a0;lowest predictive uncertainty was observed in the twenty-shot&#x000a0;tasks.</p></sec><sec id="sec5dot2-sensors-25-01072"><title>5.2. Qualitative&#x000a0;Results</title><p>The qualitative results for the OOD shifts are illustrated in <xref rid="sensors-25-01072-f003" ref-type="fig">Figure 3</xref>, <xref rid="sensors-25-01072-f004" ref-type="fig">Figure 4</xref> and <xref rid="sensors-25-01072-f005" ref-type="fig">Figure 5</xref>. In&#x000a0;all the figures, the&#x000a0;model was evaluated via a one-way five-shot setting, with&#x000a0;smaller regions extracted from larger railroad areas for clear visualization. The&#x000a0;ground truth is displayed in the &#x02018;GT&#x02019; columns, while the model predictions are shown in the corresponding &#x02018;Pred&#x02019; columns.</p><p><xref rid="sensors-25-01072-f003" ref-type="fig">Figure 3</xref> illustrates the results for the in-domain OOD shift, showing the results for the Infrabel-5 Segmentation dataset in the first row and the WHU-Railway3D dataset in the second row. In&#x000a0;the first two columns of each row, vegetation is segmented by the model that was trained on ground, cable, and&#x000a0;support devices. In&#x000a0;the last two columns, support devices are segmented using the model that was trained on ground, vegetation, and&#x000a0;poles. The&#x000a0;results indicate better segmentation of vegetation from poles in the WHU-Railway3D dataset compared to the Infrabel-5 dataset. Conversely, the&#x000a0;model performed better at segmenting support devices from cables in the Infrabel-5 dataset, while some cables were misclassified as support devices in the WHU-Railway3D dataset. Overall, the&#x000a0;figure highlights the model&#x02019;s strong generalization to novel railway infrastructure classes despite receiving minimal supervision, with only five labeled support&#x000a0;examples.</p><p><xref rid="sensors-25-01072-f004" ref-type="fig">Figure 4</xref> and <xref rid="sensors-25-01072-f005" ref-type="fig">Figure 5</xref> present the qualitative results for the cross-domain OOD shift, evaluated on the Infrabel-5 Segmentation dataset and the WHU-Railway3D dataset, respectively, with&#x000a0;the other dataset used for training. These datasets exhibit notable structural differences: the vegetation and poles are taller in the WHU-Railway3D dataset, while the support devices vary significantly in form. Both figures demonstrate that the model generalizes well to previously unseen distributions, although&#x000a0;some challenges remain in segmenting vegetation (row 1, columns 1&#x02013;2) and distinguishing support devices from cables (row 2, columns 3&#x02013;4). Nonetheless, the&#x000a0;model accurately segments poles and cables when presented against distinct&#x000a0;backgrounds.</p></sec><sec id="sec5dot3-sensors-25-01072"><title>5.3. Ablation&#x000a0;Study</title><p><bold>Fine-tuning.</bold> Fine-tuning is often used to adapt a model to the test distribution when it significantly differs from the training distribution. We fine-tuned the pretrained model on the Infrabel-5 Railroad Segmentation dataset. The&#x000a0;last two layers of the pretrained model were fine-tuned on the WHU-Railway3D dataset to&#x000a0;better align the model with our selected evaluation classes. We employed the Adam optimizer with a learning rate of <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and weight decay of <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Training and evaluation follow the same data splits as the ID shift (<xref rid="sensors-25-01072-t001" ref-type="table">Table 1</xref>), with&#x000a0;a batch size of 16. The&#x000a0;training is conducted for 30 epochs.</p><p><xref rid="sensors-25-01072-t007" ref-type="table">Table 7</xref> presents the fine-tuning results for our segmentation task. The&#x000a0;results show that fine-tuning achieves more than 60% mIoU in both datasets, achieving 92.6% OA for the WHU-Railway3D dataset, which is 10% higher than the OA value for the Infrabel-5 Railroad Segmentation dataset.</p></sec></sec><sec sec-type="discussion" id="sec6-sensors-25-01072"><title>6. Discussion</title><p>This work discusses the generalization of FSL under inevitable distributional shifts encountered in real-world railroad monitoring, arising from sensor noise (ID shift), infrastructure upgrades (in-domain OOD shift), and&#x000a0;environmental variations across geographical regions (cross-domain OOD shift). The&#x000a0;baseline performances achieved under the no-shift condition (<xref rid="sensors-25-01072-t002" ref-type="table">Table 2</xref>), representing optimal similarity to the training conditions, provide an upper bound for comparison. The&#x000a0;results for ID shift (<xref rid="sensors-25-01072-t002" ref-type="table">Table 2</xref>), compared to the baselines, demonstrate minimal impact on model performance, with&#x000a0;deviations of ~1%, highlighting FSL&#x02019;s strong generalization ability under minor variations in evaluation&#x000a0;conditions.</p><p>The in-domain OOD shift, which evaluated the model on novel, previously unseen classes from the same dataset as the training set, indicates a performance drop. For&#x000a0;the Infrabel-5 Railroad Segmentation dataset, the&#x000a0;{<monospace>pole</monospace>, <monospace>vegetation</monospace>} setting experienced a ~24.9% decrease in mIoU and 18.1% reduction in OA compared to the baseline (<xref rid="sensors-25-01072-t003" ref-type="table">Table 3</xref> vs. <xref rid="sensors-25-01072-t002" ref-type="table">Table 2</xref>). In&#x000a0;contrast, the&#x000a0;{<monospace>cable</monospace>, <monospace>support device</monospace>} setting exhibited a 1.45% decline in mIoU with a comparable OA. For&#x000a0;the WHU-Railway3D dataset (<xref rid="sensors-25-01072-t004" ref-type="table">Table 4</xref> vs. <xref rid="sensors-25-01072-t002" ref-type="table">Table 2</xref>), the&#x000a0;{<monospace>pole</monospace>, <monospace>vegetation</monospace>} setting demonstrates a ~7.4% mIoU and ~10.7% OA decrease, while the {<monospace>cable</monospace>, <monospace>support device</monospace>} setting shows a ~23.9% mIoU and ~5.1% OA decline. These performance differences across the datasets (<xref rid="sensors-25-01072-t003" ref-type="table">Table 3</xref> vs. <xref rid="sensors-25-01072-t004" ref-type="table">Table 4</xref>) for the two evaluation settings are likely attributable to class imbalances between the datasets. Vegetation has higher representation in the WHU-Railway3D dataset, whereas cable and support device classes are more prevalent in the Infrabel-5 Segmentation dataset (<xref rid="sensors-25-01072-f006" ref-type="fig">Figure 6</xref>).</p><p>The cross-domain OOD shift with one-way tasks resulted in performance decreases of ~11.3&#x02013;11.6% in mIoU and ~6.6&#x02013;7.6% in OA when compared to the baselines (<xref rid="sensors-25-01072-t005" ref-type="table">Table 5</xref> and <xref rid="sensors-25-01072-t006" ref-type="table">Table 6</xref> vs. <xref rid="sensors-25-01072-t002" ref-type="table">Table 2</xref>) for both datasets. Specifically, the&#x000a0;mIoU and OA decreased by 11.6% and 7.65% on the Infrabel-5 Railroad Segmentation dataset and by 11.3% and 6.6% on the WHU-Railway3D&#x000a0;dataset.</p><p><italic toggle="yes">N-way K-shot</italic> tasks were examined for different <italic toggle="yes">N</italic> and <italic toggle="yes">K</italic> configurations. Under&#x000a0;cross-domain OOD shifts, increasing <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> led to performance drops of 27.5% mIoU and ~14.2% OA on the Infrabel-5 Railroad Segmentation dataset, and&#x000a0;~23.7% mIoU and ~19.9% OA on the WHU-Railway3D dataset. These results align with existing FSL research, where the increased complexity of multi-class segmentation (<inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) tasks contributes to the observed performance disparity. Increasing <italic toggle="yes">K</italic> to 20 notably enhanced the model performance in some settings. For&#x000a0;instance, in&#x000a0;the in-domain OOD shift for the {<monospace>pole</monospace>, <monospace>vegetation</monospace>} setting on the Infrabel-5 Railroad Segmentation dataset, the&#x000a0;mIoU and OA improved by ~12.1% and ~10%, respectively, compared to <inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref rid="sensors-25-01072-t003" ref-type="table">Table 3</xref>). Similarly, for&#x000a0;the {<monospace>cable</monospace>, <monospace>support device</monospace>} setting on the WHU-Railway3D dataset, the performance increased by ~2.5% in mIoU and ~1.6% in OA (<xref rid="sensors-25-01072-t004" ref-type="table">Table 4</xref>). For&#x000a0;cross-domain OOD shift using one-way tasks, improvements of over 4% in both mIoU (4.43) and OA (4.16) were observed compared to <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for the Infrabel-5 Segmentation dataset. Similar behavior was observed in the five-way tasks, where increasing <italic toggle="yes">K</italic> resulted in ~4-4.4% gains in both mIoU and OA across both the Infrabel-5 Segmentation dataset and WHU-Railway3D dataset (<xref rid="sensors-25-01072-t005" ref-type="table">Table 5</xref> and <xref rid="sensors-25-01072-t006" ref-type="table">Table 6</xref>). However, in&#x000a0;other scenarios, the&#x000a0;number of support examples had a minimal effect on performance. While increasing <italic toggle="yes">K</italic> boosted the results, it occasionally introduced noise in some cases, leading to slight reductions in model&#x000a0;performance.</p><p>This study underscores the importance of evaluating model performance using the <italic toggle="yes">MCC</italic> metric, which is particularly useful for identifying trivial majority classifiers. An&#x000a0;<italic toggle="yes">MCC</italic> value of 0 indicates a model that predicts the majority class irrespective of input features&#x000a0;[<xref rid="B74-sensors-25-01072" ref-type="bibr">74</xref>]. Given the highly class-imbalanced nature of our datasets, <italic toggle="yes">MCC</italic> offers a more realistic assessment of model performance as&#x000a0;it penalizes errors in minority classes more effectively than <italic toggle="yes">OA</italic>. The baseline results (<xref rid="sensors-25-01072-t002" ref-type="table">Table 2</xref>) exhibit 82% agreement with the ground truth for the Infrabel-5 Segmentation dataset and 77% for the WHU-Railway3D dataset, while the corresponding OA values are higher, i.e., 92% and 89%, respectively. Similar trends are observed across all the experiments under distributional shifts. In those experiments involving one-way tasks, <italic toggle="yes">MCC</italic> consistently exceeded 60%, except&#x000a0;in the in-domain OOD shift for the {<monospace>pole</monospace>, <monospace>vegetation</monospace>} setting, where it decreased to 47% for the Infrabel-5 Segmentation dataset and 39% for the WHU-Railway3D dataset, respectively.</p><p>The comparison between the FSL baselines under no-shift (<xref rid="sensors-25-01072-t002" ref-type="table">Table 2</xref>) and fine-tuning (<xref rid="sensors-25-01072-t007" ref-type="table">Table 7</xref>) shows that FSL achieved an 18.1% improvement in mIoU on the Infrabel-5 Railroad Segmentation dataset and a 21.3% improvement on the WHU-Railway3D dataset. Under&#x000a0;cross-domain OOD shift with the one-way task setting (<xref rid="sensors-25-01072-t005" ref-type="table">Table 5</xref> and <xref rid="sensors-25-01072-t006" ref-type="table">Table 6</xref> vs. <xref rid="sensors-25-01072-t007" ref-type="table">Table 7</xref>), FSL achieved 17.5% and 9.9% mIoU improvements on the Infrabel-5 Railroad Segmentation dataset and WHU-Railway3D dataset, respectively. However, such cross-domain OOD generalization with limited data is not inherent to fine-tuning as&#x000a0;fine-tuning relies on the i.i.d. assumption and is evaluated on a test set drawn from the same distribution as the training data. Compared to fine-tuning, the FSL baselines (under no-shift) yielded a higher <italic toggle="yes">mH</italic> value on the WHU-Railway3D dataset. However, on&#x000a0;the Infrabel-5 Railroad Segmentation dataset, <italic toggle="yes">mH</italic> was lower, suggesting that the model exhibits greater confidence when trained on a smaller dataset with FSL. Under&#x000a0;cross-domain OOD shift, FSL resulted in higher <italic toggle="yes">mH</italic> values, reflecting increased uncertainty in unseen environments. Nevertheless, the&#x000a0;model remains capable of generalizing beyond the i.i.d. assumption&#x02014;an aspect where fine-tuning falls short. These findings further highlight the importance of incorporating predictive uncertainty measures, such as entropy, in&#x000a0;the evaluation of FSL models.</p><p>The entropy measure provides insights into the reliability of our model&#x02019;s predictions by quantifying uncertainty, which is crucial for real-world applications such as railroad monitoring, where distributional shifts are inevitable. Since test data often lack ground truth for verification, quantifying predictive outcomes through entropy can support decision-making. When the model exhibits low confidence in its predictions, highly uncertain samples can be referred to human experts for further review. <xref rid="sensors-25-01072-f007" ref-type="fig">Figure 7</xref> illustrates this for the Infrabel-5 Railroad Segmentation and WHU-Railway3D datasets under cross-domain OOD settings, highlighting highly uncertain samples for poles (red), ground (gray), vegetation (green), cables (purple), and&#x000a0;support devices (yellow). For&#x000a0;instance, three highly uncertain pole samples (red) in the Infrabel-5 Railroad Segmentation dataset could be flagged for human evaluation in critical decision-making scenarios. Moreover, in&#x000a0;domain adaptation with FSL, <italic toggle="yes">mH</italic> provides essential insights: a high <italic toggle="yes">mH</italic> value indicates low model confidence under a new distribution, aiding in decisions such as model rejection, recalibration, or&#x000a0;further&#x000a0;fine-tuning.</p></sec><sec id="sec7-sensors-25-01072"><title>7. Challenges and Future&#x000a0;Prospects</title><p>Our work currently has certain limitations, primarily linked to the availability of only <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> coordinates as features in our datasets. Segmenting vegetation from the ground solely based on <italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, <italic toggle="yes">z</italic> coordinates presents significant challenges as the most critical features often rely on height (<italic toggle="yes">z</italic> coordinates). In complex environments like railways, the height of the ground plane can vary, particularly between the track bed and its surroundings. Incorporating additional information, such as RGB color data, could enhance the model&#x02019;s ability to learn more robust features. Furthermore, the development of more balanced training datasets is crucial for improving the effectiveness and generalization of machine learning models in railway monitoring. In the future, model calibration or active learning can be explored in conjunction with FSL for models under such distributional shifts.</p></sec><sec sec-type="conclusions" id="sec8-sensors-25-01072"><title>8. Conclusions</title><p>Machine learning models are often evaluated under the i.i.d. assumption for training and test sets. However, this assumption does not hold true in dynamic real-world applications, particularly in complex railroad environments. In this work, we formulated distributional shifts that arise in railway environments due to sensor noise, infrastructure upgrades, or environmental variations across regions. We explored FSL for railway monitoring via the segmentation of 3D point clouds and evaluated its generalization capability under distributional shifts with minimal supervision at test time. We presented an extensive evaluation of the model performance, including an analysis and discussion, and assessed the model&#x02019;s predictive confidence, emphasizing the crucial role of such evaluation in developing reliable models for real-world applications.</p></sec></body><back><ack><title>Acknowledgments</title><p>We sincerely thank Infrabel for providing the data.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, A.R.F.; methodology, A.R.F.; software, A.R.F. and M.L.; validation, A.R.F., M.L. and P.V.; formal analysis, A.R.F. and P.V.; investigation, A.R.F.; resources, P.V.; data curation, M.L.; writing&#x02014;original draft preparation, A.R.F.; writing&#x02014;review and editing, P.V.; visualization, A.R.F.; supervision, P.V.; project administration, P.V.; funding acquisition, P.V. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The WHU-Railway3D dataset is publicly available from &#x02018;WHU-Railway3D: A Diverse Dataset and Benchmark for Railway Point Cloud Semantic Segmentation&#x02019; at <uri xlink:href="https://github.com/WHU-USI3DV/WHU-Railway3D/">https://github.com/WHU-USI3DV/WHU-Railway3D/</uri>, (accessed on 5 January 2025). However, access to the Infrabel-5 dataset is restricted. This dataset was obtained from Infrabel, and access must be requested directly from them.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01072"><label>1.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>Organisation for Economic Co-Operation and Development (OECD)</collab>
</person-group><article-title>Freight Transport, Indicator (Rail)</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://www.oecd.org/en/data/indicators/freight-transport.html" ext-link-type="uri">https://www.oecd.org/en/data/indicators/freight-transport.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-08-27">(accessed on 27 August 2024)</date-in-citation></element-citation></ref><ref id="B2-sensors-25-01072"><label>2.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>Organisation for Economic Co-Operation and Development (OECD)</collab>
</person-group><article-title>Container Transport, Indicator (Rail)</article-title><year>2022</year><comment>Available online: <ext-link xlink:href="https://www.oecd.org/en/data/indicators/container-transport.html" ext-link-type="uri">https://www.oecd.org/en/data/indicators/container-transport.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-08-27">(accessed on 27 August 2024)</date-in-citation></element-citation></ref><ref id="B3-sensors-25-01072"><label>3.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>Organisation for Economic Co-Operation and Development (OECD)</collab>
</person-group><article-title>Infrastructure Maintenance, Indicator (Rail)</article-title><year>2021</year><comment>Available online: <ext-link xlink:href="https://www.oecd.org/en/data/indicators/infrastructure-maintenance.html" ext-link-type="uri">https://www.oecd.org/en/data/indicators/infrastructure-maintenance.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-08-27">(accessed on 27 August 2024)</date-in-citation></element-citation></ref><ref id="B4-sensors-25-01072"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>R.</given-names></name>
<name><surname>Wu</surname><given-names>Y.</given-names></name>
<name><surname>Jin</surname><given-names>W.</given-names></name>
<name><surname>Meng</surname><given-names>X.</given-names></name>
</person-group><article-title>Deep-Learning-Based Point Cloud Semantic Segmentation: A Survey</article-title><source>Electronics</source><year>2023</year><volume>12</volume><elocation-id>3642</elocation-id><pub-id pub-id-type="doi">10.3390/electronics12173642</pub-id></element-citation></ref><ref id="B5-sensors-25-01072"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sarker</surname><given-names>S.</given-names></name>
<name><surname>Sarker</surname><given-names>P.</given-names></name>
<name><surname>Stone</surname><given-names>G.</given-names></name>
<name><surname>Gorman</surname><given-names>R.</given-names></name>
<name><surname>Tavakkoli</surname><given-names>A.</given-names></name>
<name><surname>Bebis</surname><given-names>G.</given-names></name>
<name><surname>Sattarvand</surname><given-names>J.</given-names></name>
</person-group><article-title>A comprehensive overview of deep learning techniques for 3D point cloud classification and semantic segmentation</article-title><source>Mach. Vis. Appl.</source><year>2024</year><volume>35</volume><fpage>67</fpage><pub-id pub-id-type="doi">10.1007/s00138-024-01543-1</pub-id></element-citation></ref><ref id="B6-sensors-25-01072"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gawlikowski</surname><given-names>J.</given-names></name>
<name><surname>Tassi</surname><given-names>C.R.N.</given-names></name>
<name><surname>Ali</surname><given-names>M.</given-names></name>
<name><surname>Lee</surname><given-names>J.</given-names></name>
<name><surname>Humt</surname><given-names>M.</given-names></name>
<name><surname>Feng</surname><given-names>J.</given-names></name>
<name><surname>Kruspe</surname><given-names>A.</given-names></name>
<name><surname>Triebel</surname><given-names>R.</given-names></name>
<name><surname>Jung</surname><given-names>P.</given-names></name>
<name><surname>Roscher</surname><given-names>R.</given-names></name>
<etal/>
</person-group><article-title>A survey of uncertainty in deep neural networks</article-title><source>Artif. Intell. Rev.</source><year>2023</year><volume>56</volume><fpage>1513</fpage><lpage>1589</lpage><pub-id pub-id-type="doi">10.1007/s10462-023-10562-9</pub-id></element-citation></ref><ref id="B7-sensors-25-01072"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Bansak</surname><given-names>K.C.</given-names></name>
<name><surname>Paulson</surname><given-names>E.</given-names></name>
<name><surname>Rothenh&#x000e4;usler</surname><given-names>D.</given-names></name>
</person-group><article-title>Learning under random distributional shifts</article-title><source>Proceedings of the International Conference on Artificial Intelligence and Statistics, Palau de Congressos</source><conf-loc>Valencia, Spain</conf-loc><conf-date>2&#x02013;4 May 2024</conf-date><comment>PMLR, 2024</comment><fpage>3943</fpage><lpage>3951</lpage></element-citation></ref><ref id="B8-sensors-25-01072"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Malinin</surname><given-names>A.</given-names></name>
<name><surname>Athanasopoulos</surname><given-names>A.</given-names></name>
<name><surname>Barakovic</surname><given-names>M.</given-names></name>
<name><surname>Cuadra</surname><given-names>M.B.</given-names></name>
<name><surname>Gales</surname><given-names>M.J.</given-names></name>
<name><surname>Granziera</surname><given-names>C.</given-names></name>
<name><surname>Graziani</surname><given-names>M.</given-names></name>
<name><surname>Kartashev</surname><given-names>N.</given-names></name>
<name><surname>Kyriakopoulos</surname><given-names>K.</given-names></name>
<name><surname>Lu</surname><given-names>P.J.</given-names></name>
<etal/>
</person-group><article-title>Shifts 2.0: Extending the dataset of real distributional shifts</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2206.15407</pub-id></element-citation></ref><ref id="B9-sensors-25-01072"><label>9.</label><element-citation publication-type="webpage"><article-title>Data-Centric AI (DCAI), CSAIL, MIT. Class Imbalance, Outliers, and Distribution Shift</article-title><year>2024</year><comment>Available online: <ext-link xlink:href="https://dcai.csail.mit.edu/2024/imbalance-outliers-shift/#:~:text=Distribution%20shift%20is%20a%20challenging,test%20(%20x%20%2C%20y%20)%20" ext-link-type="uri">https://dcai.csail.mit.edu/2024/imbalance-outliers-shift/#:~:text=Distribution%20shift%20is%20a%20challenging,test%20(%20x%20%2C%20y%20)%20</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-08-28">(accessed on 28 August 2024)</date-in-citation></element-citation></ref><ref id="B10-sensors-25-01072"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fayjie</surname><given-names>A.R.</given-names></name>
<name><surname>Borah</surname><given-names>J.</given-names></name>
<name><surname>Carbone</surname><given-names>F.</given-names></name>
<name><surname>Tack</surname><given-names>J.</given-names></name>
<name><surname>Vandewalle</surname><given-names>P.</given-names></name>
</person-group><article-title>Predictive uncertainty estimation in deep learning for lung carcinoma classification in digital pathology under real dataset shifts</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2408.08432</pub-id></element-citation></ref><ref id="B11-sensors-25-01072"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ravi</surname><given-names>S.</given-names></name>
<name><surname>Larochelle</surname><given-names>H.</given-names></name>
</person-group><article-title>Optimization as a model for few-shot learning</article-title><source>Proceedings of the International Conference on Learning Representations, Palais des Congr&#x000e8;s Neptune</source><conf-loc>Toulon, France</conf-loc><conf-date>24&#x02013;26 April 2017</conf-date></element-citation></ref><ref id="B12-sensors-25-01072"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Vinyals</surname><given-names>O.</given-names></name>
<name><surname>Blundell</surname><given-names>C.</given-names></name>
<name><surname>Lillicrap</surname><given-names>T.</given-names></name>
<name><surname>Wierstra</surname><given-names>D.</given-names></name>
</person-group><article-title>Matching networks for one shot learning</article-title><source>Proceedings of the International Conference on Neural Information Processing Systems</source><conf-loc>Barcelona, Spain</conf-loc><conf-date>5&#x02013;10 December 2016</conf-date><volume>Volume 29</volume><fpage>3637</fpage><lpage>3645</lpage></element-citation></ref><ref id="B13-sensors-25-01072"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sung</surname><given-names>F.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Xiang</surname><given-names>T.</given-names></name>
<name><surname>Torr</surname><given-names>P.H.</given-names></name>
<name><surname>Hospedales</surname><given-names>T.M.</given-names></name>
</person-group><article-title>Learning to compare: Relation network for few-shot learning</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#x02013;23 June 2018</conf-date><fpage>1199</fpage><lpage>1208</lpage></element-citation></ref><ref id="B14-sensors-25-01072"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Song</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>T.</given-names></name>
<name><surname>Cai</surname><given-names>P.</given-names></name>
<name><surname>Mondal</surname><given-names>S.K.</given-names></name>
<name><surname>Sahoo</surname><given-names>J.P.</given-names></name>
</person-group><article-title>A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities</article-title><source>ACM Comput. Surv.</source><year>2023</year><volume>55</volume><fpage>271</fpage><pub-id pub-id-type="doi">10.1145/3582688</pub-id></element-citation></ref><ref id="B15-sensors-25-01072"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Brown</surname><given-names>T.</given-names></name>
<name><surname>Mann</surname><given-names>B.</given-names></name>
<name><surname>Ryder</surname><given-names>N.</given-names></name>
<name><surname>Subbiah</surname><given-names>M.</given-names></name>
<name><surname>Kaplan</surname><given-names>J.D.</given-names></name>
<name><surname>Dhariwal</surname><given-names>P.</given-names></name>
<name><surname>Neelakantan</surname><given-names>A.</given-names></name>
<name><surname>Shyam</surname><given-names>P.</given-names></name>
<name><surname>Sastry</surname><given-names>G.</given-names></name>
<name><surname>Askell</surname><given-names>A.</given-names></name>
<etal/>
</person-group><article-title>Language models are few-shot learners</article-title><source>Proceedings of the International Conference on Neural Information Processing Systems</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>6&#x02013;12 December 2020</conf-date><volume>Volume 33</volume><fpage>1877</fpage><lpage>1901</lpage></element-citation></ref><ref id="B16-sensors-25-01072"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>N.</given-names></name>
<name><surname>Chua</surname><given-names>T.S.</given-names></name>
<name><surname>Lee</surname><given-names>G.H.</given-names></name>
</person-group><article-title>Few-shot 3D point cloud semantic segmentation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#x02013;25 June 2021</conf-date><fpage>8873</fpage><lpage>8882</lpage></element-citation></ref><ref id="B17-sensors-25-01072"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
<name><surname>Lin</surname><given-names>G.</given-names></name>
<name><surname>Han</surname><given-names>J.</given-names></name>
</person-group><article-title>Compositional prototype network with multi-view comparision for few-shot point cloud semantic segmentation</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2012.14255</pub-id></element-citation></ref><ref id="B18-sensors-25-01072"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>Z.</given-names></name>
<name><surname>Wu</surname><given-names>Z.</given-names></name>
<name><surname>Wu</surname><given-names>X.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
</person-group><article-title>Crossmodal few-shot 3D point cloud semantic segmentation</article-title><source>Proceedings of the ACM International Conference on Multimedia</source><conf-loc>Lisboa, Portugal</conf-loc><conf-date>10&#x02013;14 October 2022</conf-date><fpage>4760</fpage><lpage>4768</lpage></element-citation></ref><ref id="B19-sensors-25-01072"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mao</surname><given-names>Y.</given-names></name>
<name><surname>Guo</surname><given-names>Z.</given-names></name>
<name><surname>Xiaonan</surname><given-names>L.</given-names></name>
<name><surname>Yuan</surname><given-names>Z.</given-names></name>
<name><surname>Guo</surname><given-names>H.</given-names></name>
</person-group><article-title>Bidirectional feature globalization for few-shot semantic segmentation of 3D point cloud scenes</article-title><source>Proceedings of the International Conference on 3D Vision (3DV)</source><conf-loc>Prague, Czech Republic</conf-loc><conf-date>12&#x02013;15 September 2022</conf-date><fpage>505</fpage><lpage>514</lpage></element-citation></ref><ref id="B20-sensors-25-01072"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wei</surname><given-names>M.</given-names></name>
</person-group><article-title>Few-shot 3D Point Cloud Semantic Segmentation with Prototype Alignment</article-title><source>Proceedings of the International Conference on Machine Learning Technologies</source><conf-loc>Stockholm Sweden</conf-loc><conf-date>10&#x02013;12 March 2023</conf-date><fpage>195</fpage><lpage>200</lpage></element-citation></ref><ref id="B21-sensors-25-01072"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ning</surname><given-names>Z.</given-names></name>
<name><surname>Tian</surname><given-names>Z.</given-names></name>
<name><surname>Lu</surname><given-names>G.</given-names></name>
<name><surname>Pei</surname><given-names>W.</given-names></name>
</person-group><article-title>Boosting few-shot 3D point cloud segmentation via query-guided enhancement</article-title><source>Proceedings of the ACM International Conference on Multimedia</source><conf-loc>Ottawa, ON, Canada</conf-loc><conf-date>29 October&#x02013;3 November 2023</conf-date><fpage>1895</fpage><lpage>1904</lpage></element-citation></ref><ref id="B22-sensors-25-01072"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>X.</given-names></name>
<name><surname>Hu</surname><given-names>H.</given-names></name>
<name><surname>Yang</surname><given-names>X.</given-names></name>
<name><surname>Deng</surname><given-names>Y.</given-names></name>
</person-group><article-title>Enhancing Few-Shot 3D Point Cloud Semantic Segmentation through Bidirectional Prototype Learning</article-title><source>Proceedings of the International Conference on Robotics and Artificial Intelligence</source><conf-loc>Singapore</conf-loc><conf-date>17&#x02013;19 November 2023</conf-date><fpage>7</fpage><lpage>16</lpage></element-citation></ref><ref id="B23-sensors-25-01072"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>C.</given-names></name>
<name><surname>Wu</surname><given-names>Z.</given-names></name>
<name><surname>Wu</surname><given-names>X.</given-names></name>
<name><surname>Zhao</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
</person-group><article-title>Few-shot 3D point cloud semantic segmentation via stratified class-specific attention based transformer network</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Washington, DC, USA</conf-loc><conf-date>7&#x02013;14 February 2023</conf-date><volume>Volume 37</volume><fpage>3410</fpage><lpage>3417</lpage></element-citation></ref><ref id="B24-sensors-25-01072"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>Y.</given-names></name>
<name><surname>Hu</surname><given-names>C.</given-names></name>
<name><surname>Zhao</surname><given-names>N.</given-names></name>
<name><surname>Lee</surname><given-names>G.H.</given-names></name>
</person-group><article-title>Generalized few-shot point cloud segmentation via geometric words</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#x02013;6 October 2023</conf-date><fpage>21506</fpage><lpage>21515</lpage></element-citation></ref><ref id="B25-sensors-25-01072"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>He</surname><given-names>S.</given-names></name>
<name><surname>Jiang</surname><given-names>X.</given-names></name>
<name><surname>Jiang</surname><given-names>W.</given-names></name>
<name><surname>Ding</surname><given-names>H.</given-names></name>
</person-group><article-title>Prototype adaption and projection for few-and zero-shot 3D point cloud semantic segmentation</article-title><source>IEEE Trans. Image Process.</source><year>2023</year><volume>32</volume><fpage>3199</fpage><lpage>3211</lpage><pub-id pub-id-type="doi">10.1109/TIP.2023.3279660</pub-id><pub-id pub-id-type="pmid">37252865</pub-id>
</element-citation></ref><ref id="B26-sensors-25-01072"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Jiang</surname><given-names>G.</given-names></name>
<name><surname>Hua</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Part-Whole Relational Few-Shot 3D Point Cloud Semantic Segmentation</article-title><source>Comput. Mater. Contin.</source><year>2024</year><volume>78</volume><fpage>3021</fpage><lpage>3039</lpage><pub-id pub-id-type="doi">10.32604/cmc.2023.045853</pub-id></element-citation></ref><ref id="B27-sensors-25-01072"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>S.</given-names></name>
<name><surname>Ding</surname><given-names>H.</given-names></name>
<name><surname>Jiang</surname><given-names>X.</given-names></name>
</person-group><article-title>Generalized Few-Shot 3D Point Cloud Segmentation</article-title><source>Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS)</source><conf-loc>Singapore</conf-loc><conf-date>19&#x02013;22 May 2024</conf-date><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B28-sensors-25-01072"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Elberink</surname><given-names>S.O.</given-names></name>
<name><surname>Khoshelham</surname><given-names>K.</given-names></name>
<name><surname>Arastounia</surname><given-names>K.</given-names></name>
<name><surname>Benito</surname><given-names>D.D.</given-names></name>
</person-group><article-title>Rail track detection and modelling in mobile laser scanning data</article-title><source>Proceedings of the ISPRS Workshop Laser Scanning 2013. International Society for Photogrammetry and Remote Sensing (ISPRS)</source><fpage>223</fpage><lpage>228</lpage><comment>Available online: <ext-link xlink:href="https://isprs-annals.copernicus.org/articles/II-5-W2/223/2013/" ext-link-type="uri">https://isprs-annals.copernicus.org/articles/II-5-W2/223/2013/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-08-28">(accessed on 28 August 2024)</date-in-citation></element-citation></ref><ref id="B29-sensors-25-01072"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>B.</given-names></name>
<name><surname>Fang</surname><given-names>L.</given-names></name>
</person-group><article-title>Automated extraction of 3-D railway tracks from mobile laser scanning point clouds</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2014</year><volume>7</volume><fpage>4750</fpage><lpage>4761</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2014.2312378</pub-id></element-citation></ref><ref id="B30-sensors-25-01072"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>L.</given-names></name>
<name><surname>Hyyppa</surname><given-names>J.</given-names></name>
</person-group><article-title>The use of airborne and mobile laser scanning for modeling railway environments in 3D</article-title><source>Remote Sens.</source><year>2014</year><volume>6</volume><fpage>3075</fpage><lpage>3100</lpage><pub-id pub-id-type="doi">10.3390/rs6043075</pub-id></element-citation></ref><ref id="B31-sensors-25-01072"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Arastounia</surname><given-names>M.</given-names></name>
</person-group><article-title>Automated recognition of railroad infrastructure in rural areas from LiDAR data</article-title><source>Remote Sens.</source><year>2015</year><volume>7</volume><fpage>14916</fpage><lpage>14938</lpage><pub-id pub-id-type="doi">10.3390/rs71114916</pub-id></element-citation></ref><ref id="B32-sensors-25-01072"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Oude Elberink</surname><given-names>S.</given-names></name>
<name><surname>Khoshelham</surname><given-names>K.</given-names></name>
</person-group><article-title>Automatic extraction of railroad centerlines from mobile laser scanning data</article-title><source>Remote Sens.</source><year>2015</year><volume>7</volume><fpage>5565</fpage><lpage>5583</lpage><pub-id pub-id-type="doi">10.3390/rs70505565</pub-id></element-citation></ref><ref id="B33-sensors-25-01072"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lamas</surname><given-names>D.</given-names></name>
<name><surname>Soil&#x000e1;n</surname><given-names>M.</given-names></name>
<name><surname>Grand&#x000ed;o</surname><given-names>J.</given-names></name>
<name><surname>Riveiro</surname><given-names>B.</given-names></name>
</person-group><article-title>Automatic point cloud semantic segmentation of complex railway environments</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>2332</elocation-id><pub-id pub-id-type="doi">10.3390/rs13122332</pub-id></element-citation></ref><ref id="B34-sensors-25-01072"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>S&#x000e1;nchez-Rodr&#x000ed;guez</surname><given-names>A.</given-names></name>
<name><surname>Riveiro</surname><given-names>B.</given-names></name>
<name><surname>Soil&#x000e1;n</surname><given-names>M.</given-names></name>
<name><surname>Gonz&#x000e1;lez-deSantos</surname><given-names>L.</given-names></name>
</person-group><article-title>Automated detection and decomposition of railway tunnels from Mobile Laser Scanning Datasets</article-title><source>Autom. Constr.</source><year>2018</year><volume>96</volume><fpage>171</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.autcon.2018.09.014</pub-id></element-citation></ref><ref id="B35-sensors-25-01072"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Soil&#x000e1;n</surname><given-names>M.</given-names></name>
<name><surname>N&#x000f3;voa</surname><given-names>A.</given-names></name>
<name><surname>S&#x000e1;nchez-Rodr&#x000ed;guez</surname><given-names>A.</given-names></name>
<name><surname>Riveiro</surname><given-names>B.</given-names></name>
<name><surname>Arias</surname><given-names>P.</given-names></name>
</person-group><article-title>Semantic segmentation of point clouds with pointnet and kpconv architectures applied to railway tunnels</article-title><source>ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci.</source><year>2020</year><volume>2</volume><fpage>281</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.5194/isprs-annals-V-2-2020-281-2020</pub-id></element-citation></ref><ref id="B36-sensors-25-01072"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Qi</surname><given-names>C.R.</given-names></name>
<name><surname>Su</surname><given-names>H.</given-names></name>
<name><surname>Mo</surname><given-names>K.</given-names></name>
<name><surname>Guibas</surname><given-names>L.J.</given-names></name>
</person-group><article-title>Pointnet: Deep learning on point sets for 3D classification and segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;26 July 2017</conf-date><fpage>652</fpage><lpage>660</lpage></element-citation></ref><ref id="B37-sensors-25-01072"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Thomas</surname><given-names>H.</given-names></name>
<name><surname>Qi</surname><given-names>C.R.</given-names></name>
<name><surname>Deschaud</surname><given-names>J.E.</given-names></name>
<name><surname>Marcotegui</surname><given-names>B.</given-names></name>
<name><surname>Goulette</surname><given-names>F.</given-names></name>
<name><surname>Guibas</surname><given-names>L.J.</given-names></name>
</person-group><article-title>KPConv: Flexible and deformable convolution for point clouds</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#x02013;2 November 2019</conf-date><fpage>6411</fpage><lpage>6420</lpage></element-citation></ref><ref id="B38-sensors-25-01072"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>L.</given-names></name>
<name><surname>Jung</surname><given-names>J.</given-names></name>
<name><surname>Sohn</surname><given-names>G.</given-names></name>
</person-group><article-title>Multi-scale hierarchical CRF for railway electrification asset classification from mobile laser scanning data</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2019</year><volume>12</volume><fpage>3131</fpage><lpage>3148</lpage><pub-id pub-id-type="doi">10.1109/JSTARS.2019.2918272</pub-id></element-citation></ref><ref id="B39-sensors-25-01072"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Yu</surname><given-names>G.</given-names></name>
<name><surname>Chen</surname><given-names>P.</given-names></name>
<name><surname>Zhou</surname><given-names>B.</given-names></name>
<name><surname>Yang</surname><given-names>S.</given-names></name>
</person-group><article-title>FarNet: An attention-aggregation network for long-range rail track point cloud segmentation</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2021</year><volume>23</volume><fpage>13118</fpage><lpage>13126</lpage><pub-id pub-id-type="doi">10.1109/TITS.2021.3119900</pub-id></element-citation></ref><ref id="B40-sensors-25-01072"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ton</surname><given-names>B.</given-names></name>
<name><surname>Ahmed</surname><given-names>F.</given-names></name>
<name><surname>Linssen</surname><given-names>J.</given-names></name>
</person-group><article-title>Semantic segmentation of terrestrial laser scans of railway catenary arches: A use case perspective</article-title><source>Sensors</source><year>2022</year><volume>23</volume><elocation-id>222</elocation-id><pub-id pub-id-type="doi">10.3390/s23010222</pub-id><pub-id pub-id-type="pmid">36616820</pub-id>
</element-citation></ref><ref id="B41-sensors-25-01072"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Qi</surname><given-names>C.R.</given-names></name>
<name><surname>Yi</surname><given-names>L.</given-names></name>
<name><surname>Su</surname><given-names>H.</given-names></name>
<name><surname>Guibas</surname><given-names>L.J.</given-names></name>
</person-group><article-title>Pointnet++: Deep hierarchical feature learning on point sets in a metric space</article-title><source>Proceedings of the International Conference on Neural Information Processing Systems</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>4&#x02013;9 December 2017</conf-date><volume>Volume 30</volume></element-citation></ref><ref id="B42-sensors-25-01072"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Landrieu</surname><given-names>L.</given-names></name>
<name><surname>Simonovsky</surname><given-names>M.</given-names></name>
</person-group><article-title>Large-scale point cloud semantic segmentation with superpoint graphs</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#x02013;23 June 2018</conf-date><fpage>4558</fpage><lpage>4567</lpage></element-citation></ref><ref id="B43-sensors-25-01072"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>H.</given-names></name>
<name><surname>Jiang</surname><given-names>L.</given-names></name>
<name><surname>Jia</surname><given-names>J.</given-names></name>
<name><surname>Torr</surname><given-names>P.H.</given-names></name>
<name><surname>Koltun</surname><given-names>V.</given-names></name>
</person-group><article-title>Point transformer</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#x02013;17 October 2021</conf-date><fpage>16259</fpage><lpage>16268</lpage></element-citation></ref><ref id="B44-sensors-25-01072"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kharroubi</surname><given-names>A.</given-names></name>
<name><surname>Ballouch</surname><given-names>Z.</given-names></name>
<name><surname>Hajji</surname><given-names>R.</given-names></name>
<name><surname>Yarroudh</surname><given-names>A.</given-names></name>
<name><surname>Billen</surname><given-names>R.</given-names></name>
</person-group><article-title>Multi-Context Point Cloud Dataset and Machine Learning for Railway Semantic Segmentation</article-title><source>Infrastructures</source><year>2024</year><volume>9</volume><elocation-id>71</elocation-id><pub-id pub-id-type="doi">10.3390/infrastructures9040071</pub-id></element-citation></ref><ref id="B45-sensors-25-01072"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Xiao</surname><given-names>C.</given-names></name>
<name><surname>Wachs</surname><given-names>J.</given-names></name>
</person-group><article-title>Triangle-net: Towards robustness in point cloud learning</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#x02013;8 January 2021</conf-date><fpage>826</fpage><lpage>835</lpage></element-citation></ref><ref id="B46-sensors-25-01072"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yan</surname><given-names>X.</given-names></name>
<name><surname>Zheng</surname><given-names>C.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Cui</surname><given-names>S.</given-names></name>
</person-group><article-title>PointASNL: Robust point clouds processing using nonlocal neural networks with adaptive sampling</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#x02013;19 June 2020</conf-date><fpage>5589</fpage><lpage>5598</lpage></element-citation></ref><ref id="B47-sensors-25-01072"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Taghanaki</surname><given-names>S.A.</given-names></name>
<name><surname>Luo</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>R.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Jayaraman</surname><given-names>P.K.</given-names></name>
<name><surname>Jatavallabhula</surname><given-names>K.M.</given-names></name>
</person-group><article-title>Robustpointset: A dataset for benchmarking robustness of point cloud classifiers</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2011.11572</pub-id></element-citation></ref><ref id="B48-sensors-25-01072"><label>48.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>H.</given-names></name>
<name><surname>Chen</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>W.</given-names></name>
<name><surname>Fang</surname><given-names>H.</given-names></name>
<name><surname>Zhou</surname><given-names>W.</given-names></name>
<name><surname>Yu</surname><given-names>N.</given-names></name>
</person-group><article-title>DUP-Net: Denoiser and upsampler network for 3D adversarial point clouds defense</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#x02013;2 November 2019</conf-date><fpage>1961</fpage><lpage>1970</lpage></element-citation></ref><ref id="B49-sensors-25-01072"><label>49.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Jia</surname><given-names>J.</given-names></name>
<name><surname>Gong</surname><given-names>N.Z.</given-names></name>
</person-group><article-title>PointGuard: Provably robust 3D point cloud classification</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#x02013;25 June 2021</conf-date><fpage>6186</fpage><lpage>6195</lpage></element-citation></ref><ref id="B50-sensors-25-01072"><label>50.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Dong</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>D.</given-names></name>
<name><surname>Zhou</surname><given-names>H.</given-names></name>
<name><surname>Hua</surname><given-names>G.</given-names></name>
<name><surname>Zhang</surname><given-names>W.</given-names></name>
<name><surname>Yu</surname><given-names>N.</given-names></name>
</person-group><article-title>Self-robust 3D point recognition via gather-vector guidance</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#x02013;19 June 2020; 2020</conf-date><fpage>11513</fpage><lpage>11521</lpage></element-citation></ref><ref id="B51-sensors-25-01072"><label>51.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>J.</given-names></name>
<name><surname>Cao</surname><given-names>Y.</given-names></name>
<name><surname>Choy</surname><given-names>C.B.</given-names></name>
<name><surname>Yu</surname><given-names>Z.</given-names></name>
<name><surname>Anandkumar</surname><given-names>A.</given-names></name>
<name><surname>Mao</surname><given-names>Z.M.</given-names></name>
<name><surname>Xiao</surname><given-names>C.</given-names></name>
</person-group><article-title>Adversarially robust 3D point cloud recognition using self-supervisions</article-title><source>Proceedings of the International Conference on Neural Information Processing Systems</source><conf-loc>Virtual</conf-loc><conf-date>6&#x02013;14 December 2021</conf-date><volume>Volume 34</volume><fpage>15498</fpage><lpage>15512</lpage></element-citation></ref><ref id="B52-sensors-25-01072"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Veeramacheneni</surname><given-names>L.</given-names></name>
<name><surname>Valdenegro-Toro</surname><given-names>M.</given-names></name>
</person-group><article-title>A Benchmark for Out of Distribution Detection in Point Cloud 3D Semantic Segmentation</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2211.06241</pub-id></element-citation></ref><ref id="B53-sensors-25-01072"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bhardwaj</surname><given-names>A.</given-names></name>
<name><surname>Pimpale</surname><given-names>S.</given-names></name>
<name><surname>Kumar</surname><given-names>S.</given-names></name>
<name><surname>Banerjee</surname><given-names>B.</given-names></name>
</person-group><article-title>Empowering knowledge distillation via open set recognition for robust 3D point cloud classification</article-title><source>Pattern Recognit. Lett.</source><year>2021</year><volume>151</volume><fpage>172</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2021.07.023</pub-id></element-citation></ref><ref id="B54-sensors-25-01072"><label>54.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Riz</surname><given-names>L.</given-names></name>
<name><surname>Saltori</surname><given-names>C.</given-names></name>
<name><surname>Ricci</surname><given-names>E.</given-names></name>
<name><surname>Poiesi</surname><given-names>F.</given-names></name>
</person-group><article-title>Novel class discovery for 3D point cloud semantic segmentation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#x02013;24 June 2023</conf-date><fpage>9393</fpage><lpage>9402</lpage></element-citation></ref><ref id="B55-sensors-25-01072"><label>55.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>C.</given-names></name>
<name><surname>Abdelzad</surname><given-names>V.</given-names></name>
<name><surname>Mannes</surname><given-names>C.G.</given-names></name>
<name><surname>Rowe</surname><given-names>L.</given-names></name>
<name><surname>Therien</surname><given-names>B.</given-names></name>
<name><surname>Salay</surname><given-names>R.</given-names></name>
<name><surname>Czarnecki</surname><given-names>K.</given-names></name>
</person-group><article-title>Out-of-distribution detection for LiDAR-based 3D object detection</article-title><source>Proceedings of the IEEE International Conference on Intelligent Transportation Systems (ITSC)</source><conf-loc>Macau, China</conf-loc><conf-date>8&#x02013;12 October 2022</conf-date><fpage>4265</fpage><lpage>4271</lpage></element-citation></ref><ref id="B56-sensors-25-01072"><label>56.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Cen</surname><given-names>J.</given-names></name>
<name><surname>Yun</surname><given-names>P.</given-names></name>
<name><surname>Cai</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>M.Y.</given-names></name>
<name><surname>Liu</surname><given-names>M.</given-names></name>
</person-group><article-title>Open-set 3D object detection</article-title><source>Proceedings of the International Conference on 3D Vision (3DV)</source><conf-loc>London, UK</conf-loc><conf-date>1&#x02013;3 December 2021</conf-date><fpage>869</fpage><lpage>878</lpage></element-citation></ref><ref id="B57-sensors-25-01072"><label>57.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kong</surname><given-names>L.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>R.</given-names></name>
<name><surname>Zhang</surname><given-names>W.</given-names></name>
<name><surname>Ren</surname><given-names>J.</given-names></name>
<name><surname>Pan</surname><given-names>L.</given-names></name>
<name><surname>Chen</surname><given-names>K.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
</person-group><article-title>Robo3D: Towards robust and reliable 3D perception against corruptions</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>1&#x02013;6 October 2023</conf-date><fpage>19994</fpage><lpage>20006</lpage></element-citation></ref><ref id="B58-sensors-25-01072"><label>58.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wong</surname><given-names>K.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Ren</surname><given-names>M.</given-names></name>
<name><surname>Liang</surname><given-names>M.</given-names></name>
<name><surname>Urtasun</surname><given-names>R.</given-names></name>
</person-group><article-title>Identifying unknown instances for autonomous driving</article-title><source>Proceedings of the Conference on Robot Learning, PMLR</source><conf-loc>Virtual</conf-loc><conf-date>16&#x02013;18 November 2020</conf-date><fpage>384</fpage><lpage>393</lpage></element-citation></ref><ref id="B59-sensors-25-01072"><label>59.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Alliegro</surname><given-names>A.</given-names></name>
<name><surname>Cappio Borlino</surname><given-names>F.</given-names></name>
<name><surname>Tommasi</surname><given-names>T.</given-names></name>
</person-group><article-title>3DOS: Towards 3D open set learning-benchmarking and understanding semantic novelty detection on point clouds</article-title><source>Proceedings of the International Conference on Neural Information Processing Systems</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>28 November&#x02013;9 December 2022</conf-date><volume>Volume 35</volume><fpage>21228</fpage><lpage>21240</lpage></element-citation></ref><ref id="B60-sensors-25-01072"><label>60.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kumar</surname><given-names>U.R.</given-names></name>
<name><surname>Fayjie</surname><given-names>A.R.</given-names></name>
<name><surname>Hannaert</surname><given-names>J.</given-names></name>
<name><surname>Vandewalle</surname><given-names>P.</given-names></name>
</person-group><article-title>BelHouse3D: A Benchmark Dataset for Assessing Occlusion Robustness in 3D Point Cloud Semantic Segmentation</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV) Workshops</source><conf-loc>Milan, Italy</conf-loc><conf-date>29 September&#x02013;4 October 2024</conf-date></element-citation></ref><ref id="B61-sensors-25-01072"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Homa</surname><given-names>D.</given-names></name>
<name><surname>Cultice</surname><given-names>J.C.</given-names></name>
</person-group><article-title>Role of feedback, category size, and stimulus distortion on the acquisition and utilization of ill-defined categories</article-title><source>J. Exp. Psychol. Learn. Mem. Cogn.</source><year>1984</year><volume>10</volume><fpage>83</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.10.1.83</pub-id></element-citation></ref><ref id="B62-sensors-25-01072"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Donald</surname><given-names>H.</given-names></name>
<name><surname>Joseph</surname><given-names>C.</given-names></name>
<name><surname>Don</surname><given-names>C.</given-names></name>
<name><surname>David</surname><given-names>G.</given-names></name>
<name><surname>Steven</surname><given-names>S.</given-names></name>
</person-group><article-title>Prototype abstraction and classification of new instances as a function of number of instances defining the prototype</article-title><source>J. Exp. Psychol.</source><year>1973</year><volume>101</volume><fpage>116</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1037/h0035772</pub-id></element-citation></ref><ref id="B63-sensors-25-01072"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Minda</surname><given-names>J.P.</given-names></name>
<name><surname>Smith</surname><given-names>J.D.</given-names></name>
</person-group><article-title>Prototypes in category learning: The effects of category size, category structure, and stimulus complexity</article-title><source>J. Exp. Psychol. Learn. Mem. Cogn.</source><year>2001</year><volume>27</volume><fpage>775</fpage><pub-id pub-id-type="doi">10.1037/0278-7393.27.3.775</pub-id><pub-id pub-id-type="pmid">11394680</pub-id>
</element-citation></ref><ref id="B64-sensors-25-01072"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Minda</surname><given-names>J.P.</given-names></name>
<name><surname>Smith</surname><given-names>J.D.</given-names></name>
</person-group><article-title>Comparing prototype-based and exemplar-based accounts of category learning and attentional allocation</article-title><source>J. Exp. Psychol. Learn. Mem. Cogn.</source><year>2002</year><volume>28</volume><fpage>275</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.28.2.275</pub-id><pub-id pub-id-type="pmid">11911384</pub-id>
</element-citation></ref><ref id="B65-sensors-25-01072"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Smith</surname><given-names>J.D.</given-names></name>
<name><surname>Minda</surname><given-names>J.P.</given-names></name>
</person-group><article-title>Prototypes in the mist: The early epochs of category learning</article-title><source>J. Exp. Psychol. Learn. Mem. Cogn.</source><year>1998</year><volume>24</volume><fpage>1411</fpage><lpage>1436</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.24.6.1411</pub-id></element-citation></ref><ref id="B66-sensors-25-01072"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Smith</surname><given-names>J.D.</given-names></name>
<name><surname>Redford</surname><given-names>J.S.</given-names></name>
<name><surname>Haas</surname><given-names>S.M.</given-names></name>
</person-group><article-title>Prototype abstraction by monkeys (Macaca mulatta)</article-title><source>J. Exp. Psychol. Gen.</source><year>2008</year><volume>137</volume><fpage>390</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.137.2.390</pub-id><pub-id pub-id-type="pmid">18473665</pub-id>
</element-citation></ref><ref id="B67-sensors-25-01072"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Posner</surname><given-names>M.I.</given-names></name>
<name><surname>Keele</surname><given-names>S.W.</given-names></name>
</person-group><article-title>On the genesis of abstract ideas</article-title><source>J. Exp. Psychol.</source><year>1968</year><volume>77</volume><fpage>353</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1037/h0025953</pub-id><pub-id pub-id-type="pmid">5665566</pub-id>
</element-citation></ref><ref id="B68-sensors-25-01072"><label>68.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Minda</surname><given-names>J.P.</given-names></name>
<name><surname>Smith</surname><given-names>J.D.</given-names></name>
</person-group><article-title>Prototype models of categorization: Basic formulation, predictions, and limitations</article-title><source>Formal Approaches in Categorization</source><person-group person-group-type="editor">
<name><surname>Pothos</surname><given-names>E.M.</given-names></name>
<name><surname>Wills</surname><given-names>A.J.</given-names></name>
</person-group><publisher-name>Cambridge University Press</publisher-name><publisher-loc>Cambridge, UK</publisher-loc><year>2011</year><fpage>40</fpage><lpage>64</lpage></element-citation></ref><ref id="B69-sensors-25-01072"><label>69.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Snell</surname><given-names>J.</given-names></name>
<name><surname>Swersky</surname><given-names>K.</given-names></name>
<name><surname>Zemel</surname><given-names>R.</given-names></name>
</person-group><article-title>Prototypical networks for few-shot learning</article-title><source>Proceedings of the International Conference on Neural Information Processing Systems</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>4&#x02013;9 December 2017</conf-date><volume>Volume 30</volume><fpage>4080</fpage><lpage>4090</lpage></element-citation></ref><ref id="B70-sensors-25-01072"><label>70.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Sarma</surname><given-names>S.E.</given-names></name>
<name><surname>Bronstein</surname><given-names>M.M.</given-names></name>
<name><surname>Solomon</surname><given-names>J.M.</given-names></name>
</person-group><article-title>Dynamic graph cnn for learning on point clouds</article-title><source>ACM Trans. Graph. (TOG)</source><year>2019</year><volume>38</volume><fpage>146</fpage><pub-id pub-id-type="doi">10.1145/3326362</pub-id></element-citation></ref><ref id="B71-sensors-25-01072"><label>71.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kaul</surname><given-names>C.</given-names></name>
<name><surname>Mitton</surname><given-names>J.</given-names></name>
<name><surname>Dai</surname><given-names>H.</given-names></name>
<name><surname>Murray-Smith</surname><given-names>R.</given-names></name>
</person-group><article-title>Convolutional point transformer</article-title><source>Proceedings of the Asian Conference on Computer Vision</source><conf-loc>Macao, China</conf-loc><conf-date>4&#x02013;8 December 2022</conf-date><fpage>303</fpage><lpage>319</lpage></element-citation></ref><ref id="B72-sensors-25-01072"><label>72.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fayjie</surname><given-names>A.R.</given-names></name>
<name><surname>Vandewalle</surname><given-names>P.</given-names></name>
</person-group><article-title>Few-shot learning on point clouds for railroad segmentation</article-title><source>Electron. Imaging</source><year>2023</year><volume>35</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.2352/EI.2023.35.17.3DIA-100</pub-id></element-citation></ref><ref id="B73-sensors-25-01072"><label>73.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>Urban Spatial Intelligence Research Group at LIESMARS, W.U</collab>
</person-group><article-title>WHU-Railway3D: A Diverse Dataset and Benchmark for Railway Point Cloud Semantic Segmentation</article-title><year>2023</year><comment>Available online: <ext-link xlink:href="https://github.com/WHU-USI3DV/WHU-Railway3D" ext-link-type="uri">https://github.com/WHU-USI3DV/WHU-Railway3D</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-08-27">(accessed on 27 August 2024)</date-in-citation></element-citation></ref><ref id="B74-sensors-25-01072"><label>74.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chicco</surname><given-names>D.</given-names></name>
<name><surname>Jurman</surname><given-names>G.</given-names></name>
</person-group><article-title>The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation</article-title><source>BMC Genom.</source><year>2020</year><volume>21</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.1186/s12864-019-6413-7</pub-id><pub-id pub-id-type="pmid">31898477</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01072-f001"><label>Figure 1</label><caption><p>The experimental setup illustrates three types of distributional shifts: (i) ID shift (red box), (ii) in-domain OOD shift (green box), and (iii) cross-domain OOD shift (blue box) using the Infrabel-5 Railroad Segmentation dataset and the WHU-Railway3D dataset. Class labels are shown in the gray box. Rows represent the training (<inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) and evaluation (<inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) sets for each shift type.</p></caption><graphic xlink:href="sensors-25-01072-g001" position="float"/></fig><fig position="float" id="sensors-25-01072-f002"><label>Figure 2</label><caption><p>Overview of our pipeline. The network is trained on input point clouds (<inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) using batches from a subset of the WHU-Railway3D dataset in a supervised manner (top part, connected via gray arrows). The trained network is then utilized as a feature extractor for FSL given a pair of support&#x02013;query <inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> sets (bottom part, connected via blue arrows). Specifically, we illustrate a 5-way task, producing five foreground prototypes (depicted in red, green, blue, yellow, and sky blue) and one background prototype (depicted in gray). The query feature is then classified to the nearest prototype based on cosine distance.</p></caption><graphic xlink:href="sensors-25-01072-g002" position="float"/></fig><fig position="float" id="sensors-25-01072-f003"><label>Figure 3</label><caption><p>Qualitative results for in-domain OOD shift. The first row shows results for the Infrabel-5 Segmentation dataset, while the second row corresponds to the WHU-Railway3D dataset. In each row, previously unseen foreground classes&#x02014;vegetation and support device&#x02014;are segmented from the background, illustrating the two evaluation settings.</p></caption><graphic xlink:href="sensors-25-01072-g003" position="float"/></fig><fig position="float" id="sensors-25-01072-f004"><label>Figure 4</label><caption><p>Qualitative results under cross-domain OOD shift, evaluated on previously unseen Infrabel-5 Railroad Segmentation dataset. Results are shown for the 1-way 5-shot setting, comparing each model prediction (Pred) to the ground truth (GT). From left to right in two rows, segmentation of four foreground object classes is illustrated: vegetation, overhead line, pole, and support device.</p></caption><graphic xlink:href="sensors-25-01072-g004" position="float"/></fig><fig position="float" id="sensors-25-01072-f005"><label>Figure 5</label><caption><p>Qualitative results under cross-domain OOD shift, evaluated on previously unseen WHU-Railway3D dataset. Results are shown for the 1-way 5-shot setting, comparing each model prediction (Pred) to the ground truth (GT). The first row illustrates two foreground object classes, vegetation and overhead line (from left to right), and the second row shows poles and support devices (from left to right).</p></caption><graphic xlink:href="sensors-25-01072-g005" position="float"/></fig><fig position="float" id="sensors-25-01072-f006"><label>Figure 6</label><caption><p>Distribution of points per class, highlighting the highly imbalanced nature of our datasets: the Infrabel-5 Segmentation dataset (on the <bold>left</bold>) and the WHU-Railway3D dataset (on the <bold>right</bold>).</p></caption><graphic xlink:href="sensors-25-01072-g006" position="float"/></fig><fig position="float" id="sensors-25-01072-f007"><label>Figure 7</label><caption><p>Sample-wise entropy plot for the first 100 samples of each class in the Infrabel-5 Railroad Segmentation dataset (<bold>left</bold>) and the WHU-Railway3D dataset (<bold>right</bold>) under cross-domain OOD shifts. Classes, <monospace>poles</monospace>, <monospace>ground</monospace>, <monospace>vegetation</monospace>, <monospace>overhead lines</monospace>, and <monospace>support devices</monospace> are represented in red, gray, green, purple, and&#x000a0;yellow, respectively. Highly uncertain samples are represented as&#x000a0;triangles.</p></caption><graphic xlink:href="sensors-25-01072-g007" position="float"/></fig><table-wrap position="float" id="sensors-25-01072-t001"><object-id pub-id-type="pii">sensors-25-01072-t001_Table 1</object-id><label>Table 1</label><caption><p>Dataset settings and class configurations used in our experiments on distributional shifts. The&#x000a0;experiments utilize the Infrabel-5 Railroad Segmentation (Infrabel-5) dataset from Belgium (BEL) and the WHU-Railway3D (WHU) dataset from China (CHN).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Shift</th><th align="right" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Exp.</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1"><inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:msub><mml:mi mathvariant="bold-script">D</mml:mi><mml:mi mathvariant="bold-italic">T</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:msub><mml:mi mathvariant="bold-script">D</mml:mi><mml:mi mathvariant="bold-italic">E</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Location</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">ID shift</td><td align="right" valign="middle" rowspan="1" colspan="1">i.</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm146" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Infrabel-5, area: 6, 7, 8</td><td align="center" valign="middle" rowspan="1" colspan="1">BEL</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm147" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Infrabel-5, area: 5 <inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BEL</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="right" valign="middle" rowspan="1" colspan="1">ii.</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm149" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" rowspan="1" colspan="1">WHU, area: L5&#x02013;L7, L10&#x02013;L15, L17&#x02013;L20</td><td align="center" valign="middle" rowspan="1" colspan="1">CHN</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm150" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WHU, area: L14, L16 <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CHN</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">In-domain</td><td align="right" valign="middle" rowspan="1" colspan="1">i.</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm152" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Infrabel-5, <inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: ground, pole, vegetation</td><td align="center" valign="middle" rowspan="1" colspan="1">BEL</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OOD shift</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm154" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Infrabel-5, <inline-formula><mml:math id="mm155" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: cable, support device</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BEL</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="right" valign="middle" rowspan="1" colspan="1">ii.</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm156" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Infrabel-5, <inline-formula><mml:math id="mm157" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: ground, support device, pole</td><td align="center" valign="middle" rowspan="1" colspan="1">BEL</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm158" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Infrabel-5, <inline-formula><mml:math id="mm159" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: pole, vegetation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BEL</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="right" valign="middle" rowspan="1" colspan="1">iii.</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm160" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" rowspan="1" colspan="1">WHU, <inline-formula><mml:math id="mm161" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: ground, rail, track bed, mast</td><td align="center" valign="middle" rowspan="1" colspan="1">CHN</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="right" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">fence, pole, vegetation, building</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm162" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WHU, <inline-formula><mml:math id="mm163" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: cable, support device</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CHN</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="right" valign="middle" rowspan="1" colspan="1">iv.</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm164" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" rowspan="1" colspan="1">WHU, <inline-formula><mml:math id="mm165" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: ground, rail, track bed, mast</td><td align="center" valign="middle" rowspan="1" colspan="1">CHN</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="right" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="left" valign="middle" rowspan="1" colspan="1">fence, cable, support device, building</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm166" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WHU, <inline-formula><mml:math id="mm167" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>: pole, vegetation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CHN</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Cross-domain</td><td align="right" valign="middle" rowspan="1" colspan="1">i.</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm168" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" rowspan="1" colspan="1">Infrabel-5</td><td align="center" valign="middle" rowspan="1" colspan="1">BEL</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OOD shift</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm169" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WHU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CHN</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="right" valign="middle" rowspan="1" colspan="1">ii.</td><td align="left" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm170" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" rowspan="1" colspan="1">WHU</td><td align="center" valign="middle" rowspan="1" colspan="1">CHN</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="right" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm171" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Infrabel-5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BEL</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01072-t002"><object-id pub-id-type="pii">sensors-25-01072-t002_Table 2</object-id><label>Table 2</label><caption><p>Quantitative results under ID shift, where <inline-formula><mml:math id="mm172" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow></mml:math></inline-formula> = {jitter, mirroring, rotation} applied to evaluation set. The&#x000a0;first rows for both datasets with no <inline-formula><mml:math id="mm173" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow></mml:math></inline-formula> (under no-shift scenario) serve as the baselines. Results are reported for 1-way 20-shot tasks. The&#x000a0;best performance metrics are shown in bold, and&#x000a0;the second-best are underlined. Only the model with the lowest predictive uncertainty is highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Evaluation Set</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm174" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi mathvariant="bold-script">X</mml:mi></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">mIoU</italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">OA</italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">MCC</italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm175" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi mathvariant="bold-italic">mH</mml:mi></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Infrabel-5</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02013;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>89.10</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>92.48</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8250</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.0938</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x021d2; baseline</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Infrabel-5</td><td align="left" valign="middle" rowspan="1" colspan="1">jitter</td><td align="left" valign="middle" rowspan="1" colspan="1">
<underline>89.07</underline>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<underline>92.47</underline>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<underline>0.8247</underline>
</td><td align="left" valign="middle" rowspan="1" colspan="1">0.0951</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Infrabel-5</td><td align="left" valign="middle" rowspan="1" colspan="1">mirroring</td><td align="left" valign="middle" rowspan="1" colspan="1">89.02</td><td align="left" valign="middle" rowspan="1" colspan="1">92.43</td><td align="left" valign="middle" rowspan="1" colspan="1">0.8237</td><td align="left" valign="middle" rowspan="1" colspan="1">0.0952</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Infrabel-5</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">rotation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.01</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.42</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8236</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.0938</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WHU</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02013;</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>83.04</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>89.02</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.7714</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.3254</bold>
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x021d2; baseline</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">WHU</td><td align="left" valign="middle" rowspan="1" colspan="1">jitter</td><td align="left" valign="middle" rowspan="1" colspan="1">82.89</td><td align="left" valign="middle" rowspan="1" colspan="1">88.86</td><td align="left" valign="middle" rowspan="1" colspan="1">0.7682</td><td align="left" valign="middle" rowspan="1" colspan="1">0.3261</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">WHU</td><td align="left" valign="middle" rowspan="1" colspan="1">mirroring</td><td align="left" valign="middle" rowspan="1" colspan="1">
<underline>82.95</underline>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<underline>88.97</underline>
</td><td align="left" valign="middle" rowspan="1" colspan="1">
<underline>0.7701</underline>
</td><td align="left" valign="middle" rowspan="1" colspan="1">0.3255</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WHU</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">rotation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.94</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.96</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7700</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3256</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01072-t003"><object-id pub-id-type="pii">sensors-25-01072-t003_Table 3</object-id><label>Table 3</label><caption><p>Quantitative results for the in-domain OOD shift scenario based on two evaluation settings using the Infrabel-5 Railroad Segmentation dataset. Results are reported for <inline-formula><mml:math id="mm176" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The&#x000a0;highest performance metrics and the lowest predictive uncertainty for each setting are highlighted in bold. Higher values indicate better performance for mIoU, OA, and MCC (&#x02191;), while lower values indicate better performance for mH (&#x02193;).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">{<monospace>pole</monospace>, <monospace>vegetation</monospace>}</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">{<monospace>cable</monospace>, <monospace>support device</monospace>}</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Metric</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm177" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">5</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm178" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">10</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm179" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">20</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm180" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">5</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm181" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">10</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm182" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">20</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">mIoU &#x02191;</td><td align="center" valign="middle" rowspan="1" colspan="1">52.01</td><td align="center" valign="middle" rowspan="1" colspan="1">56.70</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>64.17</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">86.56</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>87.65</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">86.08</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OA &#x02191;</td><td align="center" valign="middle" rowspan="1" colspan="1">64.32</td><td align="center" valign="middle" rowspan="1" colspan="1">68.92</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>74.38</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">91.32</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>92.46</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">91.18</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MCC &#x02191;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2804</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3786</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.4708</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8091</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.8292</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8086</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mH &#x02193;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4914</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5002</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.4754</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.0938</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1149</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1007</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01072-t004"><object-id pub-id-type="pii">sensors-25-01072-t004_Table 4</object-id><label>Table 4</label><caption><p>Quantitative results for the in-domain OOD shift scenario based on two evaluation settings using the WHU-Railway3D dataset. Results are reported for <inline-formula><mml:math id="mm183" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The&#x000a0;highest performance metrics and the lowest predictive uncertainty for each setting are highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">{<monospace>pole</monospace>, <monospace>vegetation</monospace>}</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">{<monospace>cable</monospace>, <monospace>support device</monospace>}</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Metric</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm184" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">5</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm185" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">10</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm186" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">20</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm187" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">5</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm188" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">10</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm189" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">20</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">mIoU &#x02191;</td><td align="center" valign="middle" rowspan="1" colspan="1">75.31</td><td align="center" valign="middle" rowspan="1" colspan="1">75.41</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>76.03</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">56.46</td><td align="center" valign="middle" rowspan="1" colspan="1">56.61</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>59.12</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OA &#x02191;</td><td align="center" valign="middle" rowspan="1" colspan="1">77.91</td><td align="center" valign="middle" rowspan="1" colspan="1">77.63</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>78.46</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">82.26</td><td align="center" valign="middle" rowspan="1" colspan="1">82.38</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>83.87</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MCC &#x02191;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3828</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3520</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.3841</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6441</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6449</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.6673</bold>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mH &#x02193;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.3315</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4803</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6312</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4809</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.4716</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5321</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01072-t005"><object-id pub-id-type="pii">sensors-25-01072-t005_Table 5</object-id><label>Table 5</label><caption><p>Quantitative results under cross-domain OOD shift, with&#x000a0;the WHU-Railway3D dataset as the training set and the Infrabel-5 Railroad Segmentation dataset as the evaluation set. The&#x000a0;best performance metrics and lowest predictive uncertainty for <inline-formula><mml:math id="mm190" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are shown in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">
<inline-formula>
<mml:math id="mm191" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">
<inline-formula>
<mml:math id="mm192" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Metric</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm193" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">5</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm194" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">10</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm195" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">20</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm196" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">5</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm197" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">10</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm198" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">20</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">mIoU &#x02191;</td><td align="center" valign="middle" rowspan="1" colspan="1">73.05</td><td align="center" valign="middle" rowspan="1" colspan="1">76.65</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>77.48</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">46.72</td><td align="center" valign="middle" rowspan="1" colspan="1">46.02</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>49.98</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OA &#x02191;</td><td align="center" valign="middle" rowspan="1" colspan="1">80.67</td><td align="center" valign="middle" rowspan="1" colspan="1">83.80</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>84.83</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">66.49</td><td align="center" valign="middle" rowspan="1" colspan="1">66.43</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>70.58</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MCC &#x02191;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5867</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6523</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.6764</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5534</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5515</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.6022</bold>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mH &#x02193;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3437</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.2108</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2358</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7589</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7784</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.7418</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01072-t006"><object-id pub-id-type="pii">sensors-25-01072-t006_Table 6</object-id><label>Table 6</label><caption><p>Quantitative results under cross-domain OOD shift, with&#x000a0;the Infrabel-5 Railroad Segmentation dataset as the training set and the WHU-Railway3D dataset as the evaluation set. The&#x000a0;best performance metrics and lowest predictive uncertainty for <inline-formula><mml:math id="mm199" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are shown in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">
<inline-formula>
<mml:math id="mm200" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">
<inline-formula>
<mml:math id="mm201" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Metric</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm202" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">5</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm203" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">10</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm204" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">20</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm205" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">5</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm206" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">10</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm207" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="bold-italic">20</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">mIoU &#x02191;</td><td align="center" valign="middle" rowspan="1" colspan="1">70.94</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>71.73</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">70.29</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">43.58</td><td align="center" valign="middle" rowspan="1" colspan="1">45.48</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>48.02</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OA &#x02191;</td><td align="center" valign="middle" rowspan="1" colspan="1">81.75</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>82.41</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">81.37</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">58.43</td><td align="center" valign="middle" rowspan="1" colspan="1">60.83</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>62.46</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MCC &#x02191;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6227</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.6371</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6153</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4813</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5123</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.5367</bold>
</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mH &#x02193;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2385</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.2173</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2901</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7798</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7278</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.5514</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01072-t007"><object-id pub-id-type="pii">sensors-25-01072-t007_Table 7</object-id><label>Table 7</label><caption><p>Quantitative results on WHU-Railway3D dataset and Infrabel-5 Railroad Segmentation dataset using fully supervised setting.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Evaluation Set</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">mIoU</italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">OA</italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">MCC</italic>
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm208" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi mathvariant="bold-italic">mH</mml:mi></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Infrabel-5</td><td align="center" valign="middle" rowspan="1" colspan="1">60.94</td><td align="center" valign="middle" rowspan="1" colspan="1">82.37</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6864</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1261</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WHU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8854</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1709</td></tr></tbody></table></table-wrap></floats-group></article>