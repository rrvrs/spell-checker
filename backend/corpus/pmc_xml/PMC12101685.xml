<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408329</article-id><article-id pub-id-type="pmc">PMC12101685</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0324048</article-id><article-id pub-id-type="publisher-id">PONE-D-24-60568</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Grammar</subject><subj-group><subject>Phonology</subject><subj-group><subject>Phonemes</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Speech Signal Processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Learning</subject><subj-group><subject>Human Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Audio Signal Processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Cognitive Linguistics</subject><subj-group><subject>Word Recognition</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject><subj-group><subject>Recurrent Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject><subj-group><subject>Recurrent Neural Networks</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>A study on phonemes recognition method for Mandarin pronunciation based on improved Zipformer-RNN-T(Pruned) modeling</article-title><alt-title alt-title-type="running-head">Phonemes recognition method for mandarin pronunciation</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0007-4902-7174</contrib-id><name><surname>Du</surname><given-names>Zhaohui</given-names></name><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Zhao</surname><given-names>Xiaofeng</given-names></name><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Lin</given-names></name><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0727-3485</contrib-id><name><surname>Yu</surname><given-names>Baohua</given-names></name><role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Miao</surname><given-names>Lijiang</given-names></name><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>School of Information Science and Technology, Shihezi University, Shihezi, China</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Xinjiang Uygur Autonomous Region Education Examination Centre, Urumqi, China</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Zhang</surname><given-names>Jie</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Newcastle University, UNITED KINGDOM OF GREAT BRITAIN AND NORTHERN IRELAND</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>ybh_inf@shzu.edu.cn</email> (BY); <email>miaolijiang00@163.com</email> (LM)</corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0324048</elocation-id><history><date date-type="received"><day>30</day><month>12</month><year>2024</year></date><date date-type="accepted"><day>17</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Du et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Du et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0324048.pdf"/><abstract><p>In recent years, empowered by artificial intelligence technologies, computer-assisted language learning systems have gradually become a hot topic of research. Currently, the mainstream pronunciation assessment models rely on advanced speech recognition technology, converting speech into phoneme sequences, and then determining mispronounced phonemes through sequence comparison. To optimize the phoneme recognition task in pronunciation evaluation, this paper proposes a Chinese pronunciation phoneme recognition model based on the improved Zipformer-RNN-T(Pruned) architecture, aiming to improve recognition accuracy and reduce parameter count. First, the AISHELL1-PHONEME and ST-CMDS-PHONEME datasets for Mandarin phoneme recognition through data preprocessing. Then, three layers of the Zipformer Block architecture are introduced into the Zipformer encoder to significantly enhance model performance. In the stateless Pred Network, the GELU activation function is adopted to effectively prevent neuron deactivation. Furthermore, a hybrid Pruned RNN-T/CTC Loss fusion strategy is proposed, further optimizing recognition performance. The experimental results demonstrate that the method performs excellently in the phoneme recognition task, achieving a Word Error Rate (WER) of 1.92% (Dev) and 2.12% (Test) on the AISHELL1-PHONEME dataset, and 4.28% (Dev) and 4.51% (Test) on the ST-CMDS-PHONEME dataset. Moreover, the model requires only 61.1M parameters, striking a balance between performance and efficiency.</p></abstract><funding-group><award-group id="award001"><funding-source><institution>A Data-driven Regional Smart Education Service Key Technology Research and Application Demonstration</institution>
</funding-source><award-id>2021AB023</award-id><principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0727-3485</contrib-id>
<name><surname>Yu</surname><given-names>Baohua</given-names></name>
</principal-award-recipient></award-group><funding-statement>This research was funded by [the Bing-tuan Science and Technology Public Relations Project &#x0201c;A Data-driven Regional Smart Education Service Key Technology Research and Application Demonstration&#x0201d;] grantnumber [2021AB023]. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="13"/><table-count count="9"/><page-count count="28"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>The data used in this study is publicly available on the AISHELL platform. The dataset includes. 1. AISHELL1 dataset retrieval address: <ext-link xlink:href="https://www.aishelltech.com/kysjcp" ext-link-type="uri">https://www.aishelltech.com/kysjcp</ext-link>. 2. MUSAN dataset retrieval address: <ext-link xlink:href="https://www.openslr.org/17/" ext-link-type="uri">https://www.openslr.org/17/</ext-link>. The dataset includes various types of speech used for model training, validation, and evaluation.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>The data used in this study is publicly available on the AISHELL platform. The dataset includes. 1. AISHELL1 dataset retrieval address: <ext-link xlink:href="https://www.aishelltech.com/kysjcp" ext-link-type="uri">https://www.aishelltech.com/kysjcp</ext-link>. 2. MUSAN dataset retrieval address: <ext-link xlink:href="https://www.openslr.org/17/" ext-link-type="uri">https://www.openslr.org/17/</ext-link>. The dataset includes various types of speech used for model training, validation, and evaluation.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>1 Introduction</title><p>With the increasing maturity of speech recognition technology and the growing popularity of the &#x0201c;Chinese language fever,&#x0201d; how to leverage speech recognition technology to more effectively assess learners&#x02019; Chinese pronunciation proficiency has become a research hotspot in the field of education. Traditional assessment methods rely on manual evaluation, which is not only time-consuming and labor-intensive but also lacks a consistent and unified evaluation standard. To overcome these limitations, researchers have applied pronunciation assessment technology to recognize learners&#x02019; pronunciation, extracting key features from the speech data. Based on these key features, an analysis of the learners&#x02019; pronunciation is conducted, identifying specific errors in their speech. Currently, pronunciation assessment methods can be classified into three categories: feature-based pronunciation assessment, GOP (Graphical Output Probability)-based pronunciation assessment, and speech recognition-based pronunciation assessment [<xref rid="pone.0324048.ref001" ref-type="bibr">1</xref>,<xref rid="pone.0324048.ref002" ref-type="bibr">2</xref>]. Among these, speech recognition-based pronunciation assessment methods are characterized by low resource consumption, low latency, and high performance, making them the mainstream evaluation approach. The core of this method lies in utilizing advanced speech recognition technology to construct phoneme recognition models. These models can convert learners&#x02019; pronunciations into phoneme sequences, thereby enabling precise analysis of pronunciation details. Phoneme recognition, as a prerequisite task for mainstream pronunciation assessment methods, plays a significant role in advancing the development of the pronunciation evaluation field and improving learners&#x02019; pronunciation skills [<xref rid="pone.0324048.ref003" ref-type="bibr">3</xref>].</p><p>Phoneme recognition is the process of identifying phoneme units using advanced speech recognition technology. In recent years, the significant advancements in speech recognition technology have been largely driven by the development of deep learning. With the introduction of deep neural networks (DNN) [<xref rid="pone.0324048.ref004" ref-type="bibr">4</xref>] and convolutional neural networks (CNN) [<xref rid="pone.0324048.ref005" ref-type="bibr">5</xref>], the ability of speech recognition systems to model speech time-series information has been significantly enhanced. In addition, the introduction of end-to-end models such as CTC [<xref rid="pone.0324048.ref006" ref-type="bibr">6</xref>], RNN-T [<xref rid="pone.0324048.ref007" ref-type="bibr">7</xref>], and LAS [<xref rid="pone.0324048.ref008" ref-type="bibr">8</xref>] has further simplified the complexity of the models, enabling speech recognition systems to exhibit stronger performance when processing long sequences. With the advancement of speech recognition technology, phoneme recognition technology has also been correspondingly improved. Researchers have suggested various deep learning-based phoneme recognition models, such as temporal modeling methods based on recurrent neural networks (RNN) [<xref rid="pone.0324048.ref009" ref-type="bibr">9</xref>], and self-attention mechanisms based on Transformers [<xref rid="pone.0324048.ref010" ref-type="bibr">10</xref>]. Despite the significant advancements made by the aforementioned methods in the field of speech recognition, their adaptability to Chinese phoneme recognition tasks remains inadequate. Existing mainstream models often have large parameter counts and complex structures, making them difficult to deploy in practical environments that require high resource efficiency and quick response times, such as language learning applications, online evaluation platforms, and mobile devices. Therefore, there is an urgent need to develop a lightweight model that combines strong modeling capabilities with a low real-time factor, which is crucial for the advancement of Chinese phoneme recognition systems.</p><p>Currently, academic research primarily focuses on phoneme recognition for English and other Western languages, while research on Chinese phoneme recognition is relatively limited. This situation has resulted in a scarcity of high-quality datasets specifically for Chinese phoneme recognition. Compared to other languages, the phonetic system of Chinese is significantly more complex. Firstly, Chinese contains a large number of consonants and vowels that are phonetically similar, which makes them difficult to distinguish and prone to confusion. Secondly, tones play a crucial role in the semantic structure of Chinese, with the same syllable having completely different meanings depending on the tone [<xref rid="pone.0324048.ref011" ref-type="bibr">11</xref>]. For example, &#x0201c;ma&#x0201d; represents &#x0201c;mother&#x0201d; &#x0201c;hemp&#x0201d; &#x0201c;horse&#x0201d;, and &#x0201c;scold&#x0201d; in the first to fourth tones, respectively. This reliance on tones to distinguish meanings greatly increases the difficulty of fine-grained modeling in Chinese phoneme recognition systems. Meanwhile, in practical application scenarios such as language learning and speech evaluation platforms, Chinese phoneme recognition systems also face the demand for strong real-time feedback. Learners typically expect immediate feedback on their pronunciation as soon as they finish speaking, so they can promptly adjust their pronunciation. This human-computer interaction scenario imposes a very low real-time factor (RTF) requirement on the system to ensure a smooth and natural user experience. Therefore, Chinese phoneme recognition tasks face two main challenges: first, the high requirement for modeling accuracy due to subtle phonetic differences such as tones; second, the strict limitations on inference speed and response delay in application scenarios. This means that phoneme recognition models must not only have the ability to model complex phonetic features accurately but also complete fast inference and output within a limited time budget to maintain efficient interactivity and practicality.</p><p>To address these challenges, this paper adopts the Zipformer architecture, which has shown outstanding performance in the field of speech recognition in recent years. Compared to the traditional Transformer, Zipformer significantly reduces computational complexity while maintaining its ability to model temporal information through a time-compression mechanism. In contrast to RNN-based models, it demonstrates higher efficiency in handling long-range dependencies and parallel computation. Additionally, Zipformer&#x02019;s multi-layer compression and decompression structure can accommodate the modeling needs of multi-dimensional information such as consonants, vowels, and tones in Chinese pronunciation, helping to capture subtle phonetic differences and trends. Moreover, Zipformer&#x02019;s modular structure and excellent scalability make it easier to implement fast inference in online pronunciation evaluation systems, meeting the requirements for real-time feedback. Therefore, in the task of Chinese phoneme recognition, Zipformer strikes a superior balance in terms of accuracy, latency, and model size compared to other mainstream models, making it the preferred model framework for this research.</p><p>Based on the aforementioned issues and challenges, the contributions of this paper are as follows:</p><list list-type="bullet"><list-item><p>In view of the scarcity of Chinese phoneme recognition data resources, this paper is based on the Mandarin speech dataset AISHELL-1 and ST-CMDS. It utilizes the mapping relationship between Chinese characters and phonemes to replace the original character labels with phoneme labels. To address the pronunciation variations of polyphonic characters in different phrases or contexts, a phrase-level phoneme mapping table is used for label substitution, rather than character-by-character replacement. This results in the construction of the AISHELL1-PHONEME dataset and the ST-CMDS-PHONEME dataset, which is labeled with Chinese phonemes and provides data support for subsequent phoneme recognition model training</p></list-item><list-item><p>In terms of the encoder, this paper proposes a deep Zipformer Block module based on the Zipformer model. Through experimental analysis of the original Zipformer paper, it was found that the number of Encoder Blocks is positively correlated with both the model&#x02019;s parameter size and recognition accuracy. That is, increasing the number of Encoder Blocks improves recognition accuracy but also increases the number of parameters. To address this, this paper designs a three-layer Zipformer Block structure, aiming to maintain recognition accuracy while reducing the number of parameters.</p></list-item><list-item><p>In terms of the decoder, this paper proposes a GELU-based Pred Network module. To address the issue where the ReLU activation function in the stateless Pred Network may lead to neuron deactivation, this paper introduces the GELU activation function, which has demonstrated excellent performance in natural language processing. Unlike ReLU, GELU smooths and filters the input values through a Gaussian distribution, avoiding the drawback of ReLU&#x02019;s direct truncation of negative values. This enhances sensitivity to input variations and improves the network&#x02019;s expressive capacity and performance stability.</p></list-item><list-item><p>In terms of loss calculation, this paper proposes a hybrid Pruned RNN-T/CTC loss function. Based on the Pruned RNN-T loss function (including Simple Loss and Pruned Loss), the CTC loss function is introduced to enhance the model&#x02019;s adaptability to long sequences and variable-length inputs. By experimentally adjusting the weighting ratio, the Pruned RNN-T loss and the CTC loss are combined through weighted fusion to optimize the model&#x02019;s recognition accuracy.</p></list-item></list><p>The structure of this paper is as follows: <xref rid="sec002" ref-type="sec">Sect 2</xref> primarily introduces the related applications and research on phoneme recognition technology. <xref rid="sec006" ref-type="sec">Sect 3</xref> presents the methods proposed in this paper. <xref rid="sec010" ref-type="sec">Sect 4</xref> includes comparative experiments, simulated noise experiments, ablation studies, inference experiments, and Chinese phoneme analysis. Finally, <xref rid="sec022" ref-type="sec">Sect 5</xref> concludes the paper.</p></sec><sec id="sec002"><title>2 Related works</title><p>Phoneme recognition technology is becoming increasingly important in pronunciation error detection. It relies on highly accurate speech recognition technology to achieve efficient conversion from speech to phonemes, providing a foundation for speech assessment and language teaching. This section will explore the research and applications of phoneme recognition, detailing the progress of English phoneme recognition technology, and discussing Mandarin phoneme recognition technology along with confusion analysis in practical applications.</p><sec id="sec003"><title>2.1 Applications of phoneme recognition</title><p>Phoneme-level pronunciation accuracy assessment methods are widely applied in the field of pronunciation evaluation. Leung et al. proposed an end-to-end phoneme recognition system based on CNN-RNN-CTC, combining convolutional neural networks (CNN), recurrent neural networks (RNN), and connectionist temporal classification (CTC) technology [<xref rid="pone.0324048.ref012" ref-type="bibr">12</xref>], for detecting and diagnosing mispronunciations in second language learners. Building on this, Zhang et al. proposed an end-to-end automatic pronunciation error detection system based on an improved hybrid CTC/Attention architecture by combining the advantages of CTC (Connectionist Temporal Classification) and attention mechanisms [<xref rid="pone.0324048.ref013" ref-type="bibr">13</xref>]. This system effectively avoids the complex module segmentation and forced alignment requirements commonly found in traditional speech recognition systems. Meanwhile, Yan et al. focused on pronunciation error detection for second language (L2) English learners and proposed an end-to-end approach based on a novel anti-phone modeling technique [<xref rid="pone.0324048.ref014" ref-type="bibr">14</xref>]. Traditional ASR methods often struggle to accurately recognize approximate or distorted pronunciations between native language (L1) and target language (L2) phonemes. The system proposed in this paper enhances the system&#x02019;s comprehensive recognition ability for both categorical and non-categorical mispronunciations by expanding the original L2 phoneme set to include the corresponding anti-phone set.</p></sec><sec id="sec004"><title>2.2 English phoneme recognition</title><p>Speech recognition-based English phoneme recognition technology has developed rapidly. With the rise of deep learning techniques, convolutional neural networks (CNN) have been introduced into phoneme recognition. Abdel-Hamid et al. proposed a CNN-based acoustic model [<xref rid="pone.0324048.ref015" ref-type="bibr">15</xref>], which significantly enhanced the model&#x02019;s adaptability to complex acoustic environments. CNNs excel in phoneme recognition tasks by automatically extracting local features to capture important patterns in audio signals. In addition, T&#x000f3;th et al. used a hierarchical convolutional deep Maxout network [<xref rid="pone.0324048.ref016" ref-type="bibr">16</xref>], which further enhanced the model&#x02019;s ability to recognize phonemes. Building upon convolutional networks, Passricha et al. introduced Convolutional Support Vector Machines (CSVM) [<xref rid="pone.0324048.ref017" ref-type="bibr">17</xref>], attempting to combine the benefits of convolutional feature extraction and support vector machines to improve phoneme recognition performance. CSVM integrates the feature extraction capabilities of convolutional layers with the classification power of SVM, providing a new approach to phoneme recognition. Ravanelli et al. proposed SincNet [<xref rid="pone.0324048.ref018" ref-type="bibr">18</xref>], an interpretable convolutional filter model that better captures frequency information in audio signals. SincNet improves frequency feature modeling by using the Sinc function as a filter in the convolutional layers. In the integration of convolutional and recurrent networks, Zhao et al. proposed a Recurrent Convolutional Neural Network (RCNN) [<xref rid="pone.0324048.ref019" ref-type="bibr">19</xref>], which leverages the advantages of both convolutional and recurrent layers in processing speech signals, thereby improving phoneme recognition accuracy.</p></sec><sec id="sec005"><title>2.3 Chinese phoneme recognition</title><p>Mandarin phoneme recognition and confusion analysis provide targeted teaching guidance for Chinese language teachers. Ding et al. collected learners&#x02019; self-reported pronunciation difficulties and coping strategies through methods such as questionnaires, reading tasks, and recall interviews [<xref rid="pone.0324048.ref020" ref-type="bibr">20</xref>]. The study found that learners generally face challenges in consonants, vowels, and tones during pronunciation, but there is a discrepancy between self-reports and actual performance. Jiang et al. explored the correlation between the pronunciation of Mandarin consonants and the distance in speech signals, revealing the causes of consonant pronunciation confusion from the perspective of signal processing [<xref rid="pone.0324048.ref021" ref-type="bibr">21</xref>]. The experimental results show that confusable consonant pairs (such as /l/ and /n/) have the shortest signal distance at the signal level, which aligns with the fact that they are prone to confusion during pronunciation. Arkin et al. conducted an in-depth analysis of the phonemes in the pronunciation of 50 Uyghur native speakers learning Mandarin Chinese using automatic speech recognition technology [<xref rid="pone.0324048.ref022" ref-type="bibr">22</xref>]. The study found that Uyghur students often misidentify the target phoneme &#x0201c;a&#x0201d; as &#x0201c;e&#x0201d; or &#x0201c;i&#x0201d; when speaking Mandarin, and misidentify &#x0201c;i&#x0201d; as &#x0201c;e&#x0201d; or &#x0201c;ie&#x0201d;.</p></sec></sec><sec id="sec006"><title>3 The proposed model</title><p>Given the complexity of Chinese pronunciation and the need for rapid feedback in phoneme recognition, and considering that the phoneme recognition model needs to be deployed on resource-constrained devices, this paper proposes an end-to-end Chinese phoneme recognition model based on the improved Zipformer-RNN-T(Pruned). The model combines the efficient Zipformer encoder with a pruned and optimized RNN-T decoder. Compared to traditional models, this model has advantages in terms of recognition accuracy, model parameters, and computational speed. The overall structure of the model is shown in <xref rid="pone.0324048.g001" ref-type="fig">Fig 1</xref>, which is primarily divided into two parts: the encoder and the decoder. First, after feature extraction, the generated FBank features are input into the Zipformer encoder. The first stage of the encoder uses the Conv2-Embed module to downsample the input 100 Hz audio signal to 50 Hz. Then, the Zipformer-Blocks module models the features at a frame rate of 50 Hz. Subsequently, the features undergo further processing through five Encoder-Block module groups, each of which sequentially performs downsampling, Zipformer-Blocks, upsampling, and Bypass modules, with frame rates of 25 Hz, 12.5 Hz, 6.25 Hz, 12.5 Hz, and 25 Hz, respectively. Finally, the features are downsampled to 25 Hz for output. In the decoder section, the input consists of the features processed by the encoder (am) and the phoneme labels. First, the phoneme labels are processed through the Pred Network, producing label features (lm). Then, am and lm are input into the Trivial Joiner module, which calculates the Simple Loss. Based on am, lm, and Simple Loss, the pruning boundary is computed to obtain Pruned Am and Pruned Lm. Finally, Pruned Am and Pruned Lm are input into the Joiner Network for fusion, and the Pruned Loss and CTC Loss are calculated separately. To enhance the model&#x02019;s performance, this paper proposes improvements to the Zipformer Block and Pred Network, and introduces a hybrid Pruned RNN-T/CTC Loss strategy that integrates Simple Loss, Pruned Loss, and CTC Loss to optimize the end-to-end training process. Through this enhancement, the model is able to effectively learn the mapping relationship between audio features and phoneme labels, thereby achieving efficient Chinese phoneme recognition.</p><fig position="float" id="pone.0324048.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.g001</object-id><label>Fig 1</label><caption><title>The overall structure of the Zipformer-RNN-T(Pruned) model.</title></caption><graphic xlink:href="pone.0324048.g001" position="float"/></fig><sec id="sec007"><title>3.1 Deep Zipformer block</title><p>The Zipformer encoder models features across different speech frames, which results in a relatively large number of parameters while maintaining performance. To address this issue, this paper optimizes the Zipformer model by reducing the number of Encoder Blocks to decrease the overall model size. Additionally, a strategy is introduced that reuses the attention weights calculated by MHAW (Multi-Head Attention Weight) to save computational resources. The Zipformer Block&#x02019;s two-layer SCF (Self-Attention, Convolution, and Feed-forward) module group is expanded to three layers, aiming to improve the overall performance of the model by increasing its depth.</p><p>The improved Zipformer Block is shown in <xref rid="pone.0324048.g002" ref-type="fig">Fig 2</xref>. The speech features are input into the MHAW module to compute the attention weights, which are then passed into the Self Attention module and the NLA (Non-Linear Attention) module within the SCF module group. Meanwhile, the speech features are also fed into the Feed Forward module, processed by the NLA module, and then passed through three consecutive SCF module groups. Finally, the output is normalized by the BiasNorm module.</p><fig position="float" id="pone.0324048.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.g002</object-id><label>Fig 2</label><caption><title>Structure of the Improved Zipformer Block Module.</title></caption><graphic xlink:href="pone.0324048.g002" position="float"/></fig><p>The MHAW module maps the input embedding to Query, Key and relative position vectors through linear projection. It calculates the attention scores using matrix multiplication, models the sequence position relationships by incorporating relative position embeddings, and normalizes the scores into attention weights via softmax, which are then used for downstream sequence modeling tasks. The process can be described as follows:</p><disp-formula id="pone.0324048.e001"><alternatives><graphic xlink:href="pone.0324048.e001.jpg" id="pone.0324048.e001g" position="anchor"/><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mi>&#x000b7;</mml:mi><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>&#x000b7;</mml:mi><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>_</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(1)</label></disp-formula><p>Where <italic toggle="yes">Q</italic>, <italic toggle="yes">K</italic> are the query and key obtained by linear projection of the input tensor, respectively. <inline-formula id="pone.0324048.e002"><alternatives><graphic xlink:href="pone.0324048.e002.jpg" id="pone.0324048.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>_</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the relative position encoding with shape (batch_size, 2*seq_len-1, embed_dim). <inline-formula id="pone.0324048.e003"><alternatives><graphic xlink:href="pone.0324048.e003.jpg" id="pone.0324048.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>_</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is a linear transformation that maps the original relative position embedding to the required dimension of each attention head, providing independent position information for each head computation. <italic toggle="yes">Masking</italic> is used to mask invalid positions, preventing the model from focusing on information that fills the region or future time steps.</p><p>The weights computed through the MHAW module are fed into the Non Linear Attrntion module and the SCF module groups, respectively. In this case, the Non Linear Attention module aggregates the embedding vectors on the time axis using the attention weight values computed by MHAW. The module splits the input features into three parts by linear projection: <italic toggle="yes">s</italic>, <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>. <italic toggle="yes">s</italic> is input to the tanh activation function after Balancer processing, and <italic toggle="yes">x</italic> is normalised and multiplied with the processed <italic toggle="yes">s</italic> at the element level. Subsequently, the features are weighted by pre-computed attention weights to complete the aggregation of sequence information. The aggregated features are recovered to the input dimension by linear projection and normalised to generate the final output features. The specific formula is as follows:</p><disp-formula id="pone.0324048.e004"><alternatives><graphic xlink:href="pone.0324048.e004.jpg" id="pone.0324048.e004g" position="anchor"/><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>*</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02299;</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02299;</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(2)</label></disp-formula><p>Where, <italic toggle="yes">whiten</italic> is a data normalisation technique used to reduce the correlation between the input features. <italic toggle="yes">ScaledLinear</italic> is a scaling based linear transformation layer. <italic toggle="yes">tanh</italic> is a nonlinear activation function. <italic toggle="yes">Balancer</italic> is used to limit the activation values to prevent training instability.</p><p>In this paper, groups of three serially connected SCF modules are cascaded, and a Bypass module is embedded between each group of SCF modules with additional inputs of MHAW-calculated Attention Weights and Non Linear Attention output values.The Bypass module is specifically described as:</p><disp-formula id="pone.0324048.e005"><alternatives><graphic xlink:href="pone.0324048.e005.jpg" id="pone.0324048.e005g" position="anchor"/><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02299;</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mo>&#x02299;</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(3)</label></disp-formula><p>where <italic toggle="yes">x</italic> is the input value of the previous module, <italic toggle="yes">y</italic> is the output value of the previous module, and <italic toggle="yes">c</italic> is the learning channel scalar weight.</p><p>The SCF module group includes Self Attention, Convolution and Feed Forword respectively. <xref rid="pone.0324048.g003" ref-type="fig">Fig 3</xref> shows Self Attention.The module requires input features <italic toggle="yes">X</italic> and attention weights <italic toggle="yes">W</italic>. Firstly, the features <italic toggle="yes">X</italic> are mapped to <italic toggle="yes">X</italic><sub><italic toggle="yes">linear</italic></sub> through a linear layer, after which the attention weights <italic toggle="yes">W</italic> and <italic toggle="yes">X</italic><sub><italic toggle="yes">linear</italic></sub> are computed by matrix multiplication. The resulting results are processed through <italic toggle="yes">Whiten</italic> in order to adjust the covariance of the features during backpropagation to be closer to the unit matrix for better feature decorrelation and improved generalisation of the model. Convolution uses a standard one-dimensional deep convolution module with symmetric padding at the end of the sequence in order to keep the output size constant. Feed Forword uses a feed-forward neural network layer to achieve deep processing and decorrelation of input features.</p><fig position="float" id="pone.0324048.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.g003</object-id><label>Fig 3</label><caption><title>Structure of Self Attention module using input weights in this paper.</title></caption><graphic xlink:href="pone.0324048.g003" position="float"/></fig></sec><sec id="sec008"><title>3.2 GELU-based pred network module</title><p>RNN-T models perform well in tasks such as speech recognition, but the complexity and number of parameters of their model structure for processing labels is large, leading to difficulties when applied on resource-constrained devices. To address the challenge of reducing the model size in RNN-T architectures without significantly degrading the recognition performance, Ghodsi et al. proposed the stateless Pred Network [<xref rid="pone.0324048.ref023" ref-type="bibr">23</xref>], which simplifies the model structure by removing the recurrent layer in the Pred Network while maintaining the performance of the model. However, the limitation of neuron deactivation occurs when the inputs to the ReLu activation function in this model are negative, i.e., the outputs are forced to zero, which restricts its ability to handle complex data patterns. In order to solve this problem, this paper improves the stateless Pred Network by introducing the GELU (Gaussian Error Linear Unit) activation function, which makes the activation function more sensitive to changes in the inputs, and exhibits better performance and generalisation ability when dealing with complex high-dimensional data.</p><p><xref rid="pone.0324048.g004" ref-type="fig">Fig 4</xref> shows the structure of the improved stateless Pred network, transform text label sequences into a continuous feature space through embedding layers, the Balancer is used to control the distribution of activation values to avoid the negative impact of extreme values on the model training, and then a one-dimensional convolutional layer is used to capture local information in the sequence, and the GELU activation function is used to introduce the nonlinearities to help the model extract more complex features. The GELU activation function is used to introduce nonlinearities and help the model extract more complex features. The activated feature vector is passed through the Balancer layer again to further adjust the range of values in the output, and the final output is a vector with the shape of (N, U, decoder_dim). This structure ensures that the output vector captures the features of the input text while remaining numerically stable.</p><fig position="float" id="pone.0324048.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.g004</object-id><label>Fig 4</label><caption><title>Improved Pred Network module structure.</title></caption><graphic xlink:href="pone.0324048.g004" position="float"/></fig><p>The GELU activation function provides a smoother activation curve and an adaptive gating mechanism, which makes the activation function more sensitive to changes in the input and reduces the instability of the gradient during the training process. The GELU activation function formula is as follows:</p><disp-formula id="pone.0324048.e006"><alternatives><graphic xlink:href="pone.0324048.e006.jpg" id="pone.0324048.e006g" position="anchor"/><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mi>&#x003d5;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mi>&#x000b7;</mml:mi><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(4)</label></disp-formula><p>Where <inline-formula id="pone.0324048.e007"><alternatives><graphic xlink:href="pone.0324048.e007.jpg" id="pone.0324048.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the error function.</p><p>The image of the GELU activation function is shown in <xref rid="pone.0324048.g005" ref-type="fig">Fig 5</xref>, which exhibits a smooth S-shaped curve that gradually tends to zero in the negative region and gradually increases linearly in the positive region, which is more natural and continuous compared to ReLU&#x02019;s segmented linearity and hard thresholding (where the negative values go directly to zero). This smoothness allows the model to be more delicate with inputs close to zero, avoids the complete neglect of negative values in ReLU, and is more stable during gradient propagation, thus aiding the optimisation process and the learning of complex features.</p><fig position="float" id="pone.0324048.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.g005</object-id><label>Fig 5</label><caption><title>Improved Pred Network module structure.</title></caption><graphic xlink:href="pone.0324048.g005" position="float"/></fig><p>It has been proved by previous experiments that the GELU activation function can show better performance and generalisation ability when dealing with complex high-dimensional data, and it has been widely used in natural language processing models such as BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), GPT (Generative Pre-trained), XLNeT (Generalised Autoregressive Pretraining for Language Understanding), Transformer and other natural language processing models.</p></sec><sec id="sec009"><title>3.3 Mixed pruned RNN-T/CTC loss</title><p>The loss function is a key measure of the gap between the model&#x02019;s predicted results and the true results, and is a crucial part of optimising the model&#x02019;s performance.The Zipformer model introduces the Pruned RNN-T method proposed by Kuang et al. in the module of calculating the loss function [<xref rid="pone.0324048.ref024" ref-type="bibr">24</xref>], but the pruning strategy used in this method discards some information that is crucial in the alignment process, which reduces the model&#x02019;s accuracy and generalisation ability. To solve these problems, this paper introduces the CTC loss function on top of the Pruned RNN-T loss function, and proposes the hybrid Pruned RNN-T/CTC loss function method, which combines the Pruned RNN-T loss function and the CTC loss function in a weighted manner and balances the effects between them by adjusting the weights, aiming to improve the performance of the model by combining multiple different loss functions through the combination to improve the performance of the model.</p><p>The overall structure of the hybrid Pruned RNN-T/CTC Loss is shown in <xref rid="pone.0324048.g006" ref-type="fig">Fig 6</xref>, and the overall structure is divided into the two loss functions of Pruned RNN-T and the CTC loss function.The Pruned RNN-T loss values include the Simple Loss and the Pruned Loss, and the Simple Loss refers to the loss value of the model before the pruning operation, based on its complete structure and all parameters, whereas Pruned Loss is the loss value recalculated after the model has been pruned, i.e., after some of the network parameters and/or nodes have been discarded to reduce model complexity and computation. In calculating the CTC loss, the encoder output is first mapped to a space more suitable for CTC through a linear layer, followed by a Dropout layer to reduce the overfitting and enhance the model generalisation by randomly dropping some of the outputs. Afterwards, the LogSoftmax layer converts these vectors into log-probability form, ensuring that each dimension represents the category log-probability and sums to 0. Finally, these log-probabilities and labels are subjected to CTC loss calculation.</p><fig position="float" id="pone.0324048.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.g006</object-id><label>Fig 6</label><caption><title>Improved Pred Network module structure.</title></caption><graphic xlink:href="pone.0324048.g006" position="float"/></fig><p>The Hybrid Pruned RNN-T/CTC Loss is derived by weighted summation of the above three loss values and is given by the following formula:</p><disp-formula id="pone.0324048.e008"><alternatives><graphic xlink:href="pone.0324048.e008.jpg" id="pone.0324048.e008g" position="anchor"/><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mi>&#x000b7;</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mi>&#x000b7;</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.085</mml:mn><mml:mi>&#x000b7;</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(5)</label></disp-formula><p>Where <italic toggle="yes">L</italic><sub><italic toggle="yes">total</italic></sub> is the total loss value, <italic toggle="yes">L</italic><sub><italic toggle="yes">pruned</italic></sub>, <italic toggle="yes">L</italic><sub><italic toggle="yes">simple</italic></sub> and <italic toggle="yes">L</italic><sub><italic toggle="yes">ctc</italic></sub> are the Pruned loss value, the Simple loss value and the CTC loss value, respectively, <italic toggle="yes">W</italic><sub><italic toggle="yes">pruned</italic></sub> and <italic toggle="yes">W</italic><sub><italic toggle="yes">simple</italic></sub> are the scaling factors for the Pruned loss value and the Simple loss value, respectively.</p><p>The calculation formulas for the two scaling factors are as follows:</p><disp-formula id="pone.0324048.e009"><alternatives><graphic xlink:href="pone.0324048.e009.jpg" id="pone.0324048.e009g" position="anchor"/><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1.0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.278em"/><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>_</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>_</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0.1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.9</mml:mn><mml:mi>&#x000b7;</mml:mi><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>_</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>_</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(6)</label></disp-formula><disp-formula id="pone.0324048.e010"><alternatives><graphic xlink:href="pone.0324048.e010.jpg" id="pone.0324048.e010g" position="anchor"/><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>0.65</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.278em"/><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>_</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>_</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>1.0</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>_</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>_</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(7)</label></disp-formula><p>Where <inline-formula id="pone.0324048.e011"><alternatives><graphic xlink:href="pone.0324048.e011.jpg" id="pone.0324048.e011g" position="anchor"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>_</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the current training batch index, and <inline-formula id="pone.0324048.e012"><alternatives><graphic xlink:href="pone.0324048.e012.jpg" id="pone.0324048.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>_</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the step threshold.</p><p>As training progresses, the scaling factors of the loss function gradually transition from their initial values to the predetermined target values, enabling dynamic adjustment of the loss. Specifically, and are adjusted according to a linear progression during the training process, thus assigning appropriate weights to the smooth loss and pruning loss at different stages of training. Experimental results show that when the weight coefficient of the CTC loss is set to 0.085, the model achieves optimal overall performance. This approach effectively balances the influence of different loss components, significantly improving the model&#x02019;s recognition accuracy and stability.</p></sec></sec><sec id="sec010"><title>4 Experimental results and analysis</title><sec id="sec011"><title>4.1 Experimental data</title><p>The experiments in this study primarily use the Mandarin speech datasets AISHELL1 and ST-CMDS. The AISHELL1 dataset was originally recorded by 400 speakers from different regions of China using a variety of devices, including high-fidelity microphones, Android phones, and iOS phones, with an audio sampling rate of 16&#x02009;kHz. The ST-CMDS dataset is an open-source Mandarin Chinese speech dataset primarily used for tasks such as speech recognition, speech synthesis, and speaker recognition. The dataset contains a large number of monophonic audio files with a 16&#x02009;kHz sampling rate, covering various daily conversation scenarios, such as online chatting and voice command for smart devices.</p><p>To adapt the dataset for phoneme recognition tasks, this study performed a phoneme label conversion on the original Chinese character text labels, aiming to construct the AISHELL1-PHONEME and ST-CMDS-PHONEME Chinese phoneme datasets. The core step in this process was to replace the original Chinese character labels with phoneme labels, utilizing the mapping relationship between Chinese characters and phonemes. Due to the phenomenon of polyphonic characters in Chinese, where the pronunciation of a character may differ in different word groups or contexts, a simple character-by-character replacement approach cannot meet the need for accurate labeling. Therefore, this study employed a phrase-level phoneme mapping table to replace the labels, thereby avoiding the phonetic ambiguity that arises from character-by-character substitution of polyphonic characters. This method allows the dataset to more accurately reflect the phonetic characteristics of phonemes in different contexts, adapting to more fine-grained, phoneme-level speech recognition tasks. The dataset divisions for AISHELL1-PHONEME and ST-CMDS-PHONEME are shown in <xref rid="pone.0324048.t001" ref-type="table">Tables 1</xref> and <xref rid="pone.0324048.t002" ref-type="table">2</xref>.</p><table-wrap position="float" id="pone.0324048.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.t001</object-id><label>Table 1</label><caption><title>The division of the AISHELL1-PHONEME dataset.</title></caption><alternatives><graphic xlink:href="pone.0324048.t001" id="pone.0324048.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Data</th><th align="left" rowspan="1" colspan="1">Time(h)</th><th align="left" rowspan="1" colspan="1">Phoneme Quantity</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Train</td><td align="left" rowspan="1" colspan="1">150</td><td align="left" rowspan="1" colspan="1">10365415</td></tr><tr><td align="left" rowspan="1" colspan="1">Dev</td><td align="left" rowspan="1" colspan="1">18</td><td align="left" rowspan="1" colspan="1">410674</td></tr><tr><td align="left" rowspan="1" colspan="1">Test</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">209524</td></tr></tbody></table></alternatives></table-wrap><table-wrap position="float" id="pone.0324048.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.t002</object-id><label>Table 2</label><caption><title>The division of the ST-CMDS-PHONEME dataset.</title></caption><alternatives><graphic xlink:href="pone.0324048.t002" id="pone.0324048.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Data</th><th align="left" rowspan="1" colspan="1">Time(h)</th><th align="left" rowspan="1" colspan="1">Phoneme Quantity</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Train</td><td align="left" rowspan="1" colspan="1">85</td><td align="left" rowspan="1" colspan="1">1900432</td></tr><tr><td align="left" rowspan="1" colspan="1">Dev</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">223750</td></tr><tr><td align="left" rowspan="1" colspan="1">Test</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">111618</td></tr></tbody></table></alternatives></table-wrap><p>To enhance the robustness of the model, this paper mixes the MUSAN dataset into the AISHELL1-PHONEME dataset to simulate noise interference in real-world environments, thereby improving the model&#x02019;s adaptability under various noise conditions. The MUSAN dataset is a comprehensive corpus that includes various types of audio data, such as music, speech, and noise. These audio data come from multiple sources and cover a wide range of noise scenarios, including background noise, traffic noise, wind noise, animal sounds, as well as various types of music and speech materials, greatly enriching the dataset&#x02019;s diversity.</p><p>By combining the MUSAN dataset with the AISHELL1-PHONEME dataset, various noise interference situations closer to real-world environments can be simulated, including noise from different frequency bands, dynamic noise, and sudden bursts of noise. Introducing this noise during the training process can effectively improve the model&#x02019;s robustness, enhancing its ability to adapt to different environmental noises, allowing it to better handle variable and complex speech inputs in practical applications. <xref rid="pone.0324048.t003" ref-type="table">Table 3</xref> shows the classification of the MUSAN dataset.</p><table-wrap position="float" id="pone.0324048.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.t003</object-id><label>Table 3</label><caption><title>The categorization of the MUSAN dataset.</title></caption><alternatives><graphic xlink:href="pone.0324048.t003" id="pone.0324048.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Categories</th><th align="left" rowspan="1" colspan="1">Time(h)</th><th align="left" rowspan="1" colspan="1">Source</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">music</td><td align="left" rowspan="1" colspan="1">42</td><td align="left" rowspan="1" colspan="1">jazz music, classical music, pop music, etc.</td></tr><tr><td align="left" rowspan="1" colspan="1">speech</td><td align="left" rowspan="1" colspan="1">60</td><td align="left" rowspan="1" colspan="1">English, French, German, Spanish, etc.</td></tr><tr><td align="left" rowspan="1" colspan="1">noise</td><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">crowd noise, traffic noise, white noise, etc.</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec012"><title>4.2 Evaluation metrics</title><p>The experiments in this paper use three metrics to evaluate the model&#x02019;s performance and efficiency: Word Error Rate (WER), Real Time Factor (RTF), and the number of parameters (Params).</p><sec id="sec013"><title>4.2.1 WER.</title><p>Word Error Rate (WER) is a key metric for measuring the degree of difference between the output phonemes of a speech recognition model and the reference phonemes. It is used to evaluate the accuracy of a speech recognition model in transcribing speech. Specifically, WER quantifies the accuracy of the model under various conditions by calculating the difference between the model&#x02019;s output and the true labels (reference phonemes). It considers three basic types of errors: substitution (Sub), insertion (Ins), and deletion (Del), providing a comprehensive reflection of the potential biases that may occur during the recognition process.</p><p>The calculation method of WER normalizes the total occurrences of substitution, insertion, and deletion errors by the total number of reference phonemes, yielding a ratio. This ratio indicates the extent of errors made by the model when processing a segment of speech. The lower the WER, the higher the model&#x02019;s recognition accuracy, as it better matches the true phoneme labels. The calculation formula for WER is typically as follows:</p><disp-formula id="pone.0324048.e013"><alternatives><graphic xlink:href="pone.0324048.e013.jpg" id="pone.0324048.e013g" position="anchor"/><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mo>*</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(8)</label></disp-formula><p>Where <italic toggle="yes">Sub</italic> represents the number of substitution errors, which refers to the times the model&#x02019;s output phonemes do not match the reference phonemes; <italic toggle="yes">Ins</italic> represents the number of insertion errors, which refers to the number of extra or inserted phonemes in the model&#x02019;s output; <italic toggle="yes">Del</italic> represents the number of deletion errors, which refers to the number of phonemes missing or deleted in the model&#x02019;s output. <italic toggle="yes">Num</italic> represents the total number of reference phonemes, which is the number of phonemes in the true labels.</p><p>By calculating the total of these error types and normalizing it with the total number of reference phonemes, WER provides a quantitative assessment of the accuracy of the speech recognition model. It is important to note that WER not only considers the model&#x02019;s phoneme recognition accuracy but also reflects its performance in handling complex cases such as homophones, polyphonic characters, and noise interference. Therefore, WER is widely used as a standard to evaluate the performance of speech recognition systems.</p><p>In practical applications, WER also has its limitations, especially in cases with long texts or complex contexts, where a single error type may not fully capture the model&#x02019;s performance. As a result, in addition to WER, other metrics such as Real-Time Factor (RTF) and Parameters (Params) are often used in combination to comprehensively evaluate the model&#x02019;s recognition performance and operational efficiency.</p></sec><sec id="sec014"><title>4.2.2 RTF.</title><p>Real-Time Factor (RTF) is one of the key metrics used to evaluate the real-time performance and processing speed of speech recognition models, particularly in applications that require fast response times. The real-time factor represents the ratio between the time taken by the model to process a segment of audio and the actual duration of that audio. In the domain of phoneme recognition, RTF is used to measure how much audio the model can process within a given unit of time. It is typically employed to assess the response speed and processing capability of a speech recognition model when handling real-time audio data.</p><p>Specifically, the real-time factor quantifies the processing efficiency of a model by calculating the ratio between the time taken by the model to process audio data and the actual duration of the input audio. When the real-time factor is less than 1, it indicates that the model can process audio in real-time (i.e., within the same duration as the input), meaning the model processes more than 1 second of audio per second, which typically implies high processing speed and low latency. Conversely, when the real-time factor exceeds 1, it suggests that the model&#x02019;s processing speed cannot meet real-time requirements, as the time taken exceeds the duration of the audio, resulting in increased latency and making the model unsuitable for real-time applications. Therefore, the smaller the real-time factor, the faster the processing speed and the shorter the response time, making it more suitable for practical applications that require rapid responses, such as voice assistants, telephone speech recognition, and intelligent customer service. The formula for calculating the real-time factor is as follows:</p><disp-formula id="pone.0324048.e014"><alternatives><graphic xlink:href="pone.0324048.e014.jpg" id="pone.0324048.e014g" position="anchor"/><mml:math id="M14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>D</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(9)</label></disp-formula><p>Where <inline-formula id="pone.0324048.e015"><alternatives><graphic xlink:href="pone.0324048.e015.jpg" id="pone.0324048.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> refers to the time taken by the model to process the input audio, typically measured in seconds; and <inline-formula id="pone.0324048.e016"><alternatives><graphic xlink:href="pone.0324048.e016.jpg" id="pone.0324048.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>D</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> refers to the duration of the input audio data, also typically measured in seconds.</p><p>A low Real-Time Factor (RTF) is crucial for the practical application of speech recognition systems, especially in edge devices or resource-constrained environments. To optimize the real-time factor, it is typically necessary to improve the computational efficiency of algorithms, leverage hardware acceleration, and reduce the complexity of the model to enhance the system&#x02019;s response speed. In the design of speech recognition systems, the real-time factor needs to be balanced with other metrics (such as accuracy, model size, etc.) to ensure that the system can process data efficiently while maintaining good accuracy and reliability.</p></sec><sec id="sec015"><title>4.2.3 Params.</title><p>The number of parameters(Params) is one of the key indicators of model complexity and expressive capability. In speech recognition tasks, particularly phoneme-level recognition, the number of parameters plays a crucial role in the model&#x02019;s performance. A higher number of parameters enables the model to have a stronger fitting ability, allowing it to capture more complex speech features and thus improve recognition accuracy.</p><p>However, more parameters do not mean better model performance. Although increasing the number of parameters can enhance the model&#x02019;s performance, an excessively high number of parameters also brings a range of negative impacts. First, an overly large number of parameters can lead to overfitting, where the model performs well on training data but poorly on new data (especially test data or real-world data). Overfitting occurs when the model learns too much noise and unnecessary details during training, losing its ability to generalize and thus resulting in decreased performance. Second, an excessive number of parameters significantly increases the computational resources and memory consumption of the model. Each learnable parameter must be stored and computed during both training and inference, which directly leads to increased computation time. This is particularly problematic when processing large-scale speech data, where the computational and storage costs can be very high. Additionally, as the number of parameters increases, the training time also grows significantly, prolonging the development cycle. More importantly, during real-world deployment, especially in resource-constrained scenarios such as edge devices or embedded systems, models with an excessively large number of parameters increase deployment costs and complexity. This may require stronger hardware support, larger storage space, and higher computational power, which is unacceptable in many real-time application scenarios.</p><p>Therefore, when designing phoneme recognition models, it is essential to control the number of parameters appropriately. Ensuring high recognition accuracy while maintaining a low model complexity is one of the key strategies for optimizing model performance and resource utilization. Moreover, choosing an appropriate model size for different application scenarios is also an important aspect of model optimization. For instance, in embedded systems or mobile devices, smaller models may be required to balance performance, accuracy, and resource consumption.</p></sec></sec><sec id="sec016"><title>4.3 Model validation</title><sec id="sec017"><title>4.3.1 Comparison of mainstream models.</title><p>This set of experiments adopts the state-of-the-art encoder-decoder architecture to thoroughly evaluate the performance of three mainstream acoustic models&#x02014;Zipformer, Conformer, and Transformer&#x02014;on the Mandarin phoneme recognition task. The experiments are based on the AISHELL1-PHONEME and ST-CMDS-PHONEME datasets, using the Fbank technique to extract speech features. Eight different phoneme recognition models are trained, and their performance is evaluated using word error rate (WER), real-time factor (RTF), and model parameter count from both the development and test sets. In the experiments, Zipformer-RNN-T(Pruned) demonstrates excellent performance on both the AISHELL1-PHONEME and ST-CMDS-PHONEME datasets, exhibiting strong robustness and high generalization ability.</p><p>As shown in <xref rid="pone.0324048.t004" ref-type="table">Table 4</xref>, on the AISHELL1-PHONEME dataset, Zipformer-RNN-T(Pruned) achieves a Dev WER of 2.28% and a Test WER of 2.12%, significantly outperforming other models. In particular, compared to Conformer-CTC (Test WER 2.88%), Transformer-CTC (Test WER 5.78%), and Zipformer-CTC (Test WER 2.80%), Zipformer-RNN-T(Pruned) significantly reduces the error rate on the test set. Additionally, Zipformer-RNN-T(Pruned) has an inference speed (RTF) of 0.002, which is significantly faster than Conformer-RNN-T (RTF of 0.012) and Transformer-RNN-T(Pruned) (RTF of 0.003), demonstrating a clear advantage in inference efficiency.</p><table-wrap position="float" id="pone.0324048.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.t004</object-id><label>Table 4</label><caption><title>Comparative experimental results of mainstream models on the AISHELL1-PHONEME dataset.</title></caption><alternatives><graphic xlink:href="pone.0324048.t004" id="pone.0324048.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Model</th><th align="left" rowspan="1" colspan="1">Dev</th><th align="left" rowspan="1" colspan="1">Test</th><th align="left" rowspan="1" colspan="1">RTF</th><th align="left" rowspan="1" colspan="1">Params</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Conformer-CTC</td><td align="left" rowspan="1" colspan="1">2.53%</td><td align="left" rowspan="1" colspan="1">2.88%</td><td align="left" rowspan="1" colspan="1">0.029</td><td align="left" rowspan="1" colspan="1">108.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer-CTC</td><td align="left" rowspan="1" colspan="1">5.03%</td><td align="left" rowspan="1" colspan="1">5.78%</td><td align="left" rowspan="1" colspan="1">0.020</td><td align="left" rowspan="1" colspan="1">70.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-CTC</td><td align="left" rowspan="1" colspan="1">2.46%</td><td align="left" rowspan="1" colspan="1">2.80%</td><td align="left" rowspan="1" colspan="1">0.018</td><td align="left" rowspan="1" colspan="1">90.4</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-RNN-T</td><td align="left" rowspan="1" colspan="1">2.28%</td><td align="left" rowspan="1" colspan="1">2.45%</td><td align="left" rowspan="1" colspan="1">0.010</td><td align="left" rowspan="1" colspan="1">67.4</td></tr><tr><td align="left" rowspan="1" colspan="1">Conformer-RNN-T</td><td align="left" rowspan="1" colspan="1">2.44%</td><td align="left" rowspan="1" colspan="1">2.71%</td><td align="left" rowspan="1" colspan="1">0.012</td><td align="left" rowspan="1" colspan="1">83.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">3.97%</td><td align="left" rowspan="1" colspan="1">4.53%</td><td align="left" rowspan="1" colspan="1">0.003</td><td align="left" rowspan="1" colspan="1">66.4</td></tr><tr><td align="left" rowspan="1" colspan="1">Conformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">2.21%</td><td align="left" rowspan="1" colspan="1">2.51%</td><td align="left" rowspan="1" colspan="1">0.003</td><td align="left" rowspan="1" colspan="1">78.1</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">
<bold>1.92%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>2.12%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>0.002</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>61.1</bold>
</td></tr></tbody></table></alternatives></table-wrap><p>As shown in <xref rid="pone.0324048.t005" ref-type="table">Table 5</xref>, on the ST-CMDS-PHONEME dataset, Zipformer-RNN-T(Pruned) achieves a Dev WER of 4.28% and a Test WER of 4.51%, outperforming all other comparison models. Especially when compared to Transformer-CTC (Test WER 15.77%), Conformer-CTC (Test WER 6.47%), and Zipformer-CTC (Test WER 6.43%), Zipformer-RNN-T(Pruned) performs excellently in terms of accuracy. Although its Dev WER (4.81%) and Test WER (4.97%) are not significantly different from those of Conformer-RNN-T(Pruned), Zipformer-RNN-T(Pruned) still holds an advantage in inference speed (RTF of 0.002) and model size (61.1M parameters), demonstrating a balanced trade-off between efficiency and performance.</p><table-wrap position="float" id="pone.0324048.t005"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.t005</object-id><label>Table 5</label><caption><title>Comparative Experimental Results of Mainstream Models on ST-CMDS-PHONEME Dataset.</title></caption><alternatives><graphic xlink:href="pone.0324048.t005" id="pone.0324048.t005g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Model</th><th align="left" rowspan="1" colspan="1">Dev</th><th align="left" rowspan="1" colspan="1">Test</th><th align="left" rowspan="1" colspan="1">RTF</th><th align="left" rowspan="1" colspan="1">Params</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Conformer-CTC</td><td align="left" rowspan="1" colspan="1">6.22%</td><td align="left" rowspan="1" colspan="1">6.47%</td><td align="left" rowspan="1" colspan="1">0.029</td><td align="left" rowspan="1" colspan="1">108.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer-CTC</td><td align="left" rowspan="1" colspan="1">15.02%</td><td align="left" rowspan="1" colspan="1">15.77%</td><td align="left" rowspan="1" colspan="1">0.020</td><td align="left" rowspan="1" colspan="1">70.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-CTC</td><td align="left" rowspan="1" colspan="1">6.10%</td><td align="left" rowspan="1" colspan="1">6.43%</td><td align="left" rowspan="1" colspan="1">0.018</td><td align="left" rowspan="1" colspan="1">90.4</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-RNN-T</td><td align="left" rowspan="1" colspan="1">5.74%</td><td align="left" rowspan="1" colspan="1">5.94%</td><td align="left" rowspan="1" colspan="1">0.010</td><td align="left" rowspan="1" colspan="1">67.4</td></tr><tr><td align="left" rowspan="1" colspan="1">Conformer-RNN-T</td><td align="left" rowspan="1" colspan="1">4.98%</td><td align="left" rowspan="1" colspan="1">5.11%</td><td align="left" rowspan="1" colspan="1">0.011</td><td align="left" rowspan="1" colspan="1">83.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">11.73%</td><td align="left" rowspan="1" colspan="1">12.38%</td><td align="left" rowspan="1" colspan="1">0.003</td><td align="left" rowspan="1" colspan="1">66.4</td></tr><tr><td align="left" rowspan="1" colspan="1">Conformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">4.81%</td><td align="left" rowspan="1" colspan="1">4.97%</td><td align="left" rowspan="1" colspan="1">0.003</td><td align="left" rowspan="1" colspan="1">78.1</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">
<bold>4.28%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>4.51%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>0.002</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>61.1</bold>
</td></tr></tbody></table></alternatives></table-wrap><p>These results indicate that Zipformer-RNN-T(Pruned) achieves optimal performance on both the AISHELL1-PHONEME and ST-CMDS-PHONEME datasets, with the fewest parameters, lowest error rate, and fastest inference speed, showcasing its superior robustness and generalization ability in phoneme recognition tasks. Compared to all other models, Zipformer-RNN-T(Pruned) leads across all evaluation metrics, proving its advantage as the optimal model.</p><p>In the phoneme recognition task, misclassifications are typically categorized into three types: substitution, insertion, and deletion. As shown in <xref rid="pone.0324048.g007" ref-type="fig">Figs 7</xref> and <xref rid="pone.0324048.g008" ref-type="fig">8</xref>, on both the AISHELL1-PHONEME and ST-CMDS-PHONEME datasets, the number of substitution errors is significantly higher than that of insertion and deletion errors. In the specific misclassification statistics, Zipformer-RNN-T(Pruned) achieves the lowest error count across all error types. On the AISHELL1-PHONEME dataset, the numbers of substitution, insertion, and deletion errors are 7165, 380, and 356, respectively; on the ST-CMDS-PHONEME dataset, they are 7890, 741, and 820. Compared to other models, Zipformer-RNN-T(Pruned) performs exceptionally well in reducing misclassification counts, demonstrating strong modeling capability and stability both in handling the dominant substitution errors and the relatively less frequent insertion and deletion errors.</p><fig position="float" id="pone.0324048.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.g007</object-id><label>Fig 7</label><caption><title>Word error rates and the number of three types of misrecognized phonemes on the development set for different models(AISHELL1-PHONEME).</title></caption><graphic xlink:href="pone.0324048.g007" position="float"/></fig><fig position="float" id="pone.0324048.g008"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.g008</object-id><label>Fig 8</label><caption><title>Word error rates and the number of three types of misrecognized phonemes on the development set for different models(ST-CMDS-PHONEME).</title></caption><graphic xlink:href="pone.0324048.g008" position="float"/></fig><p>According to the phoneme dictionary provided by the AISHELL1 dataset, Mandarin pronunciation is subdivided into 66 distinct phonemes. As shown in <xref rid="pone.0324048.g009" ref-type="fig">Figs 9</xref> and <xref rid="pone.0324048.g010" ref-type="fig">10</xref>, the proposed model Zipformer-RNN-T(Pruned) demonstrates good phoneme recognition accuracy on both the AISHELL1-PHONEME and ST-CMDS-PHONEME datasets, covering all phoneme categories. The accuracy for the majority of phonemes is higher than that of other comparative models.</p><fig position="float" id="pone.0324048.g009"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.g009</object-id><label>Fig 9</label><caption><title>The phoneme accuracy percentage on the development set for different models (AISHELL1-PHONEME).</title></caption><graphic xlink:href="pone.0324048.g009" position="float"/></fig><fig position="float" id="pone.0324048.g010"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.g010</object-id><label>Fig 10</label><caption><title>The phoneme accuracy percentage on the development set for different models (ST-CMDS-PHONEME).</title></caption><graphic xlink:href="pone.0324048.g010" position="float"/></fig></sec><sec id="sec018"><title>4.3.2 Noise simulation experiment.</title><p>To enhance the model&#x02019;s generalization ability and robustness, this study conducts a data augmentation experiment. The MUSAN dataset is used as the background noise source and mixed with the original Train, Dev, and Test sets of the AISHELL1-PHONEME and ST-CMDS-PHONEME datasets, generating augmented datasets: Train+, Dev+, and Test+. The experiment adopts a dual-track parallel training strategy, where phoneme recognition models are trained based on both the Train+ and Train datasets, and validated and tested on the Dev+ and Test+ datasets. As shown in <xref rid="pone.0324048.t006" ref-type="table">Tables 6</xref> and <xref rid="pone.0324048.t007" ref-type="table">7</xref>, the models trained on Train+ outperform those trained on Train in both Word Error Rate (WER) and Real-Time Factor (RTF), indicating the effectiveness of the data augmentation strategy in improving model performance. This strategy not only strengthens the model&#x02019;s ability to accurately recognize speech content, but also significantly enhances its robustness when faced with noise interference. Additionally, the proposed Zipformer-RNN-T(Pruned) model demonstrates exceptional performance on both training sets in the AISHELL1-PHONEME and ST-CMDS-PHONEME datasets, with significantly lower WER and RTF compared to other comparative models. This highlights the model&#x02019;s strong robustness and ability to maintain stable performance in complex and dynamic acoustic environments.</p><table-wrap position="float" id="pone.0324048.t006"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.t006</object-id><label>Table 6</label><caption><title>Comparison of Model Performance Under Different Data Augmentation Strategies on the AISHELL1-PHONEME Dataset.</title></caption><alternatives><graphic xlink:href="pone.0324048.t006" id="pone.0324048.t006g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="2" colspan="1">Model</th><th align="left" colspan="3" rowspan="1">Train+</th><th align="left" colspan="3" rowspan="1">Train</th><th align="left" rowspan="2" colspan="1">Para</th></tr><tr><th align="left" rowspan="1" colspan="1">Dev+</th><th align="left" rowspan="1" colspan="1">Test+</th><th align="left" rowspan="1" colspan="1">RTF</th><th align="left" rowspan="1" colspan="1">Dev+</th><th align="left" rowspan="1" colspan="1">Test+</th><th align="left" rowspan="1" colspan="1">RTF</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Conformer-CTC</td><td align="left" rowspan="1" colspan="1">2.63%</td><td align="left" rowspan="1" colspan="1">3.00%</td><td align="left" rowspan="1" colspan="1">0.015</td><td align="left" rowspan="1" colspan="1">3.05%</td><td align="left" rowspan="1" colspan="1">3.39%</td><td align="left" rowspan="1" colspan="1">0.029</td><td align="left" rowspan="1" colspan="1">108.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer-CTC</td><td align="left" rowspan="1" colspan="1">5.33%</td><td align="left" rowspan="1" colspan="1">6.04%</td><td align="left" rowspan="1" colspan="1">0.020</td><td align="left" rowspan="1" colspan="1">6.14%</td><td align="left" rowspan="1" colspan="1">6.87%</td><td align="left" rowspan="1" colspan="1">0.020</td><td align="left" rowspan="1" colspan="1">70.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-CTC</td><td align="left" rowspan="1" colspan="1">2.65%</td><td align="left" rowspan="1" colspan="1">2.97%</td><td align="left" rowspan="1" colspan="1">0.018</td><td align="left" rowspan="1" colspan="1">2.87%</td><td align="left" rowspan="1" colspan="1">3.17%</td><td align="left" rowspan="1" colspan="1">0.018</td><td align="left" rowspan="1" colspan="1">90.4</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-RNN-T</td><td align="left" rowspan="1" colspan="1">2.47%</td><td align="left" rowspan="1" colspan="1">2.79%</td><td align="left" rowspan="1" colspan="1">0.005</td><td align="left" rowspan="1" colspan="1">2.92%</td><td align="left" rowspan="1" colspan="1">3.15%</td><td align="left" rowspan="1" colspan="1">0.005</td><td align="left" rowspan="1" colspan="1">67.4</td></tr><tr><td align="left" rowspan="1" colspan="1">Conformer-RNN-T</td><td align="left" rowspan="1" colspan="1">2.27%</td><td align="left" rowspan="1" colspan="1">2.51%</td><td align="left" rowspan="1" colspan="1">0.004</td><td align="left" rowspan="1" colspan="1">2.51%</td><td align="left" rowspan="1" colspan="1">2.71%</td><td align="left" rowspan="1" colspan="1">0.010</td><td align="left" rowspan="1" colspan="1">83.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">4.38%</td><td align="left" rowspan="1" colspan="1">5.03%</td><td align="left" rowspan="1" colspan="1">0.003</td><td align="left" rowspan="1" colspan="1">4.88%</td><td align="left" rowspan="1" colspan="1">5.44%</td><td align="left" rowspan="1" colspan="1">0.003</td><td align="left" rowspan="1" colspan="1">66.4</td></tr><tr><td align="left" rowspan="1" colspan="1">Conformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">2.19%</td><td align="left" rowspan="1" colspan="1">2.44%</td><td align="left" rowspan="1" colspan="1">0.003</td><td align="left" rowspan="1" colspan="1">2.57%</td><td align="left" rowspan="1" colspan="1">2.83%</td><td align="left" rowspan="1" colspan="1">0.003</td><td align="left" rowspan="1" colspan="1">78.1</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">
<bold>1.95%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>2.11%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>0.002</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>2.16%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>2.34%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>0.002</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>61.1</bold>
</td></tr></tbody></table></alternatives></table-wrap><table-wrap position="float" id="pone.0324048.t007"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.t007</object-id><label>Table 7</label><caption><title>Comparison of Model Performance Under Different Data Augmentation Strategies on the ST-CMDS-PHONEME Dataset.</title></caption><alternatives><graphic xlink:href="pone.0324048.t007" id="pone.0324048.t007g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="2" colspan="1">Model</th><th align="left" colspan="3" rowspan="1">Train+</th><th align="left" colspan="3" rowspan="1">Train</th><th align="left" rowspan="2" colspan="1">Para</th></tr><tr><th align="left" rowspan="1" colspan="1">Dev+</th><th align="left" rowspan="1" colspan="1">Test+</th><th align="left" rowspan="1" colspan="1">RTF</th><th align="left" rowspan="1" colspan="1">Dev+</th><th align="left" rowspan="1" colspan="1">Test+</th><th align="left" rowspan="1" colspan="1">RTF</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Conformer-CTC</td><td align="left" rowspan="1" colspan="1">6.38%</td><td align="left" rowspan="1" colspan="1">6.75%</td><td align="left" rowspan="1" colspan="1">0.025</td><td align="left" rowspan="1" colspan="1">6.84%</td><td align="left" rowspan="1" colspan="1">7.16%</td><td align="left" rowspan="1" colspan="1">0.028</td><td align="left" rowspan="1" colspan="1">108.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer-CTC</td><td align="left" rowspan="1" colspan="1">15.37%</td><td align="left" rowspan="1" colspan="1">15.99%</td><td align="left" rowspan="1" colspan="1">0.022</td><td align="left" rowspan="1" colspan="1">16.21%</td><td align="left" rowspan="1" colspan="1">16.93%</td><td align="left" rowspan="1" colspan="1">0.021</td><td align="left" rowspan="1" colspan="1">70.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-CTC</td><td align="left" rowspan="1" colspan="1">6.44%</td><td align="left" rowspan="1" colspan="1">6.78%</td><td align="left" rowspan="1" colspan="1">0.016</td><td align="left" rowspan="1" colspan="1">6.72%</td><td align="left" rowspan="1" colspan="1">6.97%</td><td align="left" rowspan="1" colspan="1">0.016</td><td align="left" rowspan="1" colspan="1">90.4</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-RNN-T</td><td align="left" rowspan="1" colspan="1">5.88%</td><td align="left" rowspan="1" colspan="1">6.15%</td><td align="left" rowspan="1" colspan="1">0.005</td><td align="left" rowspan="1" colspan="1">6.25%</td><td align="left" rowspan="1" colspan="1">6.48%</td><td align="left" rowspan="1" colspan="1">0.005</td><td align="left" rowspan="1" colspan="1">67.4</td></tr><tr><td align="left" rowspan="1" colspan="1">Conformer-RNN-T</td><td align="left" rowspan="1" colspan="1">5.37%</td><td align="left" rowspan="1" colspan="1">5.42%</td><td align="left" rowspan="1" colspan="1">0.004</td><td align="left" rowspan="1" colspan="1">5.55%</td><td align="left" rowspan="1" colspan="1">5.74%</td><td align="left" rowspan="1" colspan="1">0.008</td><td align="left" rowspan="1" colspan="1">83.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">12.55%</td><td align="left" rowspan="1" colspan="1">13.18%</td><td align="left" rowspan="1" colspan="1">0.005</td><td align="left" rowspan="1" colspan="1">12.97%</td><td align="left" rowspan="1" colspan="1">13.49%</td><td align="left" rowspan="1" colspan="1">0.005</td><td align="left" rowspan="1" colspan="1">66.4</td></tr><tr><td align="left" rowspan="1" colspan="1">Conformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">4.98%</td><td align="left" rowspan="1" colspan="1">5.09%</td><td align="left" rowspan="1" colspan="1">0.004</td><td align="left" rowspan="1" colspan="1">5.34%</td><td align="left" rowspan="1" colspan="1">5.53%</td><td align="left" rowspan="1" colspan="1">0.004</td><td align="left" rowspan="1" colspan="1">78.1</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">
<bold>4.39%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>4.54%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>0.003</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>4.66%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>4.90%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>0.003</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>61.1</bold>
</td></tr></tbody></table></alternatives></table-wrap></sec></sec><sec id="sec019"><title>4.4 Ablation experiment</title><p>In phoneme recognition tasks, the misalignment between the input speech features and the output phoneme sequence is a common issue. The CTC (Connectionist Temporal Classification) loss function addresses this problem by introducing a blank symbol and allowing multiple possible alignment paths, which enables automatic alignment between the input and output sequences. This improves the model&#x02019;s robustness and generalization ability. The weight configuration of the CTC loss function is an essential part of model performance tuning, and setting the appropriate weight can further enhance the accuracy of phoneme recognition.</p><p>In the experiment of adjusting the CTC weight coefficient, the model was first trained on the Train+ dataset to ensure that it could sufficiently learn the mapping between speech features and phoneme labels in the standard training set. Then, the mixed MUSAN dataset was applied to the Dev+ and Test+ datasets for testing, to evaluate the model&#x02019;s performance when noise interference was introduced. <xref rid="pone.0324048.g011" ref-type="fig">Fig 11</xref> presents the experimental results showing the difference in model performance on the Dev+ and Test+ datasets with different CTC weighting factor settings.</p><fig position="float" id="pone.0324048.g011"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.g011</object-id><label>Fig 11</label><caption><title>The influence trend of different CTC weight coefficients on Word Error Rate (WER).</title></caption><graphic xlink:href="pone.0324048.g011" position="float"/></fig><p>When the CTC weight coefficient was set to 0.085, the model achieved the lowest Word Error Rate (WER) on both the Dev+ and Test+ datasets, with rates of 1.95% and 2.11%, respectively. This result indicates that the setting of the CTC weight coefficient has a significant impact on the performance of the phoneme recognition task. In particular, in complex noisy environments, an appropriate weight coefficient can significantly improve the model&#x02019;s recognition accuracy and stability. Under this weight coefficient, the model was able to effectively capture phoneme information in noisy and misaligned speech data, thereby optimizing the accuracy of speech recognition.</p><p>When the CTC weight coefficient deviated from 0.085, the experimental results showed a gradual increase in WER, demonstrating a certain degree of performance degradation. This trend suggests that both excessively high and low CTC weight coefficients can negatively impact model training. For example, if the CTC weight coefficient is too high, the model may overly rely on the blank symbol, thus ignoring phoneme label information and leading to a decline in recognition accuracy. Conversely, if the coefficient is too low, the model may not be flexible enough in alignment, which negatively affects its ability to capture phonemes and leads to more errors. This gradually increasing WER trend further validates the role of the CTC weight coefficient in regulating the model&#x02019;s training and inference process. An appropriate weight coefficient helps the model better balance the selection of alignment paths, avoiding over-reliance on blank symbols or other unreasonable alignment methods, thus improving the accuracy of phoneme recognition.</p><p>To systematically assess the impact of each component on the overall performance of the model, this paper designs a series of ablation experiments based on the Zipformer-RNN-T(Pruned) architecture. The aim is to quantify the influence of different modules (such as the Block3 module, GELU activation function, and CTC loss function) on recognition accuracy (Word Error Rate, WER), real-time factor (RTF), and model complexity (parameter count, Params) by comparing the model&#x02019;s performance under different configurations.</p><p>First, the Zipformer-RNN-T(Pruned)-L model is constructed, using an Encoder-Block layer configuration of 2,2,4,5,4,2, but without the inclusion of the Block3 module, GELU activation function, or CTC loss function. As shown in <xref rid="pone.0324048.t008" ref-type="table">Table 8</xref>, the model with this configuration has a parameter count of 65.1M, and the word error rates on the Dev+ and Test+ datasets are 2.04% and 2.23%, respectively, indicating that the model&#x02019;s performance still requires further optimization. The preliminary results of this experiment suggest that, although the model can complete basic recognition tasks without too many additional modules, its recognition accuracy and efficiency are suboptimal, highlighting the need for the support of specific modules.</p><table-wrap position="float" id="pone.0324048.t008"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.t008</object-id><label>Table 8</label><caption><title>Ablation experiment.</title></caption><alternatives><graphic xlink:href="pone.0324048.t008" id="pone.0324048.t008g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="2" colspan="1">Model</th><th align="left" rowspan="2" colspan="1">Block3</th><th align="left" rowspan="2" colspan="1">GELU</th><th align="left" rowspan="2" colspan="1">CTC</th><th align="left" colspan="2" rowspan="1">Dev+</th><th align="left" colspan="2" rowspan="1">Test+</th><th align="left" rowspan="2" colspan="1">Para</th></tr><tr><th align="left" rowspan="1" colspan="1">Wer</th><th align="left" rowspan="1" colspan="1">RTF</th><th align="left" rowspan="1" colspan="1">Wer</th><th align="left" rowspan="1" colspan="1">RTF</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Zipformer-RNN-T(Pruned)-L</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">2.04%</td><td align="left" rowspan="1" colspan="1">0.002</td><td align="left" rowspan="1" colspan="1">2.23%</td><td align="left" rowspan="1" colspan="1">0.002</td><td align="left" rowspan="1" colspan="1">65.1</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0324048.e017">
<alternatives><graphic xlink:href="pone.0324048.e017" id="pone.0324048.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0324048.e018">
<alternatives><graphic xlink:href="pone.0324048.e018" id="pone.0324048.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0324048.e019">
<alternatives><graphic xlink:href="pone.0324048.e019" id="pone.0324048.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<bold>1.95%</bold>
</td><td align="left" rowspan="1" colspan="1">0.002</td><td align="left" rowspan="1" colspan="1">
<bold>2.11%</bold>
</td><td align="left" rowspan="1" colspan="1">0.002</td><td align="left" rowspan="1" colspan="1">61.1</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0324048.e020">
<alternatives><graphic xlink:href="pone.0324048.e020" id="pone.0324048.e020g" position="anchor"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0324048.e021">
<alternatives><graphic xlink:href="pone.0324048.e021" id="pone.0324048.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">1.99%</td><td align="left" rowspan="1" colspan="1">0.002</td><td align="left" rowspan="1" colspan="1">2.12%</td><td align="left" rowspan="1" colspan="1">0.002</td><td align="left" rowspan="1" colspan="1">61.1</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0324048.e022">
<alternatives><graphic xlink:href="pone.0324048.e022" id="pone.0324048.e022g" position="anchor"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0324048.e023">
<alternatives><graphic xlink:href="pone.0324048.e023" id="pone.0324048.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">1.97%</td><td align="left" rowspan="1" colspan="1">0.002</td><td align="left" rowspan="1" colspan="1">2.12%</td><td align="left" rowspan="1" colspan="1">0.002</td><td align="left" rowspan="1" colspan="1">61.1</td></tr><tr><td align="left" rowspan="1" colspan="1">Zipformer-RNN-T(Pruned)</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0324048.e024">
<alternatives><graphic xlink:href="pone.0324048.e024" id="pone.0324048.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0324048.e025">
<alternatives><graphic xlink:href="pone.0324048.e025" id="pone.0324048.e025g" position="anchor"/><mml:math id="M25" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">1.98%</td><td align="left" rowspan="1" colspan="1">0.002</td><td align="left" rowspan="1" colspan="1">2.16%</td><td align="left" rowspan="1" colspan="1">0.002</td><td align="left" rowspan="1" colspan="1">61.1</td></tr></tbody></table></alternatives></table-wrap><p>Next, the improved Zipformer-RNN-T(Pruned) model is constructed, with the Encoder-Block layer configuration adjusted to 2,2,3,5,3,2, reducing the model&#x02019;s parameter count to 61.1M. The model then progressively incorporates the Block3 module, GELU activation function, and CTC loss function. The experimental results show that when the model fully includes the Block3 module, GELU activation function, and CTC loss function, the word error rates on the Dev+ and Test+ datasets decrease to the lowest levels of 1.95% and 2.11%, while the real-time factor (RTF) remains stable at 0.002. This indicates that the collaboration of the Block3 module, GELU activation function, and CTC loss function significantly improves the model&#x02019;s recognition accuracy while maintaining high real-time performance and relatively low computational complexity, validating the importance of these components in phoneme recognition tasks.</p><p>Subsequently, when the Block3 module is removed, the word error rate on the Dev+ dataset increases to 1.99%, and there is also a slight increase in the Test+ dataset&#x02019;s word error rate. Similarly, when the GELU activation function or CTC loss function is removed, a slight increase in the word error rate is observed, further confirming the indispensability of these components for the model&#x02019;s performance. The removal of the GELU activation function leads to a decline in the model&#x02019;s nonlinear representation capability, affecting its generalization ability. On the other hand, removing the CTC loss function reduces the model&#x02019;s performance in alignment tasks, leading to an increase in the word error rate.</p><p>These results show that the introduction of each component plays a crucial role in the model&#x02019;s performance. The Block3 module helps the model better handle complex speech features, the GELU activation function provides stronger nonlinearity, and the CTC loss function effectively solves the alignment problem between the input and output sequences.</p></sec><sec id="sec020"><title>4.5 Inference experiments</title><p>In machine learning, inference is the process of predicting or classifying test data using a model that has already been trained.In this experiment, five search algorithms are used to inference the Zipformer-RNN-T(Pruned) model.</p><p>As shown in <xref rid="pone.0324048.t009" ref-type="table">Table 9</xref>, the analysis in terms of word error rate shows that fast-beam-search has the highest word error rate, which is 50.3% and 46.74% in Dev+ and Test+ respectively, which is due to the fact that the algorithm expands multiple candidate paths at each step, which leads to the accumulation of errors and thus increases the word error rate. In comparison, fast-beam-search-nbesth and fast-beam-search-nbest-oracle perform significantly better, but both have a real-time factor of 0.016 and 0.007, respectively, which is higher than fast-beam-search&#x02019;s 0.006. The WER of modified beam search and greedy search are the same, at 1.95% and 2.11% on Dev+and Test+, respectively. However, the real-time factor of greedy search is only 0.002, which is much lower than the 0.008 of modified beam search. The greedy search has the lowest real-time factor while ensuring a low WER.</p><table-wrap position="float" id="pone.0324048.t009"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.t009</object-id><label>Table 9</label><caption><title>Search algorithm performance comparison.</title></caption><alternatives><graphic xlink:href="pone.0324048.t009" id="pone.0324048.t009g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="2" colspan="1">Model(Train+)</th><th align="left" colspan="2" rowspan="1">Dev+</th><th align="left" colspan="2" rowspan="1">Test+</th></tr><tr><th align="left" rowspan="1" colspan="1">WER</th><th align="left" rowspan="1" colspan="1">RTF</th><th align="left" rowspan="1" colspan="1">WER</th><th align="left" rowspan="1" colspan="1">RTF</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">fast-beam-search</td><td align="left" rowspan="1" colspan="1">50.30%</td><td align="left" rowspan="1" colspan="1">0.005</td><td align="left" rowspan="1" colspan="1">46.74%</td><td align="left" rowspan="1" colspan="1">0.006</td></tr><tr><td align="left" rowspan="1" colspan="1">fast-beam-search-nbest</td><td align="left" rowspan="1" colspan="1">5.79%</td><td align="left" rowspan="1" colspan="1">0.016</td><td align="left" rowspan="1" colspan="1">5.88%</td><td align="left" rowspan="1" colspan="1">0.016</td></tr><tr><td align="left" rowspan="1" colspan="1">fast-beam-search-nbest-oracle</td><td align="left" rowspan="1" colspan="1">2.33%</td><td align="left" rowspan="1" colspan="1">0.007</td><td align="left" rowspan="1" colspan="1">2.36%</td><td align="left" rowspan="1" colspan="1">0.007</td></tr><tr><td align="left" rowspan="1" colspan="1">modified-beam-search</td><td align="left" rowspan="1" colspan="1">
<bold>1.95%</bold>
</td><td align="left" rowspan="1" colspan="1">0.008</td><td align="left" rowspan="1" colspan="1">
<bold>2.11%</bold>
</td><td align="left" rowspan="1" colspan="1">0.008</td></tr><tr><td align="left" rowspan="1" colspan="1">greedy-search</td><td align="left" rowspan="1" colspan="1">
<bold>1.95%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>0.002</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>2.11%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>0.002</bold>
</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec021"><title>4.6 Phonological analysis of Mandarin</title><p>The Zipformer-RNN-T(Pruned) model based on the Zipformer-RNN-T(Pruned) model shown in <xref rid="pone.0324048.g012" ref-type="fig">Fig 12</xref> suffers from the following typical confusions in predicting Mandarin phonemes:Firstly, in terms of vowel confusion, the bilabial bursts /B/ and /P/ are often confused with each other due to subtle differences in amplitude and spectrum in the speech signal; Secondly, for front and back nasals, the difference between /IN/ and /ING/ is mainly in the presence or absence and duration of the nasal rhyme-final; Finally, in the case of the distinction between flat and warped consonants, such as /Z/ and /ZH/, /S/ and /SH/, /C/ and /CH/, the key to the distinction lies in the subtle differences in the articulatory position of the tip of the tongue. However, these subtle pronunciation differences are affected by a variety of factors such as individual differences, variation in speech rate, pronunciation habits, and dialectal environments, making them difficult to clearly define in actual recognition.</p><fig position="float" id="pone.0324048.g012"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.g012</object-id><label>Fig 12</label><caption><title>Confusion matrix for Mandarin phoneme recognition based on Zipformer-RNN-T(Pruned) model.</title></caption><graphic xlink:href="pone.0324048.g012" position="float"/></fig><p><xref rid="pone.0324048.g013" ref-type="fig">Fig 13</xref> shows the distribution of accuracy in predicting Mandarin phonemes based on the Zipformer-RNN-T(Pruned) model. Of the 66 phonemes, 64 (97%) of the phonemes had an accuracy of greater than 95%, with only two phonemes lying between 90% and 95%, /OO/ and /UA/, with the highest accuracy being /UENG/, due to the extremely low frequency of this factor in the dataset, which contained only two samples. The model proposed in this paper performs with high accuracy on most of the phonemes, demonstrating the effectiveness and potential of the model in the task of Mandarin phoneme recognition.</p><fig position="float" id="pone.0324048.g013"><object-id pub-id-type="doi">10.1371/journal.pone.0324048.g013</object-id><label>Fig 13</label><caption><title>Accuracy of Mandarin phonemes based on Zipformer-RNN-T(Pruned) model.</title></caption><graphic xlink:href="pone.0324048.g013" position="float"/></fig></sec></sec><sec sec-type="conclusions" id="sec022"><title>5 Conclusion</title><p>To reduce the word error rate (WER) and model complexity in Mandarin phoneme recognition, this paper proposes an improved Mandarin phoneme recognition model based on the Zipformer-RNN-T(Pruned) architecture. This model incorporates several innovative improvements on top of the original Zipformer encoder and RNN-T decoder framework. First, structural optimizations are introduced for the Zipformer Block and Pred Network, enabling the model to effectively reduce computational complexity while maintaining accuracy. Secondly, a hybrid Pruned RNN-T/CTC Loss strategy is proposed, which simultaneously introduces CTC loss and RNN-T loss during training to enhance the model&#x02019;s alignment capability and improve Mandarin phoneme recognition performance. The experimental results demonstrate:</p><list list-type="bullet"><list-item><p>The improved model demonstrates outstanding performance across both benchmark datasets. On the AISHELL1-PHONEME dataset, it achieves low Word Error Rates (WER) of 1.92% on the development set and 2.12% on the test set. On the ST-CMDS-PHONEME dataset, the corresponding WERs are maintained at 4.28% and 4.51%, respectively. With only 61.1M parameters, the model holds a significant advantage over other mainstream approaches. Its efficiency and low complexity make it highly deployable in real-world scenarios, particularly on resource-constrained devices.</p></list-item><list-item><p>To further validate the model&#x02019;s performance in complex environments, a simulated noise experiment was conducted. The model was tested on Train+, Dev+, and Test+ datasets containing background noise to assess its robustness. The experimental results show that, even in noisy environments, the proposed model still achieves low WER, demonstrating its strong adaptability to noise and external disturbances. The success of this experiment proves that the improved model can maintain good performance in real-world speech recognition tasks, especially in complex noise environments.</p></list-item><list-item><p>In addition, an ablation experiment was carried out by progressively removing or modifying components of the model to to analyse the impact of each module on model performance. The experimental results confirmed the importance of the modified modules, particularly the improved Zipformer Block and Pred Network, in enhancing the model&#x02019;s performance. Furthermore, the optimal value for the CTC loss weight was determined through experimentation, which further improved the model&#x02019;s recognition accuracy and training stability.</p></list-item><list-item><p>To better understand the model&#x02019;s performance in phoneme recognition, an in-depth analysis of Mandarin phoneme misrecognition patterns was conducted. By systematically studying common issues in Mandarin phoneme recognition, this paper revealed the typical confusion patterns of phonemes and provided experimental support and theoretical foundation for future tasks such as Mandarin pronunciation evaluation. These analyses not only helped identify the weak points of the model in the Mandarin phoneme recognition task but also provided targeted solutions to improve model performance.</p></list-item></list><p>However, in the context of practical applications for Chinese phoneme recognition tasks, there is a relative scarcity of Chinese reading evaluation datasets, which limits the effectiveness of model training and evaluation. This issue results in larger recognition errors when handling phonemes with significant pronunciation differences. To further enhance the model&#x02019;s generalization ability and robustness, future research will focus on exploring data augmentation techniques and self-supervised learning methods. By incorporating more diverse training data and unsupervised learning strategies, the goal is to address the problem of data scarcity and improve the model&#x02019;s performance in real-world applications, particularly in adapting to unknown phonemes and complex speech environments. Future work will concentrate on how to strengthen the model&#x02019;s generalization ability through dataset expansion and model architecture improvements, providing more reliable and efficient solutions for Mandarin phoneme recognition applications.</p></sec></body><back><ref-list><title>References</title><ref id="pone.0324048.ref001"><label>1</label><mixed-citation publication-type="book"><name><surname>Harrison</surname><given-names>AM</given-names></name>, <name><surname>Lau</surname><given-names>WY</given-names></name>, <name><surname>Meng</surname><given-names>HM</given-names></name>, <name><surname>Wang</surname><given-names>L</given-names></name>. <article-title>Improving mispronunciation detection and diagnosis of learners&#x02019; speech with context-sensitive phonological rules based on language transfer</article-title>. <source>Interspeech</source>; <year>2008</year>. p. <fpage>2787</fpage>&#x02013;<lpage>90</lpage>. <ext-link xlink:href="https://www.isca-archive.org/interspeech_2008/harrison08_interspeech.pdf" ext-link-type="uri">https://www.isca-archive.org/interspeech_2008/harrison08_interspeech.pdf</ext-link>.</mixed-citation></ref><ref id="pone.0324048.ref002"><label>2</label><mixed-citation publication-type="book"><name><surname>Qian</surname><given-names>X</given-names></name>, <name><surname>Meng</surname><given-names>HM</given-names></name>, <name><surname>Soong</surname><given-names>FK</given-names></name>. <article-title>The use of DBN-HMMs for mispronunciation detection and diagnosis in L2 English to support computer-aided pronunciation training</article-title>. <source>Interspeech</source>; <year>2012</year>. p. <fpage>775</fpage>&#x02013;<lpage>8</lpage>. <ext-link xlink:href="https://www.isca-archive.org/interspeech_2012/qian12c_interspeech.pdf" ext-link-type="uri">https://www.isca-archive.org/interspeech_2012/qian12c_interspeech.pdf</ext-link></mixed-citation></ref><ref id="pone.0324048.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>L</given-names></name>, <name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Chen</surname><given-names>L</given-names></name>, <name><surname>Zhang</surname><given-names>J</given-names></name>, <name><surname>Koomey</surname><given-names>JG</given-names></name>. <article-title>RETRACTED: English oral evaluation algorithm based on fuzzy measure and speech recognition</article-title>. <source>J Intell Fuzzy Syst</source>. <year>2019</year>;<volume>37</volume>(<issue>1</issue>):<fpage>241</fpage>&#x02013;<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3233/jifs-179081</pub-id></mixed-citation></ref><ref id="pone.0324048.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Miao</surname><given-names>Y</given-names></name>, <name><surname>Metze</surname><given-names>F</given-names></name>. <article-title>Improving low-resource CD-DNN-HMM using dropout and multilingual DNN training</article-title>. <source>Interspeech</source>; <year>2013</year>;<volume>13</volume>:<fpage>2237</fpage>&#x02013;<lpage>41</lpage>. <ext-link xlink:href="https://www.isca-archive.org/interspeech_2013/miao13_interspeech.pdf" ext-link-type="uri">https://www.isca-archive.org/interspeech_2013/miao13_interspeech.pdf</ext-link></mixed-citation></ref><ref id="pone.0324048.ref005"><label>5</label><mixed-citation publication-type="book"><name><surname>Park</surname><given-names>S</given-names></name>, <name><surname>Jeong</surname><given-names>Y</given-names></name>, <name><surname>Kim</surname><given-names>HS</given-names></name>. <article-title>Multiresolution CNN for reverberant speech recognition.</article-title> In: <source>2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA)</source>. <publisher-name>IEEE</publisher-name>; <year>2017</year>. p. <fpage>1</fpage>&#x02013;<lpage>4</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/icsda.2017.8384470</pub-id></mixed-citation></ref><ref id="pone.0324048.ref006"><label>6</label><mixed-citation publication-type="book"><name><surname>Graves</surname><given-names>A</given-names></name>, <name><surname>Fern&#x000e1;ndez</surname><given-names>S</given-names></name>, <name><surname>Gomez</surname><given-names>F</given-names></name>, <name><surname>Schmidhuber</surname><given-names>J</given-names></name>. <article-title>Connectionist temporal classification.</article-title> In: <source>Proceedings of the 23rd International Conference on Machine Learning - ICML &#x02019;06</source>. <publisher-name>ACM Press</publisher-name>; <year>2006</year>. p. <fpage>369</fpage>&#x02013;<lpage>76</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/1143844.1143891</pub-id></mixed-citation></ref><ref id="pone.0324048.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Graves</surname><given-names>A</given-names></name>. <article-title>Sequence transduction with recurrent neural networks</article-title>. <source>arXiv preprint</source>
<year>2012</year>. <ext-link xlink:href="https://arxiv.org/abs/1211.3711" ext-link-type="uri">https://arxiv.org/abs/1211.3711</ext-link></mixed-citation></ref><ref id="pone.0324048.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Chan</surname><given-names>W</given-names></name>, <name><surname>Jaitly</surname><given-names>N</given-names></name>, <name><surname>Le</surname><given-names>QV</given-names></name>, <name><surname>Vinyals</surname><given-names>O</given-names></name>. <article-title>Listen, attend and spell</article-title>. <source>arXiv preprint</source>
<year>2015</year>. <ext-link xlink:href="https://arxiv.org/abs/1508.01211" ext-link-type="uri">https://arxiv.org/abs/1508.01211</ext-link></mixed-citation></ref><ref id="pone.0324048.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Abedi</surname><given-names>D</given-names></name>, <name><surname>Niari</surname><given-names>MH</given-names></name>, <name><surname>Ramavandi</surname><given-names>B</given-names></name>, <name><surname>De-la-Torre</surname><given-names>GE</given-names></name>, <name><surname>Renner</surname><given-names>G</given-names></name>, <name><surname>Schmidt</surname><given-names>TC</given-names></name>, <etal>et al</etal>. <article-title>Microplastics and phthalate esters in yogurt and buttermilk samples: characterization and health risk assessment</article-title>. <source>J Environ Health Sci Eng</source>. <year>2025</year>;<volume>23</volume>(<issue>1</issue>):<fpage>14</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s40201-025-00939-z</pub-id>
<pub-id pub-id-type="pmid">40226515</pub-id>
</mixed-citation></ref><ref id="pone.0324048.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>S</given-names></name>, <name><surname>Dong</surname><given-names>L</given-names></name>, <name><surname>Xu</surname><given-names>S</given-names></name>, <name><surname>Xu</surname><given-names>B</given-names></name>. <article-title>Syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese</article-title>. <source>arXiv preprint</source>
<year>2018</year>. <ext-link xlink:href="https://arxiv.org/abs/1804.10752" ext-link-type="uri">https://arxiv.org/abs/1804.10752</ext-link></mixed-citation></ref><ref id="pone.0324048.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Odinye</surname><given-names>IS</given-names></name>. <article-title>Phonology of mandarin chinese: a comparison of Pinyin and IPA</article-title>. <source>Journal</source>. <year>2022</year>. <ext-link xlink:href="https://nigerianjournalsonline.com/index.php/published_Articles/article/view/2790" ext-link-type="uri">https://nigerianjournalsonline.com/index.php/published_Articles/article/view/2790</ext-link></mixed-citation></ref><ref id="pone.0324048.ref012"><label>12</label><mixed-citation publication-type="book"><name><surname>Leung</surname><given-names>W-K</given-names></name>, <name><surname>Liu</surname><given-names>X</given-names></name>, <name><surname>Meng</surname><given-names>H</given-names></name>. <article-title>CNN-RNN-CTC based end-to-end mispronunciation detection and diagnosis.</article-title> In: <source>ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source>. <publisher-name>IEEE</publisher-name>; <year>2019</year>. p. <fpage>8132</fpage>&#x02013;<lpage>6</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/icassp.2019.8682654</pub-id></mixed-citation></ref><ref id="pone.0324048.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>L</given-names></name>, <name><surname>Zhao</surname><given-names>Z</given-names></name>, <name><surname>Ma</surname><given-names>C</given-names></name>, <name><surname>Shan</surname><given-names>L</given-names></name>, <name><surname>Sun</surname><given-names>H</given-names></name>, <name><surname>Jiang</surname><given-names>L</given-names></name>, <etal>et al</etal>. <article-title>End-to-end automatic pronunciation error detection based on improved hybrid CTC/attention architecture</article-title>. <source>Sensors (Basel)</source>. <year>2020</year>;<volume>20</volume>(<issue>7</issue>):<fpage>1809</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/s20071809</pub-id>
<pub-id pub-id-type="pmid">32218379</pub-id>
</mixed-citation></ref><ref id="pone.0324048.ref014"><label>14</label><mixed-citation publication-type="other">Yan B, Wu M, Hung H, Chen B. <article-title>An end-to-end mispronunciation detection system for L2 English speech leveraging novel anti-phone modeling</article-title>. arXiv preprint 2020. <ext-link xlink:href="https://arxiv.org/abs/2005.11950" ext-link-type="uri">https://arxiv.org/abs/2005.11950</ext-link></mixed-citation></ref><ref id="pone.0324048.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Abdel-Hamid</surname><given-names>O</given-names></name>, <name><surname>Mohamed</surname><given-names>A</given-names></name>, <name><surname>Jiang</surname><given-names>H</given-names></name>, <name><surname>Deng</surname><given-names>L</given-names></name>, <name><surname>Penn</surname><given-names>G</given-names></name>, <name><surname>Yu</surname><given-names>D</given-names></name>. <article-title>Convolutional neural networks for speech recognition</article-title>. <source>IEEE/ACM Trans Audio Speech Lang Process</source>. <year>2014</year>;<volume>22</volume>(<issue>10</issue>):<fpage>1533</fpage>&#x02013;<lpage>45</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/taslp.2014.2339736</pub-id></mixed-citation></ref><ref id="pone.0324048.ref016"><label>16</label><mixed-citation publication-type="book"><name><surname>T&#x000f3;th</surname><given-names>L</given-names></name>. <article-title>Convolutional deep maxout networks for phone recognition.</article-title> In: <source>Fifteenth Annual Conference of the International Speech Communication Association</source>; <year>2014</year>. <ext-link xlink:href="https://www.isca-archive.org/interspeech_2014/toth14_interspeech.pdf" ext-link-type="uri">https://www.isca-archive.org/interspeech_2014/toth14_interspeech.pdf</ext-link></mixed-citation></ref><ref id="pone.0324048.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Passricha</surname><given-names>V</given-names></name>, <name><surname>Aggarwal</surname><given-names>RK</given-names></name>. <article-title>Convolutional support vector machines for speech recognition</article-title>. <source>Int J Speech Technol</source>. <year>2018</year>;<volume>22</volume>(<issue>3</issue>):<fpage>601</fpage>&#x02013;<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10772-018-09584-4</pub-id></mixed-citation></ref><ref id="pone.0324048.ref018"><label>18</label><mixed-citation publication-type="journal">Ravanelli M, Bengio Y. <article-title>Interpretable convolutional filters with sincnet</article-title>. arXiv preprint <year>2018</year>. <ext-link xlink:href="https://arxiv.org/abs/1811.09725" ext-link-type="uri">https://arxiv.org/abs/1811.09725</ext-link></mixed-citation></ref><ref id="pone.0324048.ref019"><label>19</label><mixed-citation publication-type="book"><name><surname>Zhao</surname><given-names>Y</given-names></name>, <name><surname>Jin</surname><given-names>X</given-names></name>, <name><surname>Hu</surname><given-names>X</given-names></name>. <article-title>Recurrent convolutional neural network for speech processing.</article-title> In: <source>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source>. <publisher-name>IEEE</publisher-name>; <year>2017</year>. p. <fpage>5300</fpage>&#x02013;<lpage>4</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ICASSP.2017.7953168</pub-id></mixed-citation></ref><ref id="pone.0324048.ref020"><label>20</label><mixed-citation publication-type="book"><name><surname>Ding</surname><given-names>H</given-names></name>, <name><surname>Xie</surname><given-names>C</given-names></name>, <name><surname>Zeng</surname><given-names>L</given-names></name>, <name><surname>Xu</surname><given-names>Y</given-names></name>, <name><surname>Dan</surname><given-names>G</given-names></name>. <article-title>The correlation between signal distance and consonant pronunciation in Mandarin words.</article-title> In: <source>2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)</source>. <publisher-name>IEEE</publisher-name>; <year>2016</year>. p. <fpage>1</fpage>&#x02013;<lpage>5</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ISCSLP.2016.7918482</pub-id></mixed-citation></ref><ref id="pone.0324048.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Jiang</surname><given-names>X</given-names></name>, <name><surname>Cohen</surname><given-names>AD</given-names></name>. <article-title>Learner strategies for dealing with pronunciation issues in Mandarin</article-title>. <source>System</source>. <year>2018</year>;<volume>76</volume>:<fpage>25</fpage>&#x02013;<lpage>37</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.system.2018.04.012</pub-id></mixed-citation></ref><ref id="pone.0324048.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Arkin</surname><given-names>G</given-names></name>, <name><surname>Hamdulla</surname><given-names>A</given-names></name>, <name><surname>Ablimit</surname><given-names>M</given-names></name>. <article-title>Analysis of phonemes and tones confusion rules obtained by ASR</article-title>. <source>Wireless Netw</source>. <year>2020</year>;<volume>27</volume>(<issue>5</issue>):<fpage>3471</fpage>&#x02013;<lpage>81</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11276-019-02220-2</pub-id></mixed-citation></ref><ref id="pone.0324048.ref023"><label>23</label><mixed-citation publication-type="book"><name><surname>Ghodsi</surname><given-names>M</given-names></name>, <name><surname>Liu</surname><given-names>X</given-names></name>, <name><surname>Apfel</surname><given-names>J</given-names></name>, <name><surname>Cabrera</surname><given-names>R</given-names></name>, <name><surname>Weinstein</surname><given-names>E</given-names></name>. <article-title>RNN-transducer with stateless prediction network.</article-title> In: <source>ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source>. <publisher-name>IEEE</publisher-name>; <year>2020</year>. p. <fpage>7049</fpage>&#x02013;<lpage>53</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ICASSP40776.2020.9054419</pub-id></mixed-citation></ref><ref id="pone.0324048.ref024"><label>24</label><mixed-citation publication-type="book"><name><surname>Yao</surname><given-names>Z</given-names></name>, <name><surname>Guo</surname><given-names>L</given-names></name>, <name><surname>Yang</surname><given-names>X</given-names></name>, <name><surname>Kang</surname><given-names>W</given-names></name>, <name><surname>Kuang</surname><given-names>F</given-names></name>, <name><surname>Yang</surname><given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Zipformer: A faster and better encoder for automatic speech recognition</article-title>. <source>The Twelfth International Conference on Learning Representations</source>, <year>2023</year>. <ext-link xlink:href="https://openreview.net/forum?id=9WD9KwssyT" ext-link-type="uri">https://openreview.net/forum?id=9WD9KwssyT</ext-link></mixed-citation></ref></ref-list></back><sub-article article-type="author-comment" id="pone.0324048.r001" specific-use="rebutted-decision-letter-unavailable"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0324048.r001</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">30 Dec 2024</named-content>
</p></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0324048.r002" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0324048.r002</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Jie</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Jie Zhang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Jie Zhang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0324048" id="rel-obj002" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">7 Mar 2025</named-content>
</p><p><!--<div>-->PONE-D-24-60568<!--</div>--><!--<div>-->A Study on Phonemes Recognition Method for Mandarin Pronunciation Based on Improved Zipformer-RNN-T(Pruned) Modeling<!--</div>--><!--<div>-->PLOS ONE</p><p>Dear Dr. yu,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please revise the paper by considering the reviewer's comments.<!--</div>--><!--<div>--></p><p>Please submit your revised manuscript by Apr 21 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!--</div>--></p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p><!--<div>-->If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Jie Zhang</p><p>Academic Editor</p><p>PLOS ONE</p><p>Journal Requirements:</p><p>When submitting your revision, we need you to address these additional requirements.</p><p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at&#x000a0;</p><p><ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and&#x000a0;</p><p>
<ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
</p><p>2. Please note that PLOS ONE has specific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, we expect all author-generated code to be made available without restrictions upon publication of the work. Please review our guidelines at <ext-link xlink:href="https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code" ext-link-type="uri">https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code</ext-link> and ensure that your code is shared in a way that follows best practice and facilitates reproducibility and reuse.</p><p>3. Thank you for stating the following financial disclosure: This research was funded by [the Bing-tuan Science and Technology Public Relations Project &#x0201c;A Data-driven Regional Smart Education Service Key Technology Research and Application Demonstration&#x0201d;] grantnumber [2021AB023].,&#x000a0;</p><p>Please state what role the funders took in the study.&#x000a0; If the funders had no role, please state: ""The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.""&#x000a0;</p><p>If this statement is not correct you must amend it as needed.&#x000a0;</p><p>Please include this amended Role of Funder statement in your cover letter; we will change the online submission form on your behalf.</p><p>4. Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article&#x02019;s retracted status in the References list and also include a citation and full reference for the retraction notice.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!--<font color="black">-->
<bold>Comments to the Author</bold>
</p><p>1. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->2. Has the statistical analysis been performed appropriately and rigorously? <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->3. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->4. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->5. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!--</font>--></p><p>Reviewer #1:&#x000a0;The proposed model demonstrates significant advancements in recognition accuracy and real-time processing capabilities, achieving WERs of 1.92% on the Dev set and 2.12% on the Test set with a real-time factor (RTF) of 0.002. These results highlight the model's practical utility for real-time applications, such as voice assistants and live translation systems. The use of 61.1M parameters ensures a lightweight architecture that balances accuracy with computational efficiency, making it highly suitable for deployment in resource-constrained environments. Furthermore, the model&#x02019;s robustness in noisy environments is emphasized through experiments using the MUSAN dataset, where it maintains low WERs even under challenging acoustic conditions. This adaptability to diverse environments enhances its applicability in real-world scenarios with prevalent noise interference.</p><p>However, the paper exhibits several limitations. The introduction lacks clarity and a logical flow, particularly in justifying the necessity of the proposed model. Although challenges in phoneme recognition are discussed, the connection between these challenges and the choice of the Zipformer variant is insufficiently articulated. Additionally, the importance of RTF optimization for real-time Chinese phoneme recognition is not adequately supported with concrete examples or detailed reasoning. Another major drawback is the reliance on the AISHELL1-PHONEME dataset, which limits the generalizability of the model. Evaluating performance solely on a single dataset raises concerns about its effectiveness in different linguistic contexts, such as conversational or regional Mandarin. Furthermore, the paper does not sufficiently emphasize the model's relevance to Chinese phoneme recognition, as specific challenges like tonal variations and real-time processing requirements are not clearly tied to the proposed approach.</p><p>To improve the study, incorporating additional datasets such as L2-ARCTIC or augmenting AISHELL1-PHONEME with multilingual or augmented data would strengthen the model&#x02019;s validation. If other datasets are unavailable, data augmentation techniques could be explored to simulate diverse linguistic contexts. Additionally, the introduction should more effectively highlight the specific challenges in Chinese phoneme recognition and how the Zipformer variant addresses these issues, supported by clear examples and reasoning. Despite these limitations, the model presents promising results in accuracy, efficiency, and noise robustness, making it a strong candidate for real-world applications. Enhancing the scope of evaluation and refining the introduction would further solidify the paper&#x02019;s contributions to Mandarin phoneme recognition research.</p><p>**********</p><p><!--<font color="black">-->6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!--</font>--></p><p>Reviewer #1:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.<!--</div>--></p></body></sub-article><sub-article article-type="author-comment" id="pone.0324048.r003"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0324048.r003</article-id><title-group><article-title>Author response to Decision Letter 1</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0324048" id="rel-obj003" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">16 Apr 2025</named-content>
</p><p>Dear Editor,</p><p>Thank you for allowing us to resubmit the manuscript and for the opportunity to address the reviewers' comments. We have carefully revised the manuscript based on the feedback provided in the attached summary of reviewers' comments (as shared by your office).</p><p>We are now uploading the following documents:</p><p>1.A point-by-point response to the reviewers' comments (under "Response to Reviewers"), addressing each concern raised in the attached summary.</p><p>2.A highlighted version of the revised manuscript (under "Highlighted PDF"), with all changes marked in yellow.</p><p>3.A clean version of the revised manuscript (under "Main Manuscript").</p><p>Please let us know if any further adjustments are needed. We appreciate your time and consideration.</p><p>Best regards,</p><p>Zhaohui Du et al</p><supplementary-material id="pone.0324048.s001" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Response-to-Reviewers.pdf</named-content></p></caption><media xlink:href="pone.0324048.s001.pdf"/></supplementary-material></body></sub-article><sub-article article-type="editor-report" id="pone.0324048.r004" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0324048.r004</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Jie</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Jie Zhang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Jie Zhang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0324048" id="rel-obj004" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">20 Apr 2025</named-content>
</p><p>A Study on Phonemes Recognition Method for Mandarin Pronunciation Based on Improved Zipformer-RNN-T(Pruned) Modeling</p><p>PONE-D-24-60568R1</p><p>Dear Dr. yu,</p><p>We&#x02019;re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p><p>Within one week, you&#x02019;ll receive an e-mail detailing the required amendments. When these have been addressed, you&#x02019;ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p><p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">Editorial Manager&#x000ae;</ext-link>&#x000a0;and clicking the &#x02018;Update My Information' link at the top of the page. If you have any questions relating to publication charges, please contact our Author Billing department directly at <email>authorbilling@plos.org</email>.</p><p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they&#x02019;ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p><p>Kind regards,</p><p>Jie Zhang</p><p>Academic Editor</p><p>PLOS ONE</p><p>Additional Editor Comments (optional):</p><p>The authors have adequately addressed the reviewer's comments.</p><p>Reviewers' comments:</p></body></sub-article><sub-article article-type="editor-report" id="pone.0324048.r005" specific-use="acceptance-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0324048.r005</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Jie</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Jie Zhang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Jie Zhang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0324048" id="rel-obj005" related-article-type="reviewed-article"/></front-stub><body><p>PONE-D-24-60568R1</p><p>PLOS ONE</p><p>Dear Dr. Yu,</p><p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p><p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p><p>* All references, tables, and figures are properly cited</p><p>* All relevant supporting information is included in the manuscript submission,</p><p>* There are no issues that prevent the paper from being properly typeset</p><p>You will receive further&#x000a0;instructions from the production team, including instructions on how to review your proof when it&#x000a0;is ready. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few days to review your paper and let you know the next and final steps.</p><p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p><p>If we can help with anything else, please email us at <email>customercare@plos.org</email>.</p><p>Thank you for submitting your work to PLOS ONE and supporting open access.</p><p>Kind regards,</p><p>PLOS ONE Editorial Office Staff</p><p>on behalf of</p><p>Dr. Jie Zhang</p><p>Academic Editor</p><p>PLOS ONE</p></body></sub-article></article>