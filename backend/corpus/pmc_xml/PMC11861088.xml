<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Med (Lausanne)</journal-id><journal-id journal-id-type="iso-abbrev">Front Med (Lausanne)</journal-id><journal-id journal-id-type="publisher-id">Front. Med.</journal-id><journal-title-group><journal-title>Frontiers in Medicine</journal-title></journal-title-group><issn pub-type="epub">2296-858X</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40012977</article-id><article-id pub-id-type="pmc">PMC11861088</article-id><article-id pub-id-type="doi">10.3389/fmed.2025.1507258</article-id><article-categories><subj-group subj-group-type="heading"><subject>Medicine</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>MAEMC-NET: a hybrid self-supervised learning method for predicting the malignancy of solitary pulmonary nodules from CT images</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhao</surname><given-names>Tianhu</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/2862570/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Yue</surname><given-names>Yong</given-names></name><xref rid="aff3" ref-type="aff">
<sup>3</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Sun</surname><given-names>Hang</given-names></name><xref rid="aff4" ref-type="aff">
<sup>4</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/2596070/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Jingxu</given-names></name><xref rid="aff5" ref-type="aff">
<sup>5</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Wen</surname><given-names>Yanhua</given-names></name><xref rid="aff6" ref-type="aff">
<sup>6</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Yao</surname><given-names>Yudong</given-names></name><xref rid="aff7" ref-type="aff">
<sup>7</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/1100020/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Qian</surname><given-names>Wei</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/funding-acquisition/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Guan</surname><given-names>Yubao</given-names></name><xref rid="aff6" ref-type="aff">
<sup>6</sup>
</xref><xref rid="c002" ref-type="corresp">
<sup>*</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/1176779/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/funding-acquisition/"/><role content-type="https://credit.niso.org/contributor-roles/resources/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Qi</surname><given-names>Shouliang</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><xref rid="c001" ref-type="corresp">
<sup>*</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/283101/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/funding-acquisition/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/resources/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>College of Medicine and Biological Information Engineering, Northeastern University</institution>, <addr-line>Shenyang</addr-line>, <country>China</country></aff><aff id="aff2"><sup>2</sup><institution>Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University</institution>, <addr-line>Shenyang</addr-line>, <country>China</country></aff><aff id="aff3"><sup>3</sup><institution>Department of Radiology, Shengjing Hospital of China Medical University</institution>, <addr-line>Shenyang</addr-line>, <country>China</country></aff><aff id="aff4"><sup>4</sup><institution>School of Information Science and Engineering, Shenyang Ligong University</institution>, <addr-line>Shenyang</addr-line>, <country>China</country></aff><aff id="aff5"><sup>5</sup><institution>Department of Radiology, The First Affiliated Hospital of Guangzhou Medical University</institution>, <addr-line>Guangzhou</addr-line>, <country>China</country></aff><aff id="aff6"><sup>6</sup><institution>Department of Radiology, The Fifth Affiliated Hospital of Guangzhou Medical University</institution>, <addr-line>Guangzhou</addr-line>, <country>China</country></aff><aff id="aff7"><sup>7</sup><institution>Department of Electrical and Computer Engineering, Stevens Institute of Technology</institution>, <addr-line>Hoboken, NJ</addr-line>, <country>United States</country></aff><author-notes><fn id="fn0001" fn-type="edited-by"><p>Edited by: Liang Zhao, Dalian University of Technology, China</p></fn><fn id="fn0002" fn-type="edited-by"><p>Reviewed by: Jianxin Zhang, Dalian Minzu University, China</p><p>Jianqi Sun, Shanghai Jiao Tong University, China</p></fn><corresp id="c001">*Correspondence: Shouliang Qi, <email>qisl@bmie.neu.edu.cn</email></corresp><corresp id="c002">Yubao Guan, <email>yubaoguan@163.com</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>12</volume><elocation-id>1507258</elocation-id><history><date date-type="received"><day>07</day><month>10</month><year>2024</year></date><date date-type="accepted"><day>03</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2025 Zhao, Yue, Sun, Li, Wen, Yao, Qian, Guan and Qi.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Zhao, Yue, Sun, Li, Wen, Yao, Qian, Guan and Qi</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><sec><title>Introduction</title><p>Pulmonary granulomatous nodules (PGN) often exhibit similar CT morphological features to solid lung adenocarcinomas (SLA), making preoperative differentiation challenging. This study aims to address this diagnostic challenge by developing a novel deep learning model.</p></sec><sec><title>Methods</title><p>This study proposes MAEMC-NET, a model integrating generative (Masked AutoEncoder) and contrastive (Momentum Contrast) self-supervised learning to learn CT image representations of intra- and inter-solitary nodules. A generative self-supervised task of reconstructing masked axial CT patches containing lesions was designed to learn intra- and inter-slice image representations. Contrastive momentum is used to link the encoder in axial-CT-patch path with the momentum encoder in coronal-CT-patch path. A total of 494 patients from two centers were included.</p></sec><sec><title>Results</title><p>MAEMC-NET achieved an area under curve (95% Confidence Interval) of 0.962 (0.934&#x02013;0.973). These results not only significantly surpass the joint diagnosis by two experienced chest radiologists (77.3% accuracy) but also outperform the current state-of-the-art methods. The model performs best on medical images with a 50% mask ratio, showing a 1.4% increase in accuracy compared to the optimal 75% mask ratio on natural images.</p></sec><sec><title>Discussion</title><p>The proposed MAEMC-NET effectively distinguishes between benign and malignant solitary pulmonary nodules and holds significant potential to assist radiologists in improving the diagnostic accuracy of PGN and SLA.</p></sec></abstract><kwd-group><kwd>lung cancer</kwd><kwd>pulmonary granulomatous nodule</kwd><kwd>solid lung adenocarcinomas</kwd><kwd>CT image</kwd><kwd>self-supervised learning</kwd><kwd>masked autoencoder</kwd><kwd>momentum contrast</kwd></kwd-group><funding-group><funding-statement>The author(s) declare that financial support was received for the research, authorship, and/or publication of this article. This work was partly supported by the National Natural Science Foundation of China under Grant (nos. 82072008, 82272080, and 62271131) and the Fundamental Research Funds for the Central Universities (no. N2424010-19).</funding-statement></funding-group><counts><fig-count count="8"/><table-count count="6"/><equation-count count="9"/><ref-count count="47"/><page-count count="16"/><word-count count="10967"/></counts><custom-meta-group><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Pulmonary Medicine</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1"><label>1</label><title>Introduction</title><p>Lung cancer is the leading cause of cancer-related deaths worldwide, responsible for more deaths than breast, prostate, and colon cancers combined (<xref rid="ref1" ref-type="bibr">1</xref>). Each year, millions are diagnosed with lung cancer, with a significant portion identified at advanced stages where treatment options become limited and prognosis worsens (<xref rid="ref2" ref-type="bibr">2</xref>). Early detection of lung cancer dramatically elevates survival rates and can mitigate the invasiveness and costs of treatments (<xref rid="ref3" ref-type="bibr">3</xref>). Often, early-stage lung cancer presents with little to no discernible symptoms, meaning that by the time symptoms do emerge, the cancer may have metastasized (<xref rid="ref4" ref-type="bibr">4</xref>). Given this, the pivotal role of routine screening, particularly via computed tomography (CT) scans known for their high resolution and sensitivity, becomes evident in enhancing early diagnosis (<xref rid="ref5" ref-type="bibr">5</xref>).</p><p>Solitary pulmonary nodules (SPN) present as distinct, rounded opacities often smaller than 3&#x0202f;cm in diameter, typically encapsulated entirely within the pulmonary parenchyma without adjacent lung abnormalities (<xref rid="ref6" ref-type="bibr">6</xref>). These radiographic features, while seemingly straightforward, hide a more complex underlying reality. In the clinical realm, a significant proportion of malignant peripheral SPNs are identified as solid lung adenocarcinomas (SLA) (<xref rid="ref7" ref-type="bibr">7</xref>). However, a confounding factor in SPN evaluation emerges with pulmonary granulomatous nodules (PGN). These benign lesions can sometimes demonstrate spiculated or lobulated appearances on CT scans, closely mimicking the characteristics of their malignant counterparts, making differentiation between the two an intricate task (<xref rid="ref8" ref-type="bibr">8</xref>). The striking radiological similarities between PGN and SLA have led to a reliance on invasive diagnostic tools, primarily percutaneous needle biopsies, to obtain a definitive diagnosis (<xref rid="ref9" ref-type="bibr">9</xref>). While effective, these procedures come with their own set of challenges, often escalating the discomfort, anxiety, and potential complications for patients. In this context, the potential of a purely CT imaging-based diagnostic method not only represents a step forward in terms of patient comfort but also holds the promise of expediting diagnosis and subsequent treatments, thus revolutionizing the clinical approach to SPN (<xref rid="ref3" ref-type="bibr">3</xref>).</p><p>Computer-Aided Diagnosis (CAD), especially when bolstered by deep learning techniques, offers a novel approach to diagnosing lung conditions, such as differentiating isolated granulomas from adenocarcinomas. CAD&#x02019;s consistent and objective evaluations often exceed the capabilities of human expertise, which can be affected by factors such as fatigue, biases, and limited data processing abilities (<xref rid="ref10" ref-type="bibr">10</xref>). Notably, deep learning, a sophisticated branch of machine learning, swiftly scans countless image slices, identifying minor abnormalities with remarkable accuracy (<xref rid="ref11" ref-type="bibr">11</xref>). Trained on extensive datasets, these algorithms differentiate benign and malignant SPNs, thereby minimizing unnecessary biopsies and enhancing diagnostic reliability (<xref rid="ref12" ref-type="bibr">12</xref>). Lakhani et al. (<xref rid="ref13" ref-type="bibr">13</xref>) developed a convolutional neural network (CNN) that could effectively classify pulmonary tuberculosis from chest radiographs. Lee et al. (<xref rid="ref14" ref-type="bibr">14</xref>) innovated a deep learning-based CAD system that excels at localizing and diagnosing metastatic brain tumors thus redefining the efficiency of tumor identification. Rajkomar et al. (<xref rid="ref15" ref-type="bibr">15</xref>) employed machine learning to predict patient outcomes, such as unexpected readmissions, using routinely collected data during hospital admissions. However, classifying pulmonary nodules is challenging as benign and malignant lesions share similar imaging features, like morphology, edge characteristics, and tissue density, especially in CT images. This complicates automated diagnosis compared to conditions like tuberculosis or brain tumors. A key limitation of using deep learning for this task is the need for large, accurately annotated medical datasets, which require significant time and financial resources to obtain (<xref rid="ref16" ref-type="bibr">16</xref>).</p><p>In recent years, the advent of self-supervised learning (SSL) has been recognized as a potential solution to the challenges posed by the need for meticulously annotated medical data (<xref rid="ref17" ref-type="bibr">17</xref>). Distinguished from traditional supervised learning, SSL capitalizes on unlabeled data, deriving proxy tasks from the data itself to train models without human annotation (<xref rid="ref18" ref-type="bibr">18</xref>). This technique not only alleviates the constraints and costs associated with data labeling but also harnesses the vast volumes of unlabeled medical images available, often yielding results comparable to, if not surpassing, supervised methods (<xref rid="ref19" ref-type="bibr">19</xref>). In the realm of SSL, two dominant paradigms have notably emerged: generative and contrastive SSL.</p><p>Despite the pivotal advancements in self-supervised learning, two significant challenges persist. These challenges are often overlooked in prevailing literature: The first challenge pertains to the disparities between natural and medical imaging modalities. Medical images predominantly stem from radiographic, functional, magnetic resonance, and ultrasonic imaging modalities, whereas natural images are primarily captured through ambient light. This distinction underscores significant disparities, affecting both the application and the design of algorithms. Medical images are typically acquired through controlled environments with specific imaging devices, resulting in highly standardized formats, such as 3D single-channel grayscale representations for CT scans. In contrast, natural images come from natural environments and capture a broad spectrum of colors and structures under varying lighting conditions. This divergence impacts the algorithms designed for image classification. For example, traditional deep learning models are optimized for 2D, color-rich natural images, making it difficult to apply the same models to 3D grayscale medical images without losing crucial information. Additionally, natural images often contain a wide range of textures and diverse features that are not typically observed in medical images, where anatomical structures and subtle pathological variations are paramount. Additionally, the high similarity among medical images from the same anatomical region, even in healthy subjects, presents another unique challenge: minute differences between benign and malignant nodules may appear nearly identical, requiring algorithms to identify subtle variations that are critical for accurate diagnosis. The challenge here is evident: leveraging the success of SSL techniques in natural imaging for medical applications requires a comprehensive re-evaluation from a radiological perspective, followed by the formulation of a tailored approach. The second challenge involves the dichotomy between generative and contrastive SSL methodologies. Generative SSL focuses on pixel reconstruction, calculating the loss between the generated and original image, emphasizing intra-instance feature variations (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). Conversely, contrastive SSL focuses on constructing positive and negative sample pairs, thereby determining the contrastive similarity metric and emphasizing inter-instance feature differences (<xref rid="fig1" ref-type="fig">Figure 1B</xref>), Given the inherent complexity and diverse feature information within medical images, integrating these two SSL methodologies could prove beneficial.</p><fig position="float" id="fig1"><label>Figure 1</label><caption><p>The general structure of self-supervised learning. <bold>(A)</bold> Generative self-supervised learning; <bold>(B)</bold> Contrastive self-supervised learning; <bold>(C)</bold> The proposed hybrid self-supervised learning.</p></caption><graphic xlink:href="fmed-12-1507258-g001" position="float"/></fig><p>Given the distinct challenges arising from the disparities between natural and medical images and the nuanced differences between contrastive and generative SSLs, conventional neural networks often struggle in classifying pulmonary granulomatous nodules and solid lung adenocarcinomas. Consequently, developing a specialized self-supervised learning model that is custom-designed for this task and adept at navigating these challenges is crucial. To this end, we propose MAEMC-NET, an innovative self-supervised learning network conceived to address the limitations of existing methodologies. <xref rid="fig1" ref-type="fig">Figure 1C</xref> shows an approximate structure of the model. MAEMC-NET is designed for medical image classification, particularly focusing on differentiating Pulmonary granulomatous nodules and solid lung adenocarcinomas from CT scans of lung cancer patients. The architecture of MAEMC-NET integrates several critical components to achieve its outstanding effectiveness. Firstly, and most importantly, we have curated sample pairs by extracting regions of interest (ROI) from the axial and coronal planes of each patient case, thus ensuring a comprehensive and diverse dataset. Subsequently, the MAEMC-NET model, a hybrid of SSL strategies, effectively combines the strengths of both generative and contrastive SSLs. This integration allows the MAEMC-NET to exploit the capability to distinguish between inter-instance feature differences while also emphasizing intra-instance feature variations. Additionally, an optimized transformer module encoder has been developed, incorporating a dimensionality reduction module. This design enables a more effective synergy of contrastive and generative SSL techniques, further enhancing the performance of the model.</p><p>In this study, we present several key contributions: (1) We integrate generative and contrastive self-supervised learning techniques to develop robust CT image representations for both intra-and inter-solitary pulmonary nodules. (2) A novel generative self-supervised task is introduced, focusing on reconstructing masked axial CT patches that encompass lesions. This approach enhances our understanding of intra-inter-slice image representation. (3) We employ contrastive momentum to establish a connection between the encoder used in the axial-CT-patch pathway and the momentum encoder in the coronal-CT-patch pathway, thereby improving the coherence of our model. (4) To facilitate the computation of infoNCE loss in contrastive learning, we introduce a feature flattening module. This addition streamlines the processing and enhances the effectiveness of our methodology. (5) Through extensive comparative and ablation studies, we demonstrate the superior performance of our proposed model in predicting the malignancy of solitary pulmonary nodules.</p></sec><sec id="sec2"><label>2</label><title>Related works</title><sec id="sec3"><label>2.1</label><title>Generative SSL and its applications in the medical field</title><p>Generative SSL has emerged as a powerful paradigm in machine learning, particularly in the medical domain, where labeled data is often scarce and expensive to obtain. This approach revolves around training models to understand the underlying data distribution by generating or reconstructing data samples without explicit supervision. In the medical field, where accurate diagnosis and interpretation of images are paramount, generative self-supervised learning techniques play a crucial role in tasks such as disease classification, anomaly detection, and image reconstruction. <xref rid="fig1" ref-type="fig">Figure 1A</xref> shows an approximate structure of the model.</p><p>One of the seminal contributions in generative SSL is the Variational Autoencoder, proposed by Kingma et al. (<xref rid="ref20" ref-type="bibr">20</xref>), which introduced a mechanism to approximate complex data distributions using a probabilistic framework. Meanwhile, Creswell et al. (<xref rid="ref21" ref-type="bibr">21</xref>) ushered in a transformative approach with Generative Adversarial Networks, wherein a duo of neural networks (generator and discriminator) engage in an adversarial game to create synthetic data that closely mirrors real data. The Denoising Autoencoder, conceptualized by Vincent et al. (<xref rid="ref22" ref-type="bibr">22</xref>), is designed to reconstruct slightly corrupted input data. By prioritizing the reconstruction of this perturbed data, the model naturally learns to capture essential structures while discarding noise. Another significant contribution is a masked autoencoder architecture for scalable learning in computer vision tasks (<xref rid="ref23" ref-type="bibr">23</xref>). This approach emphasizes the utilization of sparse representations and convolutional layers to facilitate efficient feature extraction and dimensionality reduction in large-scale visual datasets.</p><p>In the medical domain, generative SSL has been instrumental in various applications. A notable example is the work of Chen et al. (<xref rid="ref24" ref-type="bibr">24</xref>), which introduces a SSL method for medical image analysis through image context restoration. Their model is trained to restore missing or corrupted regions in images, thereby enhancing representation learning for medical imagery. Taleb et al. (<xref rid="ref25" ref-type="bibr">25</xref>) have also contributed to this field with their SSL framework based on image reconstruction. Using a generative model, their approach learns representations by reconstructing medical images, proving effective across various medical imaging tasks. Hu et al.&#x02019;s research introduces Differentiable Architecture Search (DARTS) for 3D medical image analysis (<xref rid="ref26" ref-type="bibr">26</xref>). Although not strictly a generative SSL method, it employs SSL to optimize neural network architectures, thereby enhancing performance in medical image tasks. Zhu et al. (<xref rid="ref27" ref-type="bibr">27</xref>) have developed DeepEM, a method for weakly supervised pulmonary nodule detection using SSL. The model utilizes self-generated pseudo-labels and reconstructs 3D image patches, leading to improved nodule detection in medical imaging.</p><p>In a different application, Halimi et al.&#x02019;s work, although not in the medical sector, introduces a self-supervised approach for dense correspondence learning (<xref rid="ref28" ref-type="bibr">28</xref>). This method is significant for medical image-based 3D reconstruction tasks, enabling the generation of 3D models from 2D medical images. Another study by Chen et al. (<xref rid="ref29" ref-type="bibr">29</xref>) explores a generative SSL approach where a network is trained to predict pixel values in medical images. This enhances the network&#x02019;s capability to comprehend complex image structures. Lastly, Taleb et al.&#x02019;s research focuses on the use of 3D medical images for SSL (<xref rid="ref30" ref-type="bibr">30</xref>). Their model learns to reconstruct 3D medical scans, aiding in tasks like anomaly detection.</p><p>In summary, generative SSL is rapidly transforming the landscape of medical image analysis. Through innovative approaches like image context restoration, image reconstruction, optimization of neural network architectures, and 3D image processing, researchers are pushing the boundaries of what&#x02019;s possible in medical diagnostics and treatment planning. These advancements not only demonstrate the versatility and power of generative SSL in handling complex, unlabeled medical data, but they also pave the way for more accurate, efficient, and accessible healthcare solutions. Despite promising results, generative SSL faces limitations in medical imaging. Models like variational autoencoders and GANs struggle with capturing fine-grained pathological features and handling the complexities of medical images. Moreover, unsupervised learning may lead to suboptimal representations when data distributions differ from the training data, highlighting the need for more specialized models.</p></sec><sec id="sec4"><label>2.2</label><title>Contrastive SSL and its applications in the medical field</title><p>Contrastive SSL is a potent technique in machine learning, particularly within computer vision. It operates by discerning positive and negative pairs of data samples, optimizing neural architectures to minimize the distance between positive pairs while maximizing that between negative pairs. <xref rid="fig1" ref-type="fig">Figure 1B</xref> shows an approximate structure of the model. Several influential frameworks have emerged within this approach: SimCLR, pioneered by Chen et al. (<xref rid="ref31" ref-type="bibr">31</xref>), harnesses data augmentations to extract positive pairs from identical images, demonstrating robust performance across various visual representation tasks. Meanwhile, the Momentum Contrast method, introduced by He et al., extends SimCLR by utilizing a momentum-based encoder to dynamically update a sample dictionary during contrastive learning, particularly effective in handling restricted dictionary sizes (<xref rid="ref19" ref-type="bibr">19</xref>). SimCLR and Momentum Contrast work well with 2D images but face challenges with 3D medical images. For lung CT images, 3D-specific augmentations like rotation, translation, and scaling are needed to maintain spatial consistency and capture texture details. Additionally, Grill et al. (<xref rid="ref32" ref-type="bibr">32</xref>) innovatively introduced Bootstrap Your Own Latent (BYOL), which eliminates the need for negative pairs altogether. Instead, it utilizes two differently augmented views of the same image, simplifying the learning process while maintaining comparable performance.</p><p>In the medical field, contrastive SSL has made significant contributions, particularly in disease classification, interpretation, and various medical imaging tasks. For instance, Chen et al. (<xref rid="ref24" ref-type="bibr">24</xref>) proposed a new self-supervised learning strategy, a context-based recovery strategy, which can effectively learn the semantic features of medical images without labeled data and significantly improve the performance of the model in tasks such as classification, localization, and segmentation. In another approach, Zhang et al. (<xref rid="ref33" ref-type="bibr">33</xref>) The ConVIRT model is proposed, which is an unsupervised learning method that combines medical images and natural language description text. It can learn effective visual representations of medical images and performs well in multiple medical image tasks. In addition, compared with the traditional ImageNet pre-trained model, the ConVIRT method not only performs better in classification tasks, but also is more efficient in data utilization, especially when labeled data is scarce. Similarly, Zhuang et al. (<xref rid="ref34" ref-type="bibr">34</xref>) adopted an innovative strategy by training a model on 3D medical images to solve a Rubik&#x02019;s cube-like puzzle, facilitating the extraction of rich, transferable features for pathology analysis. Moreover, advancements in contrastive SSL have led to novel applications in histopathology image analysis. Srinidhi et al. (<xref rid="ref35" ref-type="bibr">35</xref>) introduced an innovative framework that integrates task-agnostic self-supervised pre-training with task-specific semi-supervised learning consistency strategies. This approach has led to substantial advancements in image analysis tasks within computational pathology, particularly in situations where labeled data is limited. In cardiology, The CLOCS method proposed by Kiyasseh et al. (<xref rid="ref36" ref-type="bibr">36</xref>) is an improved contrastive learning approach that effectively utilizes unlabeled physiological data. By performing contrastive learning across space, time, and patients, it learns more robust data representations. This method demonstrates excellent performance in various downstream tasks, especially when labeled data is scarce, offering strong generalization capabilities and accurate patient-specific predictions, with broad potential for application. Additionally, Xie et al.&#x02019;s work on SSL of graph neural networks, and Wang et al.&#x02019;s research in molecular contrastive learning of representations via graph neural networks have opened new avenues in molecular biology and chemistry (<xref rid="ref37" ref-type="bibr">37</xref>, <xref rid="ref38" ref-type="bibr">38</xref>).</p><p>These diverse applications of contrastive SSL in the medical field not only highlight its versatility but also underscore its potential to revolutionize medical imaging and diagnostics. The ability to leverage unlabeled data effectively, understand complex image-text relationships, and extract meaningful features from various medical data points to promising advancements in diagnostic accuracy and efficiency. Despite its potential, contrastive SSL faces challenges in handling complex medical data and class imbalances, leading to suboptimal feature representations. Additionally, it requires large negative samples, which are often scarce in medical datasets. These issues suggest that integrating both generative and contrastive SSL methods could improve model performance.</p></sec></sec><sec sec-type="materials|methods" id="sec5"><label>3</label><title>Materials and methods</title><sec id="sec6"><label>3.1</label><title>Dataset collection and characteristics</title><p>In order to comprehensively investigate the classification process of PGN and SLA, our research team collaborated with the First Affiliated Hospital of Guangzhou Medical University and Shengjing Hospital of China Medical University. We conducted a retrospective collection of data from 494 cases.</p><sec id="sec7"><label>3.1.1</label><title>Inclusion and exclusion criteria</title><p>The inclusion criteria for cases were as follows: (1) Patients with confirmed pathological diagnoses of pulmonary granulomatous lesions (tuberculous or fungal granulomas) and adenocarcinomas through surgical resection or image-guided biopsies. (2) All patients underwent routine and contrast-enhanced CT scans of the entire chest using the same CT machine and standardized reconstruction parameters within 2&#x0202f;weeks post-surgery. (3) Isolated solid SPNs with sizes ranging from 7 to 30&#x0202f;mm, without calcification or fat content, exhibiting characteristics such as spiculation, lobulation, or pleural indentation, and without associated lung atelectasis or lymph node enlargement. (4) Preoperative laboratory analysis of routine tumor markers (CEA, CA125, CA153) within 1&#x0202f;week before surgery, with positive thresholds set at &#x0003e;5&#x0202f;ng/mL, &#x0003e;35&#x0202f;ng/mL, and&#x0202f;&#x0003e;&#x0202f;25&#x0202f;ng/mL, respectively, according to our institution&#x02019;s reference ranges.</p><p>Exclusion criteria included: (1) Nodules with features highly suggestive of benign lesions, such as caseous necrosis with cavitation in tuberculomas or characteristic halo sign in fungal granulomas. These features are typically associated with granulomatous inflammation or infection, which are non-malignant in nature, and could lead to misclassification of malignant lesions if included. (2) Individuals with a history of other malignant tumors or concurrent malignant tumors. (3) Cases imaged using different algorithms, different slice thicknesses, or on different CT machines. Some typical examples (both CT and pathology images) can be seen in <xref rid="fig2" ref-type="fig">Figure 2</xref>.</p><fig position="float" id="fig2"><label>Figure 2</label><caption><p>Samples of lesion images (CT and pathology) and excluded lesion images for PGN and SLA in Dataset 1 and Dataset 2.</p></caption><graphic xlink:href="fmed-12-1507258-g002" position="float"/></fig></sec><sec id="sec8"><label>3.1.2</label><title>Data collection and preparation</title><p>Among these cases, 333 cases were sourced from the First Affiliated Hospital of Guangzhou Medical University, comprising 105 cases of PGN and 228 cases of SLA. This specific dataset, designated as Dataset 1, served as the basis for model training, testing, and validation. Additionally, an additional 161 cases were gathered from Shengjing Hospital of China Medical University, encompassing 67 cases of PGN and 94 cases of SLA. This external dataset, referred to as Dataset 2, was utilized for external model validation to assess the generalization capability of the model. All CT images of the patients were acquired using a multi-detector CT system (AS+ 128-Slice; Siemens Healthineers, Germany). For Dataset 1, the CT scan parameters were set as follows: Tube Voltage at 120 kVp, Tube Current at an average of 299.88 mAs with a standard deviation of &#x000b1;134.62 mAs, and an average Slice Thickness of 2.31&#x0202f;mm with a standard deviation of &#x000b1;0.71&#x0202f;mm. In the case of Dataset 2, the parameters included a Tube Voltage of 120 kVp, a Tube Current averaging 194.31 mAs with a standard deviation of &#x000b1;116.13 mAs, and an average slice thickness of 4.03&#x0202f;mm with a standard deviation of &#x000b1;1.57&#x0202f;mm. All images from these datasets were exported in the DICOM format and subsequently utilized for image feature extraction by the MAEMC-NET model.</p><p>Two experienced chest radiologists, each possessing 10 and 20&#x0202f;years of experience in interpreting chest images, independently reviewed all CT images stored within our Picture Archiving and Communication System. Any discrepancies were resolved through discussion and consensus. Lesion Size was defined as the maximum diameter of the tumor in the axial image. Spiculation is defined as the presence of linear or pointed extensions that emanate from the edge of a nodule or mass and extend into the lung parenchyma, without reaching the pleural surface. Lobulation is characterized by a wavy, lobulated structure on a portion of the surface of the lesion, excluding areas adjacent to the pleura. Pleural Indentation is identified as a linear structure that originates from the tumor and extends to the pleural surface. These morphological assessments offer essential information for the subsequent analysis and classification of PGN and SLA. We have summarized the clinical characteristics of Dataset 1 in <xref rid="tab1" ref-type="table">Table 1</xref>.</p><table-wrap position="float" id="tab1"><label>Table 1</label><caption><p>Clinic characteristics of dataset 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1">Clinic characteristic</th><th align="center" valign="top" rowspan="1" colspan="1">PGN (<italic>n</italic>&#x0202f;=&#x0202f;105)</th><th align="center" valign="top" rowspan="1" colspan="1">SLA (<italic>n</italic>&#x0202f;=&#x0202f;228)</th><th align="center" valign="top" rowspan="1" colspan="1"><italic>p</italic> value</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Age, mean&#x0202f;&#x000b1;&#x0202f;SD, years</td><td align="center" valign="top" rowspan="1" colspan="1">50.02&#x0202f;&#x000b1;&#x0202f;12.05</td><td align="center" valign="top" rowspan="1" colspan="1">58.66&#x0202f;&#x000b1;&#x0202f;12.63</td><td align="char" valign="top" char="." rowspan="1" colspan="1">&#x0003c;0.001*<sup>a</sup></td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Age, &#x0003c;50/&#x02265;50, years</td><td align="center" valign="top" rowspan="1" colspan="1">49/56</td><td align="center" valign="top" rowspan="1" colspan="1">50/178</td><td align="char" valign="top" char="." rowspan="1" colspan="1">&#x0003c;0.001*<sup>b</sup></td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Gender, Male/Female</td><td align="center" valign="top" rowspan="1" colspan="1">68/37</td><td align="center" valign="top" rowspan="1" colspan="1">118/110</td><td align="char" valign="top" char="." rowspan="1" colspan="1">0.009*<sup>b</sup></td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Spiculated sign, Yes/No</td><td align="center" valign="top" rowspan="1" colspan="1">72/33</td><td align="center" valign="top" rowspan="1" colspan="1">155/73</td><td align="char" valign="top" char="." rowspan="1" colspan="1">0.930<sup>b</sup></td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Lobulated sign, Yes/No</td><td align="center" valign="top" rowspan="1" colspan="1">63/42</td><td align="center" valign="top" rowspan="1" colspan="1">177/51</td><td align="char" valign="top" char="." rowspan="1" colspan="1">0.001*<sup>b</sup></td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Pleural retraction sign, Yes/No</td><td align="center" valign="top" rowspan="1" colspan="1">28/77</td><td align="center" valign="top" rowspan="1" colspan="1">139/89</td><td align="char" valign="top" char="." rowspan="1" colspan="1">&#x0003c;0.001*<sup>b</sup></td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Lesion size, mean&#x0202f;&#x000b1;&#x0202f;SD, cm</td><td align="center" valign="top" rowspan="1" colspan="1">1.81&#x0202f;&#x000b1;&#x0202f;0.62</td><td align="center" valign="top" rowspan="1" colspan="1">2.05&#x0202f;&#x000b1;&#x0202f;0.34</td><td align="char" valign="top" char="." rowspan="1" colspan="1">&#x0003c;0.001*<sup>a</sup></td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">kVp, kV</td><td align="center" valign="top" rowspan="1" colspan="1">120</td><td align="center" valign="top" rowspan="1" colspan="1">120</td><td align="char" valign="top" char="." rowspan="1" colspan="1">&#x02013;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Slice thickness, mm</td><td align="center" valign="top" rowspan="1" colspan="1">2.25&#x0202f;&#x000b1;&#x0202f;1.32</td><td align="center" valign="top" rowspan="1" colspan="1">2.34&#x0202f;&#x000b1;&#x0202f;1.07</td><td align="char" valign="top" char="." rowspan="1" colspan="1">&#x0003c;0.001*<sup>a</sup></td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">X-ray tube current, mean&#x0202f;&#x000b1;&#x0202f;SD, mA</td><td align="center" valign="top" rowspan="1" colspan="1">311.28&#x0202f;&#x000b1;&#x0202f;159.76</td><td align="center" valign="top" rowspan="1" colspan="1">294.63&#x0202f;&#x000b1;&#x0202f;114.27</td><td align="char" valign="top" char="." rowspan="1" colspan="1">&#x0003c;0.001*<sup>a</sup></td></tr></tbody></table><table-wrap-foot><p>*Indicates that the significance is available. <sup>a,b</sup> indicates the two-sample <italic>t</italic>-test and Chi-square test, respectively.</p></table-wrap-foot></table-wrap><p>This retrospective research received approval from our institution&#x02019;s Institutional Review Board, and all procedures were conducted in accordance with the ethical guidelines established by the institution.</p></sec></sec><sec id="sec9"><label>3.2</label><title>Data processing</title><p>To assure the quality and uniformity of the dataset for training and evaluation purposes, a comprehensive series of data processing and augmentation techniques was employed. The preprocessing of the CT images involved critical steps such as standardizing resolution, reducing noise, and enhancing contrast. These measures were instrumental in ensuring dataset consistency and improving the quality of the input data, which is crucial for the subsequent stages of analysis.</p><p>First, given the variability in slice thickness across different scans, an initial step involved the resampling of all CT images to a uniform slice thickness of 1&#x0202f;mm using trilinear interpolation. This meticulous process guaranteed that all images conformed to a consistent format, facilitating seamless subsequent analysis. Furthermore, a critical aspect of the preprocessing pipeline was intensity normalization. The CT images underwent intensity normalization to alleviate variations in image intensity attributed to differences in acquisition settings. This calibration involved standardizing the Hounsfield units (HU) to establish a uniform intensity scale across all images. As a result, all CT data were uniformly configured with a window width of 1,400 HU and a window level of &#x02212;500 HU. Additionally, to reduce interference from surrounding information and focus specifically on the lesions, we selected the center point of each lesion as an anchor. We then extracted square ROI regions with a side length of 50&#x0202f;mm to ensure the entire tumor region, including surrounding tissue or irregularities, was covered. This size accommodated variations in tumor size, capturing both small and large lesions, and prevented clipping of the tumor, ensuring complete coverage. A total of 25 slices containing the tumor lesion were obtained from both the axial and coronal planes. Subsequently, these slices from the axial and coronal planes were sequentially arranged to generate two sets of 5 &#x000d7; 5 sample pairs, which were used as the training dataset for the model.</p><p>To mitigate the risk of overfitting during model training, a variety of data augmentation techniques were applied to enlarge the dataset. This approach successfully enhanced the diversity of the dataset, thereby improving the model&#x02019;s ability to generalize. In this study, specific augmentation techniques such as rotation, horizontal flipping, and four-directional translations centered on the central anchor point were implemented, resulting in the generation of additional valuable data. The incorporation of these techniques substantially increased the dataset size, leading to a total of 13,320 sample pairs in Dataset 1. Subsequently, Dataset 1 was utilized for the self-supervised pre-training of the MAEMC-NET model. During the fine-tuning phase in downstream tasks, the unexpanded original dataset was employed for training, where the data was split into training and testing sets in an 8:2 ratio and subjected to five-fold cross-validation. Dataset 2 was utilized as an external validation set to independently assess the performance of the model.</p></sec><sec id="sec10"><label>3.3</label><title>Architecture of MAEMC-NET</title><p>In our study, we introduce the MAEMC-NET, a novel self-supervised learning network tailored for CT image-based classification of PGN and SLA. This innovative network uniquely combines generative and contrastive self-supervised learning. A custom-designed pretext task is also developed, ensuring a perfect fit for the classification requirements. <xref rid="fig3" ref-type="fig">Figure 3</xref> in our paper provides a detailed overview of the methodology, comprising three integral components: a data processing module, a generative self-supervised learning task module, a contrastive self-supervised learning task module. These modules collaboratively work to significantly enhance the accuracy and efficiency of PGN and SLA classification in CT imaging.</p><fig position="float" id="fig3"><label>Figure 3</label><caption><p>The overview of our proposed MAEMC network.</p></caption><graphic xlink:href="fmed-12-1507258-g003" position="float"/></fig><sec id="sec11"><label>3.3.1</label><title>Data processing module and position embedding</title><p>In alignment with the Vision Transformer (ViT) architecture and the intermediate task designed for this experiment, we amalgamated training set images composed of multiple lesion images (<xref rid="ref39" ref-type="bibr">39</xref>). They were derived from a previously obtained preprocessed dataset extracting sample pairs from multi-slice axial and coronal views of the lesion. We divided these composite images into regular, non-overlapping patches, treating each patch independently. Specifically, we partitioned the 250 &#x000d7; 250 images into 25 uniform, non-overlapping 50<inline-formula><mml:math id="M1" overflow="scroll"><mml:mo>&#x000d7;</mml:mo></mml:math></inline-formula>50 patches, each encompassing a portion of the lesion image. Subsequently, these patches underwent uniform, randomly distributed sampling, with a certain proportion being masked.</p><p>Following this, we employed linear projection to obtain token embeddings for the unmasked patches. To preserve vital positional information, we incorporated positional embedding techniques. For this purpose, we adopted the Sinusoidal Position Embedding method detailed in reference [<xref rid="ref40" ref-type="bibr">40</xref>]. This method involves adding sinusoidal waves of varying frequencies to the embeddings of input tokens. The goal is to achieve the superposition of multiple cosine waves when performing inner products between tokens. This superposition effectively encodes the relative positional information between tokens, representing the spatial relationships within the image. The formula for Sinusoidal Position Embedding is presented below (<xref rid="EQ1" ref-type="disp-formula">Equations 1</xref>&#x02013;<xref rid="EQ2" ref-type="disp-formula">2</xref>):</p><disp-formula id="EQ1">
<label>(1)</label>
<mml:math id="M2" overflow="scroll"><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>sin</mml:mi><mml:mfenced open="(" close=")"><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:msup><mml:mn>10000</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo stretchy="true">/</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mtext mathvariant="italic">model</mml:mtext></mml:msub></mml:mrow></mml:msup></mml:mfrac></mml:mfenced></mml:math>
</disp-formula><disp-formula id="EQ2">
<label>(2)</label>
<mml:math id="M3" overflow="scroll"><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>cos</mml:mi><mml:mfenced open="(" close=")"><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:msup><mml:mn>10000</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo stretchy="true">/</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mtext mathvariant="italic">model</mml:mtext></mml:msub></mml:mrow></mml:msup></mml:mfrac></mml:mfenced></mml:math>
</disp-formula><p>where <inline-formula><mml:math id="M4" overflow="scroll"><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id="M5" overflow="scroll"><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> represent the positional embedding values for even <inline-formula><mml:math id="M6" overflow="scroll"><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and odd <inline-formula><mml:math id="M7" overflow="scroll"><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> dimensions, respectively. <inline-formula><mml:math id="M8" overflow="scroll"><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:math></inline-formula> signifies the specific position in the sequence, while <inline-formula><mml:math id="M9" overflow="scroll"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>mod</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> refers to the embedding dimension of the model. The application of this technique enhances the token embeddings by incorporating positional data, thereby enabling a more detailed and thorough representation of spatial relationships in the image.</p></sec><sec id="sec12"><label>3.3.2</label><title>Hybrid self-supervised learning</title><p>In the realm of SSL, we delve into the synergy of contrastive SSL and generative SSL. Contrastive SSL, as the name suggests, uncovers distinctive image features by contrasting positive and negative instances within a high-dimensional space. On the flip side, generative SSL harnesses the power of image reconstruction to grasp valuable image information. Our observation revealed an interesting facet: the masked autoencoder (MAE) tends to adopt a global perspective when considering an image, while the momentum contrast (MoCo) method leans toward scrutinizing unique image regions, strategically positioning positive and negative examples.</p><p>MoCo is a contrastive SSL model known for its effectiveness in various visual tasks. It employs a unique strategy involving a dynamic dictionary and momentum-based updates. This method allows MoCo to efficiently learn robust and distinct features by contrasting positive and negative data samples. MAE is a generative SSL model renowned for its impressive performance in a range of visual tasks. It utilizes an asymmetric encoder-decoder architecture. Its success is largely due to the use of the Vanilla Vision Transformer, adept at extracting global features from input data. This architecture enables MAE to effectively reconstruct missing parts of the input, thereby learning comprehensive representations.</p><p>To optimize the training of our model and to enhance the capabilities of the Transformer, we introduce Hybrid SSL as a pivotal intermediary task within the MAEMC-NET. This strategic addition empowers the transformer to explore both the idiosyncratic characteristics and holistic image context. Our approach commences with the initialization of two encoders, <inline-formula><mml:math id="M10" overflow="scroll"><mml:mi>E</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M11" overflow="scroll"><mml:mtext mathvariant="italic">ME</mml:mtext></mml:math></inline-formula>, furnished with identical weights. However, it is important to note that <inline-formula><mml:math id="M12" overflow="scroll"><mml:mtext mathvariant="italic">ME</mml:mtext></mml:math></inline-formula> experiences momentum-based updates. Subsequently, a series of random masking operations are applied to augmented images <inline-formula><mml:math id="M13" overflow="scroll"><mml:msup><mml:mi>X</mml:mi><mml:mi>q</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M14" overflow="scroll"><mml:msup><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula>, both originating from the same lesion and sourced from multi-layered axial and coronal planes. This masking operation leads to the creation of mask vectors <inline-formula><mml:math id="M15" overflow="scroll"><mml:mi>q</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M16" overflow="scroll"><mml:mi>k</mml:mi></mml:math></inline-formula>.</p><p>On one front, vectors <inline-formula><mml:math id="M17" overflow="scroll"><mml:mi>q</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M18" overflow="scroll"><mml:mi>k</mml:mi></mml:math></inline-formula> undergo dimension reduction through individual one-dimensional convolution layers, resulting in <inline-formula><mml:math id="M19" overflow="scroll"><mml:msup><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M20" overflow="scroll"><mml:msup><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula>, respectively. Notably, <inline-formula><mml:math id="M21" overflow="scroll"><mml:msup><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> finds its residence in a queue-like memory bank, which continually evolves as new training batches are introduced, seamlessly replacing the oldest ones. Consequently, we employ the InfoNCE loss function to gage the similarity between <inline-formula><mml:math id="M22" overflow="scroll"><mml:msup><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M23" overflow="scroll"><mml:msup><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula>, acting as the cornerstone of our comparative task. The formula for the InfoNCE loss function is as follows (<xref rid="EQ3" ref-type="disp-formula">Equation 3</xref>):</p><disp-formula id="EQ3">
<label>(3)</label>
<mml:math id="M24" overflow="scroll"><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mtext>InfoNCE</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>log</mml:mi><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mi mathvariant="normal">q</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi mathvariant="normal">k</mml:mi><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:msubsup><mml:mo stretchy="true">/</mml:mo><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msubsup><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:msubsup><mml:mi>exp</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mi mathvariant="normal">q</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo stretchy="true">/</mml:mo><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:math>
</disp-formula><p>where <inline-formula><mml:math id="M25" overflow="scroll"><mml:msup><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> serves as the query key, while <inline-formula><mml:math id="M26" overflow="scroll"><mml:msubsup><mml:mi>k</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="M27" overflow="scroll"><mml:msubsup><mml:mi>k</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="M28" overflow="scroll"><mml:msubsup><mml:mi>k</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula> act as keys in the dictionary. Among these, <inline-formula><mml:math id="M29" overflow="scroll"><mml:msup><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M30" overflow="scroll"><mml:msubsup><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula> are treated as positive sample pairs, whereas the other keys function as negative sample pairs for <inline-formula><mml:math id="M31" overflow="scroll"><mml:msup><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula>. Additionally, <inline-formula><mml:math id="M32" overflow="scroll"><mml:mi>&#x003c4;</mml:mi></mml:math></inline-formula> represents a temperature parameter that controls the concentration level of the distribution. By minimizing the infoNCE loss, the model is trained to effectively distinguish between positive and negative sample pairs, thereby enhancing its capability to learn discriminative features from the data.</p><p>The combination of <inline-formula><mml:math id="M33" overflow="scroll"><mml:mi>q</mml:mi></mml:math></inline-formula> with the mask token constitutes the input to the decoder module, which is tasked with predicting the pixel values of the reconstructed mask patch. Subsequently, the evaluation utilizes the Mean Squared Error (MSE) loss function to quantify the discrepancy between the predicted and original pixel values. The formula for the MSE loss function is as follows (<xref rid="EQ4" ref-type="disp-formula">Equation 4</xref>):</p><disp-formula id="EQ4">
<label>(4)</label>
<mml:math id="M34" overflow="scroll"><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant="normal">N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:munderover></mml:mstyle><mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">X</mml:mi><mml:mi mathvariant="normal">q</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mover><mml:msubsup><mml:mi mathvariant="normal">X</mml:mi><mml:mi mathvariant="normal">q</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msubsup><mml:mo>&#x02227;</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:math>
</disp-formula><p>where N represents the total number of missing pixels in the original image, <inline-formula><mml:math id="M35" overflow="scroll"><mml:msubsup><mml:mi>X</mml:mi><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula> denotes the true pixel value of the i-th pixel, and <inline-formula><mml:math id="M36" overflow="scroll"><mml:mover><mml:msubsup><mml:mi>X</mml:mi><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>&#x02227;</mml:mo></mml:mover></mml:math></inline-formula> signifies the predicted pixel value by the model. By minimizing the MSE loss, the model is trained to accurately reconstruct each pixel, thus facilitating the learning of the global features of the image.</p><p>Finally, we combine the InfoNCE loss function with the MSE loss function using a specific temperature coefficient <inline-formula><mml:math id="M37" overflow="scroll"><mml:mi>&#x003bb;</mml:mi></mml:math></inline-formula>, to serve as the loss function to optimize the model. The formula for the loss function is as follows (<xref rid="EQ5" ref-type="disp-formula">Equation 5</xref>):</p><disp-formula id="EQ5">
<label>(5)</label>
<mml:math id="M38" overflow="scroll"><mml:mi mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi mathvariant="normal">L</mml:mi><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi mathvariant="normal">L</mml:mi><mml:mtext>InfoNCE</mml:mtext></mml:msub></mml:math>
</disp-formula></sec><sec id="sec13"><label>3.3.3</label><title>Detailing encoder, decoder, and momentum contrast components</title><p>In this section, we introduce the components of the MAEMC-NET model, including the encoder module, decoder module, and momentum encoder module, along with the specifics of the momentum update operation and the implementation of the dictionary as a queue. The detailed network architecture of this module is visually represented in <xref rid="fig4" ref-type="fig">Figure 4</xref>. The encoder and momentum encoder modules utilize the ViT-large model, characterized by 24 stacked encoder blocks, a token vector length of 1,024, and 16 heads in the multi-head attention mechanism. These modules exclusively process un-masked patches. Initially, image data is segmented into patches of a specified size and then flattened into one-dimensional arrays. Subsequently, these arrays undergo a linear transformation, which is then merged with the position embeddings of the original image and augmented by the addition of a class token at the beginning. Owing to the asymmetric Encoder-Decoder architecture employed, where the encoder only processes un-masked patch information to conserve computational resources, the decoder module is configured with 8 stacked decoder blocks and a token vector length of 512. The decoder is tasked with processing not only the un-masked tokens encoded by the Encoder but also the masked tokens. It is important to note that these masked tokens are not derived from the embedded transformation of the previously masked patches; rather, they are learnable and shared across all masked patches.</p><fig position="float" id="fig4"><label>Figure 4</label><caption><p>The structure of the MAEMC network and its details and parameters.</p></caption><graphic xlink:href="fmed-12-1507258-g004" position="float"/></fig><p>In traditional contrastive learning, the size of the dictionary is typically equivalent to that of the mini-batch. However, this approach is often constrained by the limitations of GPU memory and computational power, preventing the use of large batch sizes. To address this limitation, the MoCo framework employs a queue-based mechanism to store the dictionary, which contains feature vectors of images, allowing for a substantially larger dictionary size. Nonetheless, in cases where the dataset size is not exceptionally large, selecting an overly extensive dictionary can impede model convergence. Therefore, in our implementation, the dictionary size is set to 700, with each vector having a dimensionality of 128. The maintenance of the queue involves enqueuing the feature vectors of the most recent batch of images and dequeuing those of the earliest batch.</p><p>To ensure consistency among the keys in the queue, it is imperative that the Momentum Encoder associated with the dictionary is updated gradually. This is achieved through the implementation of a momentum-based approach. Specifically, after each update of the encoder, only 1% of its updated parameters are used to modify the Momentum Encoder. Such a method ensures the slow and controlled update of the Momentum Encoder, maintaining the stability and consistency of the keys within the queue.</p></sec></sec><sec id="sec14"><label>3.4</label><title>Performance evaluation measures</title><p>We selected commonly used evaluation metrics to assess the performance of our model. These include Area Under the Curve (AUC) (<xref rid="EQ6" ref-type="disp-formula">Equation 6</xref>) with 95% Confidence Interval (95% CI), Accuracy (ACC), Sensitivity (SEN), and Specificity (SPE). AUC quantifies the ability of the model to differentiate between classes.</p><disp-formula id="EQ6">
<label>(6)</label>
<mml:math id="M39" overflow="scroll"><mml:mi>A</mml:mi><mml:mi>U</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:munderover></mml:mstyle><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mfenced open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced><mml:mtext mathvariant="italic">dFPR</mml:mtext><mml:mfenced open="(" close=")"><mml:mi>t</mml:mi></mml:mfenced></mml:math>
</disp-formula><p>where TPR (True Positive Rate) and FPR (False Positive Rate) vary with different thresholds t. AUC is presented with its 95% CI, offering a statistical range indicating where the true AUC value is likely to lie with 95% confidence. This measure enhances the interpretability and reliability of the AUC metric.</p><p>ACC (<xref rid="EQ7" ref-type="disp-formula">Equation 7</xref>) reflects the overall effectiveness of the model in correctly classifying both positive and negative cases. SEN (<xref rid="EQ8" ref-type="disp-formula">Equation 8</xref>) plays a crucial role in determining the proficiency of the model in identifying true positive cases, which is essential for ensuring that no actual cases are overlooked. Conversely, SPE (<xref rid="EQ9" ref-type="disp-formula">Equation 9</xref>) assesses the capability of the model in accurately recognizing negative cases, a critical factor in minimizing the occurrence of false positives.</p><disp-formula id="EQ7">
<label>(7)</label>
<mml:math id="M40" overflow="scroll"><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math>
</disp-formula><disp-formula id="EQ8">
<label>(8)</label>
<mml:math id="M41" overflow="scroll"><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math>
</disp-formula><disp-formula id="EQ9">
<label>(9)</label>
<mml:math id="M42" overflow="scroll"><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:math>
</disp-formula><p>where TP is True Positives, TN is True Negatives, FP is False Positives, and FN is False Negatives.</p></sec><sec id="sec15"><label>3.5</label><title>Training of the models and experiment setting</title><p>We incorporated the MAEMC-NET model, training and testing all variants and comparison models on an NVIDIA GeForce RTX 4070 with 12GB memory. This was implemented using PyTorch (version 1.7), with all graphics created using matplotlib in Python. Our network, an evolution of MAE and MoCo, pre-trained model parameters were not found to be loadable into MAEMC-NET, necessitating training from scratch. To mitigate overfitting, we implemented an early stopping mechanism during the fine-tuning phase. The validation loss was monitored after each epoch, and training was halted if no improvement was observed for 10 consecutive epochs. This helped in preserving the model&#x02019;s generalization ability.</p><p>During pre-training stage, we used grid search to adjust hyperparameters such as learning rate, batch size, and optimizer settings. We tested different combinations of learning rates (1&#x0202f;&#x000d7;&#x0202f;10<sup>&#x02212;2</sup>, 1&#x0202f;&#x000d7;&#x0202f;10<sup>&#x02212;3</sup>, 1&#x0202f;&#x000d7;&#x0202f;10<sup>&#x02212;4</sup>) and batch sizes (32, 64, 128) to determine the best configuration. Finally, the images size was 250&#x0202f;&#x000d7;&#x0202f;250 with a batch size set to 64. Optimization was carried out using the AdamW optimizer with betas&#x0202f;=&#x0202f;(0.9, 0.95), epochs set to 100 with an initial learning rate of 1&#x0202f;&#x000d7;&#x0202f;10<sup>&#x02212;2</sup>, reduced by 0.1 at epochs 120 and 160. In the fine-tuning stage for downstream tasks, we utilized the pre-trained encoder module, adding a two-class fully connected layer, maintaining the same image size and batch size, with 100 epochs and an initial learning rate of 1&#x0202f;&#x000d7;&#x0202f;10<sup>&#x02212;3</sup>, reduced by 0.1 at epochs 40 and 70.</p></sec></sec><sec sec-type="results" id="sec16"><label>4</label><title>Results</title><sec id="sec17"><label>4.1</label><title>Performance of MAEMC-NET and counterparts</title><p>We evaluated the performance of our MAEMC-NET model, aimed at classifying PGN and SLA in CT images, against several state-of-the-art SSL methods, such as SimCLR (<xref rid="ref31" ref-type="bibr">31</xref>), MoCo v1 (<xref rid="ref19" ref-type="bibr">19</xref>), MoCo v2 (<xref rid="ref19" ref-type="bibr">19</xref>), MAE (<xref rid="ref23" ref-type="bibr">23</xref>), CMAE (<xref rid="ref41" ref-type="bibr">41</xref>), and Convnext v2 (<xref rid="ref42" ref-type="bibr">42</xref>). A supervised learning model using the ViT architecture served as the baseline for performance comparison.</p><p>The test accuracy progression of each model over increasing epochs, as depicted in <xref rid="fig5" ref-type="fig">Figure 5A</xref>, highlights that our MAEMC-NET model, although initially moderate in performance, consistently achieved the highest test accuracy after epoch 75. <xref rid="fig5" ref-type="fig">Figure 5B</xref> presents the training loss curves of these models, indicating an initial slight increase in loss for the MAE model but eventual convergence to optimal accuracy for all models. <xref rid="tab2" ref-type="table">Table 2</xref> showcases the final results: MAEMC-NET achieved an AUC of 0.962 and an ACC of 93.7% on our PGN and SLA dataset, outperforming the baseline ViT model by 0.032 in AUC and 3.6% in ACC. Additionally, it improved upon the previously best-performing model, CMAE (AUC 0.957, ACC 92.5%), by 1.2% in ACC. It is noteworthy that in all models, SPE was higher than SEN, a reflection of the class imbalance in our dataset, which has a greater number of SLA compared to PGN. This imbalance is common in medical datasets, where malignant cases often outnumber benign ones. Despite this, our model also excelled in sensitivity and specificity, achieving the best results with 91.2 and 95.9%, respectively. To further evaluate the generalization capability of the MAEMC-NET model, we conducted an assessment using Dataset 2. The model achieved an AUC of 0.949 and an ACC of 91.5%. Additionally, the sensitivity and specificity were 90.3 and 92.8%, respectively. These results indicate that the MAEMC-NET model not only performs well on the original PGN and SLA dataset but also maintains high performance when applied to an independent external dataset, underscoring its robust generalization ability.</p><fig position="float" id="fig5"><label>Figure 5</label><caption><p>Comparison of the MAEMC-NET model and state-of-the-art SSL methods. <bold>(A)</bold> Test accuracy curves; <bold>(B)</bold> Training loss curves.</p></caption><graphic xlink:href="fmed-12-1507258-g005" position="float"/></fig><table-wrap position="float" id="tab2"><label>Table 2</label><caption><p>Results of the comparison between MAEMC-NET and the state-of-the-art methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1">Method</th><th align="center" valign="top" rowspan="1" colspan="1">AUC (95% CI)</th><th align="center" valign="top" rowspan="1" colspan="1">ACC</th><th align="center" valign="top" rowspan="1" colspan="1">SEN</th><th align="center" valign="top" rowspan="1" colspan="1">SPE</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Supervised learning (ViT) (<xref rid="ref39" ref-type="bibr">39</xref>)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.930 (0.912&#x02013;0.947)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">90.1%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">87.3%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">92.2%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">MoCo v1 (<xref rid="ref19" ref-type="bibr">19</xref>)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.907 (0.883&#x02013;0.929)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">88.4%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">85.8%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">90.1%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">MoCo v2 (<xref rid="ref19" ref-type="bibr">19</xref>)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.918 (0.894&#x02013;0.942)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">89.2%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">86.5%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">91.3%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">MAE (<xref rid="ref23" ref-type="bibr">23</xref>)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.937 (0.915&#x02013;0.959)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">91.3%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">88.7%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">93.4%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">SimCLR (<xref rid="ref31" ref-type="bibr">31</xref>)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.951 (0.927&#x02013;0.975)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">92.0%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">89.4%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">94.6%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">CMAE (<xref rid="ref41" ref-type="bibr">41</xref>)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.957 (0.931&#x02013;0.966)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">92.5%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">89.9%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">94.3%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Convnext v2 (<xref rid="ref42" ref-type="bibr">42</xref>)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.935 (0.914&#x02013;0.951)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">90.8%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">87.5%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">92.6%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">MAEMC-NET (Ours)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.962 (0.934&#x02013;0.973)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">93.7%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">91.2%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">95.9%</td></tr></tbody></table></table-wrap></sec><sec id="sec18"><label>4.2</label><title>Ablation analysis</title><p>Our MAEMC-NET model consists of three pivotal modules: Firstly, a data processing module (DP) designed for generating pretext tasks in pretraining. This module plays a crucial role in preparing the dataset for subsequent learning tasks. Secondly, the generative SSL (GSSL) module emphasizes pixel reconstruction with a global perspective, aiming to capture valuable image information efficiently. This approach proves to be essential in understanding the intricate details and broader context of medical images. Thirdly, the contrastive SSL (CSSL) module, which concentrates on discerning unique image features by contrasting high-dimensional instances, focusing especially on distinctive image regions. This method plays a pivotal role in enhancing the discriminative power of the model.</p><p>To assess the impact of each module on the classification of PGN and SLA in CT images, we conducted comprehensive ablation experiments. As indicated in <xref rid="tab3" ref-type="table">Table 3</xref>, we initially replicated the MoCo model as a baseline for contrastive SSL and the MAE model for generative SSL. Both models were then enhanced using our data processing module for pretext tasks. This enhancement led to significant improvements in performance metrics, with notable increases in AUC and ACC for both models. We attribute this improvement primarily to our tailored pretext tasks and data processing techniques, which align more closely with the extraction of pathological features from medical images.</p><table-wrap position="float" id="tab3"><label>Table 3</label><caption><p>Ablation studies for MAEMC-NET.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" colspan="3" rowspan="1">Method</th><th align="center" valign="top" rowspan="2" colspan="1">AUC (95%CI)</th><th align="center" valign="top" rowspan="2" colspan="1">ACC</th><th align="center" valign="top" rowspan="2" colspan="1">SEN</th><th align="center" valign="top" rowspan="2" colspan="1">SPE</th></tr><tr><th align="left" valign="top" rowspan="1" colspan="1">DP</th><th align="center" valign="top" rowspan="1" colspan="1">CSSL</th><th align="center" valign="top" rowspan="1" colspan="1">GSSL</th></tr></thead><tbody><tr><td rowspan="1" colspan="1"/><td align="center" valign="top" rowspan="1" colspan="1">&#x02713;</td><td rowspan="1" colspan="1"/><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.875 (0.853&#x02013;0.897)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">86.7%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">83.3%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">89.6%</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="center" valign="top" rowspan="1" colspan="1">&#x02713;</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.892 (0.870&#x02013;0.914)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">87.5%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">84.7%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">90.3%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="top" rowspan="1" colspan="1">&#x02713;</td><td rowspan="1" colspan="1"/><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.907 (0.883&#x02013;0.929)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">88.4%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">85.8%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">90.1%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02713;</td><td rowspan="1" colspan="1"/><td align="center" valign="top" rowspan="1" colspan="1">&#x02713;</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.937 (0.915&#x02013;0.959)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">91.3%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">88.7%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">93.4%</td></tr><tr><td rowspan="1" colspan="1"/><td align="center" valign="top" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="top" rowspan="1" colspan="1">&#x02713;</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.925 (0.903&#x02013;0.947)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">90.9%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">87.8%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">93.7%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="top" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="top" rowspan="1" colspan="1">&#x02713;</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.962 (0.934&#x02013;0.973)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">93.7%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">91.2%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">95.9%</td></tr></tbody></table></table-wrap><p>Furthermore, we experimented with a hybrid model that combines both the MoCo and MAE models. This experiment yielded substantial improvements in performance over the individual models, validating the effectiveness of integrating contrastive and generative SSL approaches. The hybrid model capitalizes on the strengths of both learning strategies, merging the detailed recovery and contextual understanding characteristic of generative learning with the feature distinction and relational understanding central to contrastive learning.</p><p>Finally, our MAEMC-NET model, which incorporates all three modules, achieved the most superior results. This outcome further illustrates the indispensable role of each module in the overall performance of our model, highlighting the synergy achieved through their combination.</p></sec><sec id="sec19"><label>4.3</label><title>Optimizing mask ratios in MAEMC-NET</title><p>This section investigates the influence of various mask ratios on our MAEMC-NET model during its generative SSL phase. The mask ratio is an essential factor, representing the proportion of the input image that is masked or hidden during the training process. Our experiments focused on different ratios to strike a delicate balance between providing the model with a sufficient challenge for learning robust features, and retaining enough visible information to ensure effective reconstruction.</p><p>As evidenced in <xref rid="fig6" ref-type="fig">Figure 6</xref> and <xref rid="tab4" ref-type="table">Table 4</xref>, we observed that a 50% mask ratio delivered the most favorable outcomes for our model on the CT image dataset, specifically in distinguishing between PGN and SLA. This finding stands in stark contrast to the optimal 75% mask ratio observed in applications involving natural images. We attribute this variance to the complex and dense nature of medical images. Lesion images, for instance, contain a wealth of tissue and pathological features, offering a higher information density compared to natural images. Therefore, the MAEMC-NET model requires access to more image information for the accurate reconstruction of medical images.</p><fig position="float" id="fig6"><label>Figure 6</label><caption><p>A line graph of the test accuracy of MAEMC-NET at different mask ratios.</p></caption><graphic xlink:href="fmed-12-1507258-g006" position="float"/></fig><table-wrap position="float" id="tab4"><label>Table 4</label><caption><p>The performance of MAEMC-NET under different mask ratios.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1">Mask Ratio</th><th align="center" valign="top" rowspan="1" colspan="1">AUC (95%CI)</th><th align="center" valign="top" rowspan="1" colspan="1">ACC</th><th align="center" valign="top" rowspan="1" colspan="1">SEN</th><th align="center" valign="top" rowspan="1" colspan="1">SPE</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">20%</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.933 (0.906&#x02013;0.954)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">90.2%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">88.7%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">91.8%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">30%</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.941 (0.912&#x02013;0.967)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">92.1%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">90.2%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">93.5%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">40%</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.954 (0.923&#x02013;0.972)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">92.6%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">90.8%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">93.4%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">50% (Ours)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.962 (0.934&#x02013;0.973)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">93.7%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">91.2%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">95.9%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">60%</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.968 (0.935&#x02013;0.973)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">93.2%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">91.6%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">94.4%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">70%</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.957 (0.924&#x02013;0.961)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">92.8%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">90.3%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">94.7%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">75%</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.955 (0.923&#x02013;0.965)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">92.3%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">89.8%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">95.5%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">80%</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.946 (0.910&#x02013;0.957)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">91.8%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">89.3%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">93.0%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">90%</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.932 (0.907&#x02013;0.948)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">90.8%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">88.4%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">92.2%</td></tr></tbody></table></table-wrap><p>Furthermore, our experimental results across different mask ratios have consistently shown superior performance of our model over other comparative models, regardless of the fluctuation in results due to varying mask ratios. This consistent outperformance underlines the robustness and adaptability of MAEMC-NET, further validating its superiority in handling medical image classification tasks.</p></sec><sec id="sec20"><label>4.4</label><title>Evaluation of different backbone networks in MAEMC-NET</title><p>The backbone network plays a pivotal role in our MAEMC-NET model, and this section provides an in-depth analysis of its impact on the performance of the model. We evaluated various networks, including Vit-base, Swin Transformer (<xref rid="ref43" ref-type="bibr">43</xref>), DeiT (<xref rid="ref44" ref-type="bibr">44</xref>), LeVit (<xref rid="ref45" ref-type="bibr">45</xref>), PVT (<xref rid="ref46" ref-type="bibr">46</xref>), and our customized Vit-Large, particularly focusing on their efficacy in classifying PGN and SLA in CT images. The experimental results are shown in <xref rid="tab5" ref-type="table">Table 5</xref>, and the assessment utilized metrics such as AUC, ACC, SEN, and SPE.</p><table-wrap position="float" id="tab5"><label>Table 5</label><caption><p>Comparison of classification performance with different backbones on the dataset 1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1">Backbone</th><th align="center" valign="top" rowspan="1" colspan="1">AUC (95%CI)</th><th align="center" valign="top" rowspan="1" colspan="1">ACC</th><th align="center" valign="top" rowspan="1" colspan="1">SEN</th><th align="center" valign="top" rowspan="1" colspan="1">SPE</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Vit-base (<xref rid="ref23" ref-type="bibr">23</xref>)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.927 (0.903&#x02013;0.951)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">90.3%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">87.9%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">92.7%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Swin Transformer (<xref rid="ref43" ref-type="bibr">43</xref>)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.943 (0.921&#x02013;0.965)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">91.9%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">89.4%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">94.3%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">DeiT (<xref rid="ref44" ref-type="bibr">44</xref>)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.938 (0.915&#x02013;0.961)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">91.6%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">89.1%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">93.9%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">LeVit (<xref rid="ref45" ref-type="bibr">45</xref>)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.947 (0.929&#x02013;0.975)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">92.3%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">90.2%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">95.1%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">PVT (<xref rid="ref46" ref-type="bibr">46</xref>)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.943 (0.924&#x02013;0.970)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">92.1%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">89.7%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">94.6%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Vit-Large (Ours)</td><td align="char" valign="top" char="(" rowspan="1" colspan="1">0.962 (0.934&#x02013;0.973)</td><td align="char" valign="top" char="." rowspan="1" colspan="1">93.7%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">91.2%</td><td align="char" valign="top" char="." rowspan="1" colspan="1">95.9%</td></tr></tbody></table></table-wrap><p>Our modified Vit-Large backbone network stood out, showcasing an enhanced ability in processing the intricate details of medical imaging data. Unlike other networks such as Swin Transformer and DeiT, which have their strengths, they not uniformly extract features across resolutions or effectively capture the subtle nuances typical in medical images. In comparison to Swin Transformer, it achieved an increased AUC of 0.019 and an ACC improvement of 1.8%. Against DeiT, our model showed further superiority with a 0.024 increase in AUC and a 2.1% boost in ACC. LeVit and PVT, while efficient in their respective domains, lack in the depth and complexity of feature extraction crucial for medical imaging. Therefore, our model to outperform LeVit with a 0.015 AUC and 1.4% ACC increase, and PVT with a 0.019 AUC and 1.6% ACC improvement.</p><p>Consequently, LeVit and PVT struggle with medical image tasks due to their limited ability to capture detailed features and global context. In contrast, Vit-Large&#x02019;s global self-attention mechanism excels at extracting complex, high-level features essential for accurate PGN and SLA classification in CT images. Its architecture is well-suited for tasks requiring precise feature extraction and detailed analysis.</p></sec><sec id="sec21"><label>4.5</label><title>T-SNE analysis of original image and model-extracted features</title><p>We employed t-SNE for dimensionality reduction on both original images and high-dimensional feature maps extracted by our model from the training and test sets (<xref rid="ref47" ref-type="bibr">47</xref>). <xref rid="fig7" ref-type="fig">Figure 7A</xref> displays the 2D t-SNE projection of the original training set images, where PGN and SLA are intermixed. This mix illustrates that the raw images of PGN and SLA lack distinct, separable features in a lower-dimensional space, highlighting the complexity of classifying these conditions from raw images alone. In contrast, <xref rid="fig7" ref-type="fig">Figure 7C</xref> presents the 2D projection of the high-dimensional feature maps extracted by the model. This visualization shows a clear distinction between PGN and SLA, with only a minor overlap. The marked separation achieved in this projection highlights the effectiveness of our model in extracting and transforming raw image data into a format enriched with discernible features. The distinct clustering of PGN and SLA in this space demonstrates the ability of the model to identify and emphasize unique class characteristics, thereby significantly enhancing the accuracy of medical imaging classification.</p><fig position="float" id="fig7"><label>Figure 7</label><caption><p>T-SNE projections demonstrating the comparative analysis of original and model-extracted features for PGN and SLA classification. <bold>(A)</bold> The original images of the training set; <bold>(B)</bold> The original images of the testing set; <bold>(C)</bold> The model-extracted features of training set; <bold>(D)</bold> model-extracted features of testing set.</p></caption><graphic xlink:href="fmed-12-1507258-g007" position="float"/></fig><p>Furthermore, <xref rid="fig7" ref-type="fig">Figures 7B</xref>,<xref rid="fig7" ref-type="fig">D</xref> display the t-SNE results for the original images of the test set and the feature maps extracted by the model, respectively. The resemblance of <xref rid="fig7" ref-type="fig">Figures 7A</xref>&#x02013;<xref rid="fig7" ref-type="fig">D</xref> indicates a consistency in the performance of the model. This similarity across training and validation sets indicates the robustness and generalizability of our model, affirming its capability to effectively differentiate between PGN and SLA in varied datasets. In a word, this comparison between the original images and the model-processed feature maps via t-SNE projection powerfully demonstrates the value added by our MAEMC-NET in medical image analysis.</p></sec><sec id="sec22"><label>4.6</label><title>Comparison of reconstructed images between MAEMC-NET model and MAE model</title><p>To further validate the performance of the MAEMC-NET model proposed in this study, we conducted a comparative analysis by individually assessing the reconstructed images generated by the trained model and those produced by the MAE model against the target images, In <xref rid="fig8" ref-type="fig">Figure 8</xref>, we present a series of axial and coronal planes originating from the same lesion, reconstructed separately using the MAEMC-NET model and the MAE model, juxtaposed with the corresponding target images. Each image is accompanied by a magnified view of the same region on its right side, revealing a remarkable resemblance between the reconstructed images of the coronal planes generated by our model and the target images. Furthermore, we computed the Mean Absolute Error (MAE), Mean Squared Error (MSE), Structural Similarity Index (SSIM), and Peak Signal-to-Noise Ratio (PSNR) values for both sets of images, which are summarized in <xref rid="tab6" ref-type="table">Table 6</xref>.</p><fig position="float" id="fig8"><label>Figure 8</label><caption><p>A pair of reconstructed images of axial planes and coronal planes from our model and the MAE model.</p></caption><graphic xlink:href="fmed-12-1507258-g008" position="float"/></fig><table-wrap position="float" id="tab6"><label>Table 6</label><caption><p>Result of our models and MAE.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="2" colspan="1">Model</th><th align="center" valign="top" colspan="4" rowspan="1">Axial planes</th><th align="center" valign="top" colspan="4" rowspan="1">Coronal planes</th></tr><tr><th align="center" valign="top" rowspan="1" colspan="1">MAE</th><th align="center" valign="top" rowspan="1" colspan="1">MSE</th><th align="center" valign="top" rowspan="1" colspan="1">PSNR</th><th align="center" valign="top" rowspan="1" colspan="1">SSIM</th><th align="center" valign="top" rowspan="1" colspan="1">MAE</th><th align="center" valign="top" rowspan="1" colspan="1">MSE</th><th align="center" valign="top" rowspan="1" colspan="1">PSNR</th><th align="center" valign="top" rowspan="1" colspan="1">SSIM</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">MAE (<xref rid="ref23" ref-type="bibr">23</xref>)</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">11.17 &#x000b1; 2.01</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">505.02 &#x000b1; 264.84</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">21.09 &#x000b1; 1.94</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">0.81 &#x000b1; 0.02</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">8.92 &#x000b1; 1.16</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">327.91 &#x000b1; 172.24</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">22.97 &#x000b1; 1.54</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">0.86 &#x000b1; 0.02</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Ours</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">12.03 &#x000b1; 2.13</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">628.21 &#x000b1; 325.71</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">20.15 &#x000b1; 1.63</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">0.78 &#x000b1; 0.02</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">8.20 &#x000b1; 1.74</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">250.39 &#x000b1; 157.63</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">24.14 &#x000b1; 1.86</td><td align="char" valign="top" char="&#x000b1;" rowspan="1" colspan="1">0.87 &#x000b1; 0.02</td></tr></tbody></table></table-wrap><p>When the reconstructed images were obtained from the axial planes of the encoder and decoder modules of the training model, the corresponding MAE, MSE, SSIM, and PSNR values were 12.03&#x0202f;&#x000b1;&#x0202f;2.13, 628.21&#x0202f;&#x000b1;&#x0202f;325.71, 20.15&#x0202f;&#x000b1;&#x0202f;1.63, and 0.78&#x0202f;&#x000b1;&#x0202f;0.02, respectively, slightly lower than those of the MAE model. However, when the reconstructed images were sourced from the coronal planes of the momentum encoder module of the training model, the MAE, MSE, SSIM, and PSNR values were notably superior at 8.20&#x0202f;&#x000b1;&#x0202f;1.74, 250.39&#x0202f;&#x000b1;&#x0202f;157.63, 24.14&#x0202f;&#x000b1;&#x0202f;1.86, and 0.87&#x0202f;&#x000b1;&#x0202f;0.02, respectively, outperforming those of the MAE model.</p><p>Axial slices are effective for localizing small lesions like PGN, providing a clearer cross-sectional view to assess size and relationship with surrounding structures. However, due to their anatomical limitations, they perform slightly worse in overall reconstruction quality compared to coronal slices. Coronal slices offer more comprehensive spatial information, crucial for evaluating tumor invasion and metastasis, especially in SLA staging. Due to the specific design of the MAEMC-NET model, wherein the coronal planes are solely utilized for the contrastive SSL task module and do not directly engage in the training of the generative SSL task module, these planes remain invisible to both the encoder and decoder components used for image reconstruction in both the MAEMC-NET and MAE models. Thus, when the coronal planes are employed for image reconstruction, the resulting MAE, MSE, SSIM, and PSNR values surpass those of the MAE model, thus demonstrating the superior capability of MAEMC-NET in capturing and utilizing information from the coronal planes for enhanced image reconstruction. This underscores the model&#x02019;s effectiveness in leveraging multi-modal features and exploiting contextual information, leading to improved accuracy and fidelity in medical image reconstruction tasks.</p></sec></sec><sec sec-type="conclusions" id="sec23"><label>5</label><title>Conclusion</title><p>In conclusion, our study introduces MAEMC-NET, a novel SSL model specifically designed to address the classification of PGN and SLA from CT images. This model successfully amalgamates the strengths of both contrastive and generative SSL techniques. This synthesis enables the model not only to adopt a global perspective in feature extraction from medical images but also to meticulously examine unique image areas, significantly enhancing the generalizability of prototype representations. Our approach represents a groundbreaking SSL method that fully leverages the comprehensive contextual information present in medical imaging. Differing from traditional 2D medical imaging methods, it facilitates the extraction of multifaceted lesion features, ensuring thorough data representation and maximizing the informational content of samples. Additionally, we incorporated a Feature Flattening module, effectively reducing the dimensionality of lesion features extracted by the ViT model. Extensive comparative and ablation studies have confirmed the significant advantages of MAEMC-NET in the classification tasks of PGN and SLA.</p><p>Looking ahead, there are several potential avenues for further enhancing the performance of MAEMC-NET. First, integrating other advanced image processing techniques, such as hybrid 3D convolutional networks or attention mechanisms, could improve the model&#x02019;s ability to capture complex spatial relationships in medical images. Second, the incorporation of more diverse and larger clinical datasets, potentially from multi-center studies, could help to further validate the generalizability of the model and enable it to handle the variations present in clinical environments. Additionally, fine-tuning the model with more advanced semi-supervised or unsupervised learning strategies, such as few-shot learning or active learning, could enhance its performance on limited labeled data, a common challenge in medical image analysis. Finally, future research could explore the integration of MAEMC-NET with real-time clinical decision support systems, enabling seamless adoption in clinical workflows and aiding radiologists in making more accurate diagnoses.</p><p>Overall, this research provides an efficient and practical new method for the classification of PGN and SLA, laying a solid foundation for future clinical applications and research.</p></sec></body><back><sec sec-type="data-availability" id="sec24"><title>Data availability statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.</p></sec><sec sec-type="ethics-statement" id="sec25"><title>Ethics statement</title><p>The studies involving humans were approved by the Medical Ethics Committee of Shengjing Hospital affiliated with China Medical University, the Medical Ethics Committee of the First Affiliated Hospital of Guangzhou Medical University, and the Medical Ethics Committee of the Fifth Affiliated Hospital of Guangzhou Medical University. The studies were conducted in accordance with the local legislation and institutional requirements. The ethics committee/institutional review board waived the requirement of written informed consent for participation from the participants or the participants&#x02019; legal guardians/next of kin because formal consent was not required for this retrospective study.</p></sec><sec sec-type="author-contributions" id="sec26"><title>Author contributions</title><p>TZ: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Writing &#x02013; original draft. YoY: Conceptualization, Data curation, Investigation, Validation, Writing &#x02013; review &#x00026; editing. HS: Conceptualization, Data curation, Formal analysis, Investigation, Writing &#x02013; original draft. JL: Conceptualization, Data curation, Formal analysis, Validation, Writing &#x02013; original draft. YW: Conceptualization, Data curation, Investigation, Validation, Writing &#x02013; original draft. YuY: Conceptualization, Data curation, Supervision, Validation, Writing &#x02013; review &#x00026; editing. WQ: Conceptualization, Data curation, Funding acquisition, Supervision, Validation, Writing &#x02013; review &#x00026; editing. YG: Conceptualization, Funding acquisition, Resources, Supervision, Validation, Writing &#x02013; review &#x00026; editing. SQ: Conceptualization, Formal analysis, Funding acquisition, Investigation, Resources, Software, Supervision, Validation, Writing &#x02013; review &#x00026; editing.</p></sec><sec sec-type="COI-statement" id="sec28"><title>Conflict of interest</title><p>The author(s) declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type="ai-statement" id="sec29"><title>Generative AI statement</title><p>The authors declare that no Gen AI was used in the creation of this manuscript.</p></sec><sec sec-type="disclaimer" id="sec30"><title>Publisher&#x02019;s note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><ref-list><title>References</title><ref id="ref1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname><given-names>RL</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Goding Sauer</surname><given-names>A</given-names></name><name><surname>Fedewa</surname><given-names>SA</given-names></name><name><surname>Butterly</surname><given-names>LF</given-names></name><name><surname>Anderson</surname><given-names>JC</given-names></name><etal/></person-group>. <article-title>Colorectal cancer statistics, 2020</article-title>. <source>CA Cancer J Clin</source>. (<year>2020</year>) <volume>70</volume>:<fpage>145</fpage>&#x02013;<lpage>64</lpage>. doi: <pub-id pub-id-type="doi">10.3322/caac.21601</pub-id><pub-id pub-id-type="pmid">32133645</pub-id>
</mixed-citation></ref><ref id="ref2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bray</surname><given-names>F</given-names></name><name><surname>Ferlay</surname><given-names>J</given-names></name><name><surname>Soerjomataram</surname><given-names>I</given-names></name><name><surname>Siegel</surname><given-names>RL</given-names></name><name><surname>Torre</surname><given-names>LA</given-names></name><name><surname>Jemal</surname><given-names>A</given-names></name></person-group>. <article-title>Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries</article-title>. <source>CA Cancer J Clin</source>. (<year>2018</year>) <volume>68</volume>:<fpage>394</fpage>&#x02013;<lpage>424</lpage>. doi: <pub-id pub-id-type="doi">10.3322/caac.21492</pub-id>, PMID: <pub-id pub-id-type="pmid">30207593</pub-id>
</mixed-citation></ref><ref id="ref3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aberle</surname><given-names>DR</given-names></name><name><surname>Adams</surname><given-names>AM</given-names></name><name><surname>Berg</surname><given-names>CD</given-names></name><name><surname>Black</surname><given-names>WC</given-names></name><name><surname>Clapp</surname><given-names>JD</given-names></name><name><surname>Fagerstrom</surname><given-names>RM</given-names></name><etal/></person-group>. <article-title>Reduced lung-cancer mortality with low-dose computed tomographic screening</article-title>. <source>N Engl J Med</source>. (<year>2011</year>) <volume>365</volume>:<fpage>395</fpage>&#x02013;<lpage>409</lpage>. doi: <pub-id pub-id-type="doi">10.1056/NEJMoa1102873</pub-id>, PMID: <pub-id pub-id-type="pmid">21714641</pub-id>
</mixed-citation></ref><ref id="ref4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaklitsch</surname><given-names>MT</given-names></name><name><surname>Jacobson</surname><given-names>FL</given-names></name><name><surname>Austin</surname><given-names>JH</given-names></name><name><surname>Field</surname><given-names>JK</given-names></name><name><surname>Jett</surname><given-names>JR</given-names></name><name><surname>Keshavjee</surname><given-names>S</given-names></name><etal/></person-group>. <article-title>The American Association for Thoracic Surgery guidelines for lung cancer screening using low-dose computed tomography scans for lung cancer survivors and other high-risk groups</article-title>. <source>J Thorac Cardiovasc Surg</source>. (<year>2012</year>) <volume>144</volume>:<fpage>33</fpage>&#x02013;<lpage>8</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.jtcvs.2012.05.060</pub-id>, PMID: <pub-id pub-id-type="pmid">22710039</pub-id>
</mixed-citation></ref><ref id="ref5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henschke</surname><given-names>CI</given-names></name><name><surname>Yankelevitz</surname><given-names>DF</given-names></name><name><surname>Libby</surname><given-names>DM</given-names></name><name><surname>Pasmantier</surname><given-names>MW</given-names></name><name><surname>Smith</surname><given-names>JP</given-names></name><name><surname>Miettinen</surname><given-names>OS</given-names></name></person-group>. <article-title>Survival of patients with stage I lung cancer detected on CT screening</article-title>. <source>N Engl J Med</source>. (<year>2006</year>) <volume>355</volume>:<fpage>1763</fpage>&#x02013;<lpage>71</lpage>. doi: <pub-id pub-id-type="doi">10.1056/NEJMoa060476</pub-id> PMID: <pub-id pub-id-type="pmid">17065637</pub-id>
</mixed-citation></ref><ref id="ref6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacMahon</surname><given-names>H</given-names></name><name><surname>Naidich</surname><given-names>DP</given-names></name><name><surname>Goo</surname><given-names>JM</given-names></name><name><surname>Lee</surname><given-names>KS</given-names></name><name><surname>Leung</surname><given-names>ANC</given-names></name><name><surname>Mayo</surname><given-names>JR</given-names></name><etal/></person-group>. <article-title>Guidelines for management of incidental pulmonary nodules detected on CT images: from the Fleischner society 2017</article-title>. <source>Radiology</source>. (<year>2017</year>) <volume>284</volume>:<fpage>228</fpage>&#x02013;<lpage>43</lpage>. doi: <pub-id pub-id-type="doi">10.1148/radiol.2017161659</pub-id>, PMID: <pub-id pub-id-type="pmid">28240562</pub-id>
</mixed-citation></ref><ref id="ref7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Travis</surname><given-names>WD</given-names></name><name><surname>Brambilla</surname><given-names>E</given-names></name><name><surname>Noguchi</surname><given-names>M</given-names></name><name><surname>Nicholson</surname><given-names>AG</given-names></name><name><surname>Geisinger</surname><given-names>KR</given-names></name><name><surname>Yatabe</surname><given-names>Y</given-names></name><etal/></person-group>. <article-title>International association for the study of lung cancer/american thoracic society/european respiratory society international multidisciplinary classification of lung adenocarcinoma</article-title>. <source>J Thorac Oncol</source>. (<year>2011</year>) <volume>6</volume>:<fpage>244</fpage>&#x02013;<lpage>85</lpage>. doi: <pub-id pub-id-type="doi">10.1097/JTO.0b013e318206a221</pub-id>, PMID: <pub-id pub-id-type="pmid">21252716</pub-id>
</mixed-citation></ref><ref id="ref8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gould</surname><given-names>MK</given-names></name><name><surname>Fletcher</surname><given-names>J</given-names></name><name><surname>Iannettoni</surname><given-names>MD</given-names></name><name><surname>Lynch</surname><given-names>WR</given-names></name><name><surname>Midthun</surname><given-names>DE</given-names></name><name><surname>Naidich</surname><given-names>DP</given-names></name><etal/></person-group>. <article-title>Evaluation of patients with pulmonary nodules: when is it lung cancer?: ACCP evidence-based clinical practice guidelines (2nd edition)</article-title>. <source>Chest</source>. (<year>2007</year>) <volume>132</volume>:<fpage>108S</fpage>&#x02013;<lpage>30S</lpage>. doi: <pub-id pub-id-type="doi">10.1378/chest.07-1353</pub-id>, PMID: <pub-id pub-id-type="pmid">17873164</pub-id>
</mixed-citation></ref><ref id="ref9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiener</surname><given-names>RS</given-names></name><name><surname>Schwartz</surname><given-names>LM</given-names></name><name><surname>Woloshin</surname><given-names>S</given-names></name><name><surname>Welch</surname><given-names>HG</given-names></name></person-group>. <article-title>Population-based risk for complications after transthoracic needle lung biopsy of a pulmonary nodule: an analysis of discharge records</article-title>. <source>Ann Intern Med</source>. (<year>2011</year>) <volume>155</volume>:<fpage>137</fpage>&#x02013;<lpage>44</lpage>. doi: <pub-id pub-id-type="doi">10.7326/0003-4819-155-3-201108020-00003</pub-id>, PMID: <pub-id pub-id-type="pmid">21810706</pub-id>
</mixed-citation></ref><ref id="ref10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shaukat</surname><given-names>F</given-names></name><name><surname>Raja</surname><given-names>G</given-names></name><name><surname>Frangi</surname><given-names>AF</given-names></name></person-group>. <article-title>Computer-aided detection of lung nodules: a review</article-title>. <source>J Med Imaging</source>. (<year>2019</year>) <volume>6</volume>:<fpage>020901</fpage>&#x02013;<lpage>1</lpage>. doi: <pub-id pub-id-type="doi">10.1117/1.JMI.6.2.020901</pub-id>, PMID: <pub-id pub-id-type="pmid">39697822</pub-id>
</mixed-citation></ref><ref id="ref11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>D</given-names></name><name><surname>Wu</surname><given-names>G</given-names></name><name><surname>Suk</surname><given-names>HI</given-names></name></person-group>. <article-title>Deep learning in medical image analysis</article-title>. <source>Annu Rev Biomed Eng</source>. (<year>2017</year>) <volume>19</volume>:<fpage>221</fpage>&#x02013;<lpage>48</lpage>. doi: <pub-id pub-id-type="doi">10.1146/annurev-bioeng-071516-044442</pub-id>, PMID: <pub-id pub-id-type="pmid">28301734</pub-id>
</mixed-citation></ref><ref id="ref12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldenberg</surname><given-names>SL</given-names></name><name><surname>Nir</surname><given-names>G</given-names></name><name><surname>Salcudean</surname><given-names>SE</given-names></name></person-group>. <article-title>A new era: artificial intelligence and machine learning in prostate cancer</article-title>. <source>Nat Rev Urol</source>. (<year>2019</year>) <volume>16</volume>:<fpage>391</fpage>&#x02013;<lpage>403</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41585-019-0193-3</pub-id><pub-id pub-id-type="pmid">31092914</pub-id>
</mixed-citation></ref><ref id="ref13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakhani</surname><given-names>P</given-names></name><name><surname>Sundaram</surname><given-names>B</given-names></name></person-group>. <article-title>Deep learning at chest radiography: automated classification of pulmonary tuberculosis by using convolutional neural networks</article-title>. <source>Radiology</source>. (<year>2017</year>) <volume>284</volume>:<fpage>574</fpage>&#x02013;<lpage>82</lpage>. doi: <pub-id pub-id-type="doi">10.1148/radiol.2017162326</pub-id>, PMID: <pub-id pub-id-type="pmid">28436741</pub-id>
</mixed-citation></ref><ref id="ref14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>JH</given-names></name><name><surname>Baek</surname><given-names>JH</given-names></name><name><surname>Kim</surname><given-names>JH</given-names></name><name><surname>Shim</surname><given-names>WH</given-names></name><name><surname>Chung</surname><given-names>SR</given-names></name><name><surname>Choi</surname><given-names>YJ</given-names></name><etal/></person-group>. <article-title>Deep learning&#x02013;based computer-aided diagnosis system for localization and diagnosis of metastatic lymph nodes on ultrasound: a pilot study</article-title>. <source>Thyroid</source>. (<year>2018</year>) <volume>28</volume>:<fpage>1332</fpage>&#x02013;<lpage>8</lpage>. doi: <pub-id pub-id-type="doi">10.1089/thy.2018.0082</pub-id>, PMID: <pub-id pub-id-type="pmid">30132411</pub-id>
</mixed-citation></ref><ref id="ref15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miotto</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Dudley</surname><given-names>JT</given-names></name></person-group>. <article-title>Deep learning for healthcare: review, opportunities and challenges</article-title>. <source>Brief Bioinform</source>. (<year>2018</year>) <volume>19</volume>:<fpage>1236</fpage>&#x02013;<lpage>46</lpage>. doi: <pub-id pub-id-type="doi">10.1093/bib/bbx044</pub-id>, PMID: <pub-id pub-id-type="pmid">28481991</pub-id>
</mixed-citation></ref><ref id="ref16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Topol</surname><given-names>EJ</given-names></name></person-group>. <article-title>High-performance medicine: the convergence of human and artificial intelligence</article-title>. <source>Nat Med</source>. (<year>2019</year>) <volume>25</volume>:<fpage>44</fpage>&#x02013;<lpage>56</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41591-018-0300-7</pub-id>, PMID: <pub-id pub-id-type="pmid">30617339</pub-id>
</mixed-citation></ref><ref id="ref17"><label>17.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name></person-group>. <italic>Deep learning</italic>. (<year>2016</year>).</mixed-citation></ref><ref id="ref18"><label>18.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Gidaris</surname><given-names>S</given-names></name><name><surname>Singh</surname><given-names>P</given-names></name><name><surname>Komodakis</surname><given-names>N</given-names></name></person-group>. <italic>Unsupervised representation learning by predicting image rotations</italic>. arXiv preprint (<year>2018</year>).</mixed-citation></ref><ref id="ref19"><label>19.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Fan</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name></person-group>. <italic>Momentum contrast for unsupervised visual representation learning</italic>. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729&#x02013;9738. (<year>2020</year>).</mixed-citation></ref><ref id="ref20"><label>20.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D P</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group>. <italic>Auto-encoding variational bayes</italic>. arXiv preprint. (<year>2013</year>).</mixed-citation></ref><ref id="ref21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Creswell</surname><given-names>A</given-names></name><name><surname>White</surname><given-names>T</given-names></name><name><surname>Dumoulin</surname><given-names>V</given-names></name><name><surname>Arulkumaran</surname><given-names>K</given-names></name><name><surname>Sengupta</surname><given-names>B</given-names></name><name><surname>Bharath</surname><given-names>AA</given-names></name></person-group>. <article-title>Generative adversarial networks: an overview</article-title>. <source>IEEE Signal Process Mag</source>. (<year>2018</year>) <volume>35</volume>:<fpage>53</fpage>&#x02013;<lpage>65</lpage>. doi: <pub-id pub-id-type="doi">10.1109/MSP.2017.2765202</pub-id></mixed-citation></ref><ref id="ref22"><label>22.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Vincent</surname><given-names>P</given-names></name><name><surname>Larochelle</surname><given-names>H</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Manzagol</surname><given-names>PA</given-names></name></person-group>. <italic>Extracting and composing robust features with denoising autoencoders</italic>. Proceedings of the 25th international conference on machine learning, pp. 1096&#x02013;1103. (<year>2008</year>).</mixed-citation></ref><ref id="ref23"><label>23.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Doll&#x000e1;r</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name></person-group>. <italic>Masked autoencoders are scalable vision learners</italic>. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16000&#x02013;16009. (<year>2022</year>).</mixed-citation></ref><ref id="ref24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Bentley</surname><given-names>P</given-names></name><name><surname>Mori</surname><given-names>K</given-names></name><name><surname>Misawa</surname><given-names>K</given-names></name><name><surname>Fujiwara</surname><given-names>M</given-names></name><name><surname>Rueckert</surname><given-names>D</given-names></name></person-group>. <article-title>Self-supervised learning for medical image analysis using image context restoration</article-title>. <source>Med Image Anal</source>. (<year>2019</year>) <volume>58</volume>:<fpage>101539</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.media.2019.101539</pub-id>, PMID: <pub-id pub-id-type="pmid">31374449</pub-id>
</mixed-citation></ref><ref id="ref25"><label>25.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Taleb</surname><given-names>A</given-names></name><name><surname>Lippert</surname><given-names>C</given-names></name><name><surname>Klein</surname><given-names>T</given-names></name><name><surname>Nabi</surname><given-names>M</given-names></name></person-group>. <italic>Multimodal self-supervised learning for medical image analysis</italic>. International conference on information processing in medical imaging. Cham: Springer International Publishing, pp. <fpage>661</fpage>&#x02013;<lpage>673</lpage>. (<year>2021</year>)</mixed-citation></ref><ref id="ref26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Jiang</surname><given-names>F</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name></person-group>. <article-title>A-DARTS: attention-guided differentiable architecture search for lung nodule classification</article-title>. <source>J Electron Imaging</source>. (<year>2021</year>) <volume>30</volume>:<fpage>013012</fpage>&#x02013;<lpage>2</lpage>. doi: <pub-id pub-id-type="doi">10.1117/1.JEI.30.1.013012</pub-id>, PMID: <pub-id pub-id-type="pmid">39697822</pub-id>
</mixed-citation></ref><ref id="ref27"><label>27.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>W</given-names></name><name><surname>Vang</surname><given-names>YS</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Xie</surname><given-names>X</given-names></name></person-group>. <italic>Deepem: deep 3d convnets with em for weakly supervised pulmonary nodule detection</italic>. Medical image computing and computer assisted intervention&#x02013;MICCAI 2018: 21st international conference, Granada, Spain, September 16-20, 2018, proceedings, part II 11. Springer International Publishing pp. 812&#x02013;820. (<year>2018</year>).</mixed-citation></ref><ref id="ref28"><label>28.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Halimi</surname><given-names>O</given-names></name><name><surname>Litany</surname><given-names>O</given-names></name><name><surname>Rodola</surname><given-names>E</given-names></name><name><surname>Bronstein</surname><given-names>AM</given-names></name><name><surname>Kimmel</surname><given-names>R</given-names></name></person-group>. <italic>Unsupervised learning of dense shape correspondence</italic>. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4370&#x02013;4379. (<year>2019</year>).</mixed-citation></ref><ref id="ref29"><label>29.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Child</surname><given-names>R</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Jun</surname><given-names>H</given-names></name><name><surname>Luan</surname><given-names>D</given-names></name><etal/></person-group>. (<year>2020</year>) <italic>Generative pretraining from pixels</italic>. International conference on machine learning. PMLR, pp. 1691&#x02013;1703.</mixed-citation></ref><ref id="ref30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taleb</surname><given-names>A</given-names></name><name><surname>Loetzsch</surname><given-names>W</given-names></name><name><surname>Danz</surname><given-names>N</given-names></name><etal/></person-group>. <article-title>3d self-supervised methods for medical imaging</article-title>. <source>Adv Neural Inf Proces Syst</source>. (<year>2020</year>) <volume>33</volume>:<fpage>18158</fpage>&#x02013;<lpage>72</lpage>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.2006.03829</pub-id></mixed-citation></ref><ref id="ref31"><label>31.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Norouzi</surname><given-names>M</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group>. <italic>A simple framework for contrastive learning of visual representations</italic>. International conference on machine learning. PMLR, pp. 1597&#x02013;1607. (<year>2020</year>).</mixed-citation></ref><ref id="ref32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill</surname><given-names>JB</given-names></name><name><surname>Strub</surname><given-names>F</given-names></name><name><surname>Altch&#x000e9;</surname><given-names>F</given-names></name><name><surname>Tallec</surname><given-names>C</given-names></name><name><surname>Richemond</surname><given-names>PH</given-names></name><name><surname>Buchatskaya</surname><given-names>E</given-names></name><etal/></person-group>. <article-title>Bootstrap your own latent-a new approach to self-supervised learning</article-title>. <source>Adv Neural Inf Proces Syst</source>. (<year>2022</year>) <volume>33</volume>:<fpage>21271</fpage>&#x02013;<lpage>84</lpage>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.2006.07733</pub-id></mixed-citation></ref><ref id="ref33"><label>33.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Miura</surname><given-names>Y</given-names></name><name><surname>Manning</surname><given-names>CD</given-names></name><name><surname>Langlotz</surname><given-names>CP</given-names></name></person-group>. <italic>Contrastive learning of medical visual representations from paired images and text</italic>. Machine learning for healthcare conference. PMLR, pp. 2&#x02013;25. (<year>2022</year>).</mixed-citation></ref><ref id="ref34"><label>34.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>Y</given-names></name><name><surname>Ma</surname><given-names>K</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name></person-group>
<italic>Self-supervised feature learning for 3d medical images by playing a rubik&#x02019;s cube</italic>. Medical image computing and computer assisted intervention&#x02013;MICCAI 2019: 22nd international conference, Shenzhen, China, October 13&#x02013;17, 2019, proceedings, part IV 22. Springer International Publishing, pp. 420&#x02013;428, (<year>2019</year>).</mixed-citation></ref><ref id="ref35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinidhi</surname><given-names>CL</given-names></name><name><surname>Kim</surname><given-names>SW</given-names></name><name><surname>Chen</surname><given-names>FD</given-names></name><name><surname>Martel</surname><given-names>AL</given-names></name></person-group>. <article-title>Self-supervised driven consistency training for annotation efficient histopathology image analysis</article-title>. <source>Med Image Anal</source>. (<year>2022</year>) <volume>75</volume>:<fpage>102256</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.media.2021.102256</pub-id>, PMID: <pub-id pub-id-type="pmid">34717189</pub-id>
</mixed-citation></ref><ref id="ref36"><label>36.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Kiyasseh</surname><given-names>D</given-names></name><name><surname>Zhu</surname><given-names>T</given-names></name><name><surname>Clifton</surname><given-names>DA</given-names></name></person-group>. <italic>Clocs: Contrastive learning of cardiac signals across space, time, and patients</italic>. International conference on machine learning. PMLR, pp. 5606&#x02013;5615. (<year>2021</year>).</mixed-citation></ref><ref id="ref37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Ji</surname><given-names>S</given-names></name></person-group>. <article-title>Self-supervised learning of graph neural networks: a unified review</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. (<year>2022</year>) <volume>45</volume>:<fpage>2412</fpage>&#x02013;<lpage>29</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TPAMI.2022.3170559</pub-id></mixed-citation></ref><ref id="ref38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Cao</surname><given-names>Z</given-names></name><name><surname>Barati Farimani</surname><given-names>A</given-names></name></person-group>. <article-title>Molecular contrastive learning of representations via graph neural networks</article-title>. <source>Nat Machine Intell</source>. (<year>2022</year>) <volume>4</volume>:<fpage>279</fpage>&#x02013;<lpage>87</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s42256-022-00447-x</pub-id></mixed-citation></ref><ref id="ref39"><label>39.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Dosovitskiy</surname><given-names>A</given-names></name><name><surname>Beyer</surname><given-names>L</given-names></name><name><surname>Kolesnikov</surname><given-names>A</given-names></name><name><surname>Weissenborn</surname><given-names>D</given-names></name><name><surname>Zhai</surname><given-names>X</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><etal/></person-group>. <italic>An image is worth 16x16 words: Transformers for image recognition at scale</italic>. arXiv preprint (<year>2020</year>).</mixed-citation></ref><ref id="ref40"><label>40.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Gehring</surname><given-names>J</given-names></name><name><surname>Auli</surname><given-names>M</given-names></name><name><surname>Grangier</surname><given-names>D</given-names></name><name><surname>Yarats</surname><given-names>D</given-names></name><name><surname>Dauphin</surname><given-names>YN</given-names></name></person-group>. <italic>Convolutional sequence to sequence learning</italic>. International conference on machine learning. PMLR, pp. 1243&#x02013;1252. (<year>2017</year>).</mixed-citation></ref><ref id="ref41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Jin</surname><given-names>X</given-names></name><name><surname>Lu</surname><given-names>C</given-names></name><name><surname>Hou</surname><given-names>Q</given-names></name><name><surname>Cheng</surname><given-names>MM</given-names></name><name><surname>Fu</surname><given-names>D</given-names></name></person-group>. <article-title>Contrastive masked autoencoders are stronger vision learners</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. (<year>2023</year>) <volume>46</volume>:<fpage>2506</fpage>&#x02013;<lpage>17</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TPAMI.2023.3336525</pub-id></mixed-citation></ref><ref id="ref42"><label>42.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Woo</surname><given-names>S</given-names></name><name><surname>Debnath</surname><given-names>S</given-names></name><name><surname>Hu</surname><given-names>R</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Kweon</surname><given-names>IS</given-names></name><etal/></person-group>. <italic>Convnext v2: Co-designing and scaling convnets with masked autoencoders</italic>. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16133&#x02013;16142. (<year>2023</year>).</mixed-citation></ref><ref id="ref43"><label>43.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Lin</surname><given-names>Y</given-names></name><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>H</given-names></name><name><surname>Wei</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><etal/></person-group>. <italic>Swin transformer: Hierarchical vision transformer using shifted windows</italic>. proceedings of the IEEE/CVF international conference on computer vision, pp. 10012&#x02013;10022. (<year>2021</year>).</mixed-citation></ref><ref id="ref44"><label>44.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Touvron</surname><given-names>H</given-names></name><name><surname>Cord</surname><given-names>M</given-names></name><name><surname>Douze</surname><given-names>M</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Sablayrolles</surname><given-names>A</given-names></name><name><surname>Jegou</surname><given-names>H</given-names></name></person-group>. <italic>Training data-efficient image transformers &#x00026; distillation through attention</italic>. International conference on machine learning. PMLR, pp. 10347&#x02013;10357. (<year>2021</year>).</mixed-citation></ref><ref id="ref45"><label>45.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Graham</surname><given-names>B</given-names></name><name><surname>El-Nouby</surname><given-names>A</given-names></name><name><surname>Touvron</surname><given-names>H</given-names></name><name><surname>Stock</surname><given-names>P</given-names></name><name><surname>Joulin</surname><given-names>A</given-names></name><name><surname>J&#x000e9;gou</surname><given-names>H</given-names></name><etal/></person-group>. <italic>Levit: A vision transformer in convnet&#x02019;s clothing for faster inference</italic>. Proceedings of the IEEE/CVF international conference on computer vision, pp. 12259&#x02013;12269. (<year>2021</year>).</mixed-citation></ref><ref id="ref46"><label>46.</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Xie</surname><given-names>E</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Fan</surname><given-names>DP</given-names></name><name><surname>Song</surname><given-names>K</given-names></name><name><surname>Liang</surname><given-names>D</given-names></name><etal/></person-group>. <italic>Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</italic>. Proceedings of the IEEE/CVF international conference on computer vision, pp. 568&#x02013;578. (<year>2021</year>).</mixed-citation></ref><ref id="ref47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Maaten</surname><given-names>L</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group>. <article-title>Visualizing data using t-SNE</article-title>. <source>J Mach Learn Res</source>. (<year>2008</year>) <volume>9</volume>:<fpage>11</fpage>.</mixed-citation></ref></ref-list></back></article>