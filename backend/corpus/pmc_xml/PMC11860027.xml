<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006452</article-id><article-id pub-id-type="pmc">PMC11860027</article-id><article-id pub-id-type="doi">10.3390/s25041223</article-id><article-id pub-id-type="publisher-id">sensors-25-01223</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Multimodal Pain Sentiment Analysis System Using Ensembled Deep Learning Approaches for IoT-Enabled Healthcare Framework</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-6237-7905</contrib-id><name><surname>Ghosh</surname><given-names>Anay</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-25-01223" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1476-041X</contrib-id><name><surname>Umer</surname><given-names>Saiyed</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af2-sensors-25-01223" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Dhara</surname><given-names>Bibhas Chandra</given-names></name><xref rid="af3-sensors-25-01223" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5861-0475</contrib-id><name><surname>Ali</surname><given-names>G. G. Md. Nawaz</given-names></name><xref rid="af4-sensors-25-01223" ref-type="aff">4</xref><xref rid="c1-sensors-25-01223" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Sato</surname><given-names>Wataru</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01223"><label>1</label>Department of Computer Science &#x00026; Engineering, University of Engineering &#x00026; Management, Kolkata 700160, India; <email>anay.ghosh1@gmail.com</email></aff><aff id="af2-sensors-25-01223"><label>2</label>Department of Computer Science and Engineering, Aliah University, Kolkata 700156, India; <email>saiyed.umer@aliah.ac.in</email></aff><aff id="af3-sensors-25-01223"><label>3</label>Department of Information Technology, Jadavpur University, Kolkata 700032, India; <email>bcdhara@gmail.com</email></aff><aff id="af4-sensors-25-01223"><label>4</label>Department of Computer Science and Information Systems, Bradley University, Peoria, IL 61625, USA</aff><author-notes><corresp id="c1-sensors-25-01223"><label>*</label>Correspondence: <email>nali@bradley.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>17</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1223</elocation-id><history><date date-type="received"><day>11</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>07</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>07</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>This study introduces a multimodal sentiment analysis system to assess and recognize human pain sentiments within an Internet of Things (IoT)-enabled healthcare framework. This system integrates facial expressions and speech-audio recordings to evaluate human pain intensity levels. This integration aims to enhance the recognition system&#x02019;s performance and enable a more accurate assessment of pain intensity. Such a multimodal approach supports improved decision making in real-time patient care, addressing limitations inherent in unimodal systems for measuring pain sentiment. So, the primary contribution of this work lies in developing a multimodal pain sentiment analysis system that integrates the outcomes of image-based and audio-based pain sentiment analysis models. The system implementation contains five key phases. The first phase focuses on detecting the facial region from a video sequence, a crucial step for extracting facial patterns indicative of pain. In the second phase, the system extracts discriminant and divergent features from the facial region using deep learning techniques, utilizing some convolutional neural network (CNN) architectures, which are further refined through transfer learning and fine-tuning of parameters, alongside fusion techniques aimed at optimizing the model&#x02019;s performance. The third phase performs the speech-audio recording preprocessing; the extraction of significant features is then performed through conventional methods followed by using the deep learning model to generate divergent features to recognize audio-based pain sentiments in the fourth phase. The final phase combines the outcomes from both image-based and audio-based pain sentiment analysis systems, improving the overall performance of the multimodal system. This fusion enables the system to accurately predict pain levels, including &#x02018;high pain&#x02019;, &#x02018;mild pain&#x02019;, and &#x02018;no pain&#x02019;. The performance of the proposed system is tested with the three image-based databases such as a 2D Face Set Database with Pain Expression, the UNBC-McMaster database (based on shoulder pain), and the BioVid database (based on heat pain), along with the VIVAE database for the audio-based dataset. Extensive experiments were performed using these datasets. Finally, the proposed system achieved accuracies of 76.23%, 84.27%, and 38.04% for two, three, and five pain classes, respectively, on the 2D Face Set Database with Pain Expression, UNBC, and BioVid datasets. The VIVAE audio-based system recorded a peak performance of 97.56% and 98.32% accuracy for varying training&#x02013;testing protocols. These performances were compared with some state-of-the-art methods that show the superiority of the proposed system. By combining the outputs of both deep learning frameworks on image and audio datasets, the proposed multimodal pain sentiment analysis system achieves accuracies of 99.31% for the two-class, 99.54% for the three-class, and 87.41% for the five-class pain problems.</p></abstract><kwd-group><kwd>IoT</kwd><kwd>sentiment analysis</kwd><kwd>pain recognition</kwd><kwd>multimodal</kwd><kwd>CNN</kwd><kwd>audio</kwd><kwd>fusion</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01223"><title>1. Introduction</title><p>The Internet of Things (IoT) [<xref rid="B1-sensors-25-01223" ref-type="bibr">1</xref>] has transformed automation systems by integrating software, sensors, and electronic devices to collect and transmit data online without requiring human intervention. IoT-based devices are widely used in smart homes, traffic congestion management, and autonomous driving. In addition to these applications, IoT collaborates with artificial intelligence (AI) in the healthcare sector [<xref rid="B2-sensors-25-01223" ref-type="bibr">2</xref>]. The healthcare industry is a key contributor to employment, revenue, and overall well-being, playing a crucial role in promoting a healthy society within smart cities. In healthcare, the IoT plays a pivotal role in automating systems such as patient monitoring and nurse call mechanisms [<xref rid="B3-sensors-25-01223" ref-type="bibr">3</xref>]. For example, patient monitoring systems analyze data from smart devices, including thermometers, ECGs, pulse oximeters, and sphygmomanometers, to predict health conditions and notify medical staff to initiate timely interventions. The healthcare framework includes a wide range of components, such as medical professionals, hospitals, doctors, nurses, outpatient clinics, clinical tests, telemedicine, medical devices, machinery, and health insurance services [<xref rid="B4-sensors-25-01223" ref-type="bibr">4</xref>]. Advances in technology have significantly enhanced healthcare through the adoption of electronic and mobile-based health services. Portable healthcare systems now offer patients daily health support, facilitating continuous monitoring and assistance [<xref rid="B4-sensors-25-01223" ref-type="bibr">4</xref>]. These systems leverage speech recognition, gesture analysis, vocal tone, and facial expression analysis to remotely evaluate patients&#x02019; medical conditions. Gesture-based interaction and identification have become vital aspects of human&#x02013;computer interaction (HCI), making e-healthcare systems more user-friendly and accessible for medical professionals.</p><p>Building on these advancements, this study introduces a multimodal pain sentiment analysis system (MPSAS) for remote patient monitoring, utilizing IoT-enabled medical sensors. The system integrates data from various sources, including audio and video, to categorize sentiments as positive, negative, or neutral. It also analyzes emotions such as happiness, anger, and frustration. By identifying these emotions, the system offers organizations valuable insights into sentiment, enabling them to make informed decisions to improve their products and services. This proposed sentiment analysis system enhances e-healthcare solutions, providing more cost-effective and efficient healthcare delivery. Facial expressions are known to effectively communicate unpleasant emotions, making them crucial for pain assessment [<xref rid="B5-sensors-25-01223" ref-type="bibr">5</xref>]. In real-world applications, sentiment analysis systems remotely detect and interpret patients&#x02019; facial expressions to determine their emotional states [<xref rid="B4-sensors-25-01223" ref-type="bibr">4</xref>]. For non-verbal patients, the American Society outlines a method for estimating pain through facial expressions. This method highlights a hierarchical framework for pain evaluation, where facial representation-dependent pain estimation is considered a valid approach. Payen et al. [<xref rid="B6-sensors-25-01223" ref-type="bibr">6</xref>] found that facial expressions can play a crucial role in assessing pain in severely ill, unresponsive patients. In elderly individuals with serious mental health conditions, who are often unable to communicate verbally due to their deteriorating condition, facial expressions serve as a valuable tool for pain assessment. This approach is especially relevant for patients with advanced mental illnesses who cannot articulate their pain. McGuire and colleagues [<xref rid="B7-sensors-25-01223" ref-type="bibr">7</xref>] concluded that individuals in non-verbal populations may not recognize or receive adequate treatment for their pain, primarily due to their inability to effectively communicate discomfort.</p><p>However, certain vulnerable groups&#x02014;such as the physiologically immobile, terminally ill [<xref rid="B7-sensors-25-01223" ref-type="bibr">7</xref>], critically ill but coherent, individuals in psychological distress, or those suffering from cancers affecting the head, neck, or brain&#x02014;require dependable technological support to consistently monitor pain and alleviate the burden on healthcare professionals. Behavioral responses such as grimacing, eye closure, and wincing have been shown to strongly correlate with procedural pain, as highlighted in patient studies [<xref rid="B8-sensors-25-01223" ref-type="bibr">8</xref>]. Pain assessment in clinical settings is vital for providing appropriate care and evaluating its effectiveness [<xref rid="B9-sensors-25-01223" ref-type="bibr">9</xref>]. Traditionally, the gold standard for pain reporting involves patient self-reports through verbal numerical scales and visual analogue scales [<xref rid="B10-sensors-25-01223" ref-type="bibr">10</xref>]. Additionally, sentiment analysis systems have been developed for conditions like Parkinson&#x02019;s disease, depression, and for detecting intent and emotions from facial expressions, as well as for contextual semantic search. These systems utilize neural network techniques to analyze text, image, and video data to classify emotions and detect patterns. The principal contributions of this work are as follows:<list list-type="bullet"><list-item><p>The proposed work develops a pain sentiment analysis system based on non-verbal communication (image or video) by utilizing multiple top-down deep learning models built on convolutional neural network (CNN) architectures. These models extract discriminative features from facial regions, enhancing feature representation for detecting varying levels of pain intensity.</p></list-item><list-item><p>The proposed work also focuses on verbal communication (audio or speech) for pain sentiment analysis by employing handcrafted audio features as input, followed by deep learning models using CNN architectures to extract meaningful features for identifying different pain intensity levels.</p></list-item><list-item><p>Performance improvements are achieved through extensive experimentation, including comparisons between batch processing and epoch cycles, data augmentation, progressive image resizing, hyperparameter tuning, and the application of pre-trained models via transfer learning.</p></list-item><list-item><p>Post-classification fusion techniques are applied to enhance system accuracy by integrating classification scores from multiple deep learning models. This approach addresses challenges arising from variations in age, pose, lighting conditions, and noisy artefacts.</p></list-item><list-item><p>Finally, a multimodal pain sentiment analysis system is developed by combining the results of image-based and audio-based models. This integration improves the system&#x02019;s performance, enabling more accurate recognition of pain intensity and supporting real-time decision making in patient care.</p></list-item></list></p><p><xref rid="sensors-25-01223-t001" ref-type="table">Table 1</xref> shows the list of abbreviations employed in this paper.</p><p>The structure of this research paper is as follows: <xref rid="sec2-sensors-25-01223" ref-type="sec">Section 2</xref> presents a review of the relevant literature. <xref rid="sec3-sensors-25-01223" ref-type="sec">Section 3</xref> provides a detailed explanation of the proposed methodology. <xref rid="sec4-sensors-25-01223" ref-type="sec">Section 4</xref> focuses on the evaluation of the conducted experiments and an overall analysis of the system. Finally, <xref rid="sec5-sensors-25-01223" ref-type="sec">Section 5</xref> summarizes the conclusions drawn from the study.</p></sec><sec id="sec2-sensors-25-01223"><title>2. Related&#x000a0;Work</title><p>This section reviews related work on pain sentiment analysis systems (PSASs) utilizing audio-based, image or video-based, and multimodal or hybrid prediction models. Acoustic or auditory pain refers to the discomfort experienced when exposed to sound. This pain can manifest in several ways: (i) hyperacusis, an increased sensitivity to everyday environmental sounds that can cause pain or discomfort even at low decibel levels; (ii) phonophobia, a severe fear or avoidance of loud noises, often accompanied by physical discomfort and anxiety; and (iii) tinnitus, a condition characterized by the perception of ringing, buzzing, or other phantom noises in the ears, which may not always be painful [<xref rid="B11-sensors-25-01223" ref-type="bibr">11</xref>]. Furthermore, it has been demonstrated that acoustic elements of music, such as rhythm, tempo, and harmony, play a pivotal role in connecting pain relief with music therapy. A pioneering method for recognizing acoustic pain indicators in neonatal populations using audio features is proposed by Giordano et al. [<xref rid="B12-sensors-25-01223" ref-type="bibr">12</xref>]. To investigate the relationship between self-reported pain intensity and measurable bioacoustic indicators in human vocalizations, Oshrat et al. [<xref rid="B13-sensors-25-01223" ref-type="bibr">13</xref>] have developed a prosody-based approach. The work in [<xref rid="B14-sensors-25-01223" ref-type="bibr">14</xref>] has introduced an automated process for evaluating pain based on paralinguistic speech features, aiming to enhance the objectivity and accuracy of pain diagnosis.</p><p>In computer vision, features extracted by various convolutional neural network (CNN) frameworks are employed for tasks such as emotion recognition and object identification. CNN-derived features have been observed to outperform handcrafted features in terms of performance. Recent advancements in deep learning have contributed significantly to solving complex challenges, establishing it as a leading approach in many domains. Automatic techniques for pain identification often demonstrate limited proficiency when relying on facial representations. In the domain of pain recognition systems using facial expressions, several widely adopted feature extraction methods are employed, such as active shape models and active appearance models [<xref rid="B15-sensors-25-01223" ref-type="bibr">15</xref>], local binary pattern [<xref rid="B16-sensors-25-01223" ref-type="bibr">16</xref>], and Gabor wavelets [<xref rid="B17-sensors-25-01223" ref-type="bibr">17</xref>]. Existing approaches predominantly focus on facial representation-based pain detection and encompass deep learning and non-deep learning-based methodologies [<xref rid="B18-sensors-25-01223" ref-type="bibr">18</xref>]. Significant advancements have been made in this research area in recent years. It has been observed that the classification outcomes are influenced by the extracted facial features [<xref rid="B19-sensors-25-01223" ref-type="bibr">19</xref>]. Feature extraction is commonly performed using established methods. At the same time, classification is carried out using supervised machine learning techniques such as K-Nearest Neighbors, Support Vector Machines (SVMs), and Logistic Regression [<xref rid="B20-sensors-25-01223" ref-type="bibr">20</xref>]. Nagarajan et al. [<xref rid="B21-sensors-25-01223" ref-type="bibr">21</xref>] proposed a hybrid method combining Support Vector Machines (SVMs) and recurrent neural networks (RNNs) for detecting chronic pain. A technique to identify central neuropathic pain from EEG data, employing linear discriminant analysis and Linear SVMs, was developed in [<xref rid="B22-sensors-25-01223" ref-type="bibr">22</xref>].</p><p>The classification of pain using a CNN framework was demonstrated in [<xref rid="B23-sensors-25-01223" ref-type="bibr">23</xref>], and a comparative study utilizing bio-inspired algorithms for pain recognition was shown in [<xref rid="B24-sensors-25-01223" ref-type="bibr">24</xref>]. A pain detection techniques was developed using the &#x02018;Triplet Loss for Pain Detection&#x02019; technique to classify pain levels based on facial expressions in [<xref rid="B25-sensors-25-01223" ref-type="bibr">25</xref>]. A two-stage method involving a 2D-CNN for frame-level feature extraction followed by an LSTM for sequence-level pain recognition was developed in [<xref rid="B26-sensors-25-01223" ref-type="bibr">26</xref>]; similarly, a two-stage deep learning architecture incorporating the facial landmark localization and action-based analysis for pain detection was developed in [<xref rid="B27-sensors-25-01223" ref-type="bibr">27</xref>]. Pain detection algorithms such as convolutional neural networks (CNNs), XGBoost, Extra Trees, and Random Forests are applied in [<xref rid="B28-sensors-25-01223" ref-type="bibr">28</xref>]. In contrast, the Children Pain Assessment Neural Network was introduced in [<xref rid="B29-sensors-25-01223" ref-type="bibr">29</xref>]. Rasha et al. [<xref rid="B30-sensors-25-01223" ref-type="bibr">30</xref>] surveyed pain detection techniques, including multi-domain neural networks, CNN-LSTMs, and additive attributes. They also proposed a method for comparing feature extraction techniques, such as skin conductance level and skin conductance response, which retrieve features like the root mean square, local maxima and minima mean values, and mean absolute value for pain detection [<xref rid="B31-sensors-25-01223" ref-type="bibr">31</xref>]. Kornprom et al. [<xref rid="B32-sensors-25-01223" ref-type="bibr">32</xref>] developed a pain assessment method, introducing a pain intensity metric that incorporates tools such as the Sensory Scale, Affective Motivational Scale, Visual Analog Scale, and Observer Rating Scale. Leila et al. [<xref rid="B33-sensors-25-01223" ref-type="bibr">33</xref>] created an automatic pain detection model using the VGG-Face model on the UNBC dataset. Jiang et al. [<xref rid="B34-sensors-25-01223" ref-type="bibr">34</xref>] conducted experiments using the GLA-CNN approach for pain detection. Ehsan et al. [<xref rid="B35-sensors-25-01223" ref-type="bibr">35</xref>] proposed a pain classification system utilizing video data from the X-ITE dataset and incorporated high-performance CNN frameworks such as GoogleNet and AlexNet.</p><p>Thiam et al. [<xref rid="B36-sensors-25-01223" ref-type="bibr">36</xref>] presented a multimodal system for recognizing pain intensity by integrating audio and visual cues. Hossain [<xref rid="B37-sensors-25-01223" ref-type="bibr">37</xref>] developed a multimodal system for assessing patient affective states, combining speech and facial expression modalities. Zeng et al. [<xref rid="B38-sensors-25-01223" ref-type="bibr">38</xref>] surveyed, from a psychological perspective, human emotion perception through audio, visual, and spontaneous expressions. The work in [<xref rid="B39-sensors-25-01223" ref-type="bibr">39</xref>] introduced a pain detection method that integrated unimodal and multimodal concepts. Deep learning algorithms can uncover intricate patterns within complex datasets, significantly enhancing feature extraction and classification processes [<xref rid="B40-sensors-25-01223" ref-type="bibr">40</xref>]. Dashtipur et al. [<xref rid="B41-sensors-25-01223" ref-type="bibr">41</xref>] proposed a multimodal framework based on context awareness for Persian sentiment analysis. Sagum et al. [<xref rid="B42-sensors-25-01223" ref-type="bibr">42</xref>] introduced a sentiment measurement technique focused on emotion analysis using movie review comments. Rustam et al. [<xref rid="B43-sensors-25-01223" ref-type="bibr">43</xref>] conducted a comparative study of various supervised machine learning methods for sentiment analysis of COVID-19-related tweets. The introduction of a neural network model combining CNN and BiLSTM architectures for emotion analysis was proposed in [<xref rid="B44-sensors-25-01223" ref-type="bibr">44</xref>]. A pain detection method employing spatiotemporal data, using long short-term memory (LSTM) and spatiotemporal graph convolution networks (STGCNs), was developed in [<xref rid="B45-sensors-25-01223" ref-type="bibr">45</xref>].</p></sec><sec sec-type="methods" id="sec3-sensors-25-01223"><title>3. Proposed&#x000a0;Methodology</title><p>This section outlines the step-by-step implementation of the proposed pain sentiment analysis system, including data preprocessing, feature extraction, model training, and evaluation. Facial expressions constitute a crucial factor in pain sentiment recognition, with the features of these expressions contributing to the identification of pain intensity. In cases where facial features are unavailable, audio-based sentiment features are utilized. Additionally, as facial expressions may vary based on individual pain perception, audio-based information can assist in determining the pain level. Therefore, combining both modalities enhances the overall effectiveness and competency of the proposed pain detection system. While several established pain detection algorithms exist, significant potential exists for further advancements in pain recognition techniques. In the current landscape, cloud-dependent mobile applications that rely on real-time data and live video processing are becoming increasingly popular. These applications typically consist of two components: the mobile front-end and cloud-based back-end. The cloud offers advancement in the processing of data and capabilities in computation. Our proposed method facilitates the execution of complex applications on devices with limited resources, optimizing performance and efficiency. This section describes a sentiment assessment method for the evaluation of pain levels based on facial expressions. The research integrates cache and computing facilities to provide essential factors for a smart portable healthcare framework. The system that has been proposed to recognize pain is structured into five key components, as illustrated in <xref rid="sensors-25-01223-f001" ref-type="fig">Figure 1</xref>.</p><p>This pictorial representation of our proposed system shows that there is a cloud network capable of storing some data. Various devices from outside the system communicate through that network. Our database is also connected to the cloud network. Our database mainly consists of images, videos, and audio. Before classification, images are fetched from the database, preprocessed, and reshaped based on the algorithm. After preprocessing, images are split into training and testing sets and fed into two <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> architectures. These <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> architectures are trained independently by using images considered for training purposes. Upon completion of training, the prediction models are obtained. These models generate scores for input samples, which are subsequently fused at the score level to arrive at a final prediction. The resulting prediction model is integrated into a network to enable remote pain intensity prediction. The detailed implementation is described in the following subsections.</p><sec id="sec3dot1-sensors-25-01223"><title>3.1. Image-Based&#x000a0;PSAS</title><p>During the image acquisition phase, noise is often introduced, which reduces the textural information and detail of facial patterns essential for pain sentiment recognition. This noise can negatively impact the accuracy of the experiment. In the image preprocessing phase, a color image <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">I</mml:mi></mml:mrow></mml:math></inline-formula> is processed to extract the facial region for identifying pain emotion. A tree-structured part model (TSPM) [<xref rid="B46-sensors-25-01223" ref-type="bibr">46</xref>] is employed, which is effective for handling various face poses. In this approach, the image is first divided into smaller parts, such as the nose, ears, eyes, and mouth. These parts are then connected using a tree-based data structure, and traditional computer vision techniques like Histogram of Oriented Gradients (HOG) and Scale-Invariant Feature Transform (SIFT) are applied to extract features from each part. The tree structure also defines the geometric relationships between the parts, which aids in handling pose variations of the face. Each part is assigned a score based on a scoring function that evaluates the spatial relationships between the parts. Optimization algorithms are employed to maximize the scoring function by training on annotated facial part locations and predicted scores, identifying several landmark points for each facial region. For frontal faces, the method detects 68 key facial landmarks, while for profile faces, 39 key landmarks are identified, focusing on features that are visible in both orientations. After detecting the facial landmarks, the extracted facial region undergoes normalization using bilinear image interpolation, resizing the face to dimensions of <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. After facial region extraction, the feature computation tasks are performed. Here, several deep learning approaches are utilized to extract valuable and discriminating features. Distinct deep learning architectures are applied to the same dataset, and the resulting outputs are fused to move towards a final decision. The mechanism of detecting the facial region for the proposed pain identification technique is illustrated in <xref rid="sensors-25-01223-f002" ref-type="fig">Figure 2</xref>.</p></sec><sec id="sec3dot2-sensors-25-01223"><title>3.2. Image-Based Feature Representation Using Proposed Deep Learning&#x000a0;Architectures</title><p>Visual feature extraction offers a reliable and consistent method for distinguishing between various objects. In applications such as emotion classification and facial expression identification, effective feature representation plays a pivotal role [<xref rid="B47-sensors-25-01223" ref-type="bibr">47</xref>]. Deep learning-based convolutional neural network (CNN) architectures are utilized here [<xref rid="B48-sensors-25-01223" ref-type="bibr">48</xref>] to learn features and classify facial region images into two or three pain-level classes. However, the performance of emotion recognition systems based on facial expressions is often hindered by overfitting, which results in sensitivity to factors such as pose, age, identity, and lighting variations. This approach addresses these challenges while reducing computational complexity. The direct classification of pain levels based on raw, unprocessed images is not feasible; hence, robust and efficient local-to-global feature representation schemes are applied for both feature synthesis and representation. CNN architectures have been highly effective in extracting diverse features from images and videos through advanced deep learning techniques [<xref rid="B49-sensors-25-01223" ref-type="bibr">49</xref>]. In CNNs, various kernels process images using mixed convolution operations [<xref rid="B50-sensors-25-01223" ref-type="bibr">50</xref>]. After the convolution operation, pooling techniques with weight sharing are employed to optimize the network training parameters. While structural and statistical dependent methodologies exist in the field of computer vision, state-of-the-art CNN-based feature representation techniques have proven to be more effective for facial expression-based pain sentiment analysis. These techniques particularly excel in handling challenges like articulation and occlusion in facial images captured under unconstrained conditions [<xref rid="B49-sensors-25-01223" ref-type="bibr">49</xref>]. They enhance system performance by overcoming the difficulties posed by occlusion and variability in face images taken in uncontrolled environments.</p><p>The proposed system incorporates a sentiment analysis component that analyzes facial expressions using deep learning features while accounting for variables that influence the effectiveness of these features. In CNNs, the convolution operation serves as the foundation of the proposed network, forming the backbone of the CNN architecture and enabling effective image feature extraction. In this process, a filter or kernel of size <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> slides across the input image <italic toggle="yes">I</italic> to generate feature maps. At each position, the operation involves non-linear transformations and element-wise matrix multiplication, with the results producing feature map elements centered on the mask pattern. To optimize CNN architectures, parameter fine-tuning is employed, and several specialized layers are integrated to enhance performance:<list list-type="order"><list-item><p><bold>Convolutional layer:</bold> This layer applies convolution to extract features from the input image. Non-linearity is introduced using an activation function, and the generated feature maps depend on learnable weights and biases.</p></list-item><list-item><p><bold>Max-pooling layer:</bold> Max pooling reduces the size of feature maps by selecting the maximum value within each patch, thereby lowering computational complexity, maintaining translation invariance, and improving feature discrimination.</p></list-item><list-item><p><bold>Batch normalization layer:</bold> This layer normalizes intermediate outputs, stabilizing and speeding up training. It also allows the use of higher learning rates effectively.</p></list-item><list-item><p><bold>Dropout layer:</bold> Acting as a regularization technique, dropout prevents overfitting by randomly deactivating neurons during training, encouraging the network to generalize better.</p></list-item><list-item><p><bold>Flatten layer:</bold> This layer converts multi-dimensional feature maps into a one-dimensional vector, bridging the convolutional layers and the fully connected layers.</p></list-item><list-item><p><bold>Fully connected layers:</bold> These layers aggregate the extracted features into a final vector, with the size corresponding to the number of class labels. The output is used for classification.</p></list-item><list-item><p><bold>Output layer:</bold> The final layer employs the Softmax function to perform classification and determine the output probabilities for each class.</p></list-item></list></p><p>The proposed feature representation schemes outline two key functions of the CNN baseline model: Extraction of features and performing classification. The CNN models are structured with seven to eight deep image perturbation layers, each designed to enhance performance. The ReLU activation function is applied as the introduction of non-linearity is required; for the reduction of spatial dimensionality max pooling is applied and captures key features; and for accelerating the training procedure and upgrading of generalization, batch normalization is employed. After being flattened into a one-dimensional vector, the resulting feature maps are processed through two fully sequentially connected layers and finally, for multi-class classification, a Softmax layer is used. To improve the performance of the CNN model, progressive image resizing techniques are applied, adding new levels to the model. These techniques help reduce overfitting, tackle imbalanced data, and optimize computational resource usage.</p><p>The proposed hybrid design of the <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> architectures enables the extraction of shape along with texture information from images. The recommended system utilizes textual and shape-based features to provide both geometrical and appearance-based descriptions. The primary drawback of the CNN model, which is thought to be significant computation power requirements, can be mitigated by using graphical processing units. Graphical processing units are primarily designed to handle high-volume computations, encompassing a wide range of representation and concept levels, including transfer learning, pre-training, data patterns, and techniques for fine-tuning convolutional neural networks. In this work, during training the proposed <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> frameworks, some large databases are used to train the architectures by weight adjustment of the network depending on the number of defined pain classes. So, this research proposes two CNN frameworks: <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The respective trained models generated from these <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> architectures are <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. For a better understanding and transparency regarding the models, the specifics of these architectures with their use of input&#x02013;output hidden layers, convoluted image output shape, and parameters produced at each layer are further described. These are shown in <xref rid="sensors-25-01223-f003" ref-type="fig">Figure 3</xref> and <xref rid="sensors-25-01223-f004" ref-type="fig">Figure 4</xref>, respectively. Similarly, <xref rid="sensors-25-01223-t002" ref-type="table">Table 2</xref> and <xref rid="sensors-25-01223-t003" ref-type="table">Table 3</xref> illustrate the list of parameters required for training these <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> architectures.</p><p>Once training is completed, the trained model <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is derived from the <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> architecture. To enhance feature representation, another architecture, <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, is introduced. It comprises eight blocks, each containing a convolutional layer, max-pooling, batch normalization, and dropout layers. After these blocks, the architecture includes three flattened layers, each followed by a dense layer with batch normalization, the ReLU activation function, and a dropout layer. The final layer is a dense layer used for output classification. The <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> model is compiled using the Adam optimizer, and the trained model <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is derived upon completion of training. Both the <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> architectures are designed to enhance performance through improved feature abstraction. These architectures significantly contribute to the image-based pain sentiment analysis system by providing refined and efficient models.</p><p>(i) <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>: The first factor involves incremental image resizing. The input layers in <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> continuously receive images as input data. Convolution operations are performed using multiple masks or filter banks, generating various convolved images or feature maps based on the filters [<xref rid="B51-sensors-25-01223" ref-type="bibr">51</xref>]. As image size increases, convolution operations become more computationally demanding due to the growing number of filters. The parameters within these filter sets are considered as the refined weights. For max-pooling layers, the number of parameters in the network also impacts computational overhead. It has been observed that analyzing patterns from images at multiple resolutions enhances the feature representation capabilities of <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> frameworks using deep neural network techniques. Adding layers to the architecture as image resolution increases allows for a deeper analysis of hidden patterns in the feature maps. Motivated by this, multi-resolution facial image techniques have been implemented using various CNN architectures and layers. Specifically, two different image resolutions of the normalized facial region <italic toggle="yes">F</italic> are considered in <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is down-sampled to <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>(ii) <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>: The second aspect involves utilizing transfer learning combined with fine-tuning techniques. Transfer learning facilitates the development of discriminant features essential for the pain sentiment analysis system (PSAS), demanding higher-order feature representation. In this process, certain layers and their parameters are frozen, while others are retrained to reduce computational complexity. This approach enables a model trained for one problem to be effectively applied to related tasks. In the proposed model, transfer learning integrates CNN-based deep learning, utilizing layers and weights from a pre-trained model. This reduces training time and minimizes generalization errors.</p><p>(iii) <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>: The third factor involves the application of fusion techniques. Trained models generate features or scores, which are subsequently transformed into new sets of features or scores. Fusion techniques are employed to combine classification scores from trained deep learning models, to enhance the overall performance of the proposed pain identification system. In this particular research activity, the scores are fused after the completion of the classification with the help of <italic toggle="yes">product-of-score</italic>, <italic toggle="yes">weighted-sum-of-score</italic>, and <italic toggle="yes">sum-of-score</italic>. For instance, consider two trained deep learning models, <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b6;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b6;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, which solve an <italic toggle="yes">L</italic>-class problem and generate classification scores <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for a test sample <italic toggle="yes">a</italic>. Fusion methods are applied as follows:<list list-type="bullet"><list-item><p><inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (<italic toggle="yes">product of score</italic>);</p></list-item><list-item><p><inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mi>w</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (<italic toggle="yes">weighted sum of score</italic>);</p></list-item><list-item><p><inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (<italic toggle="yes">sum of score</italic>).</p></list-item></list>
Here, <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are weights assigned to models <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b6;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b6;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, respectively, with <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c1;</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Experimentally, it has been observed that if <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b6;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> outperforms <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b6;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, then <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and vice versa.</p></sec><sec id="sec3dot3-sensors-25-01223"><title>3.3. Audio-Based&#x000a0;PSAS</title><p>In the digital world, audio-based media content is kept in the form of signal processing, where dealing with, processing, and manipulation of audio signals are performed for various applications such as speaker recognition, gender classification, age classification, disease detection, entertainment, and health care. Converting analogue signals to digital signals in audio files is challenging for various types of media content, such as text, images, videos, and audio. The chances of unwanted noise introduction and unbalanced time&#x02013;frequency distribution make audio-based recognition systems much more challenging. In this work, an audio-based speaker&#x02019;s sentiment analysis is performed to detect the intensity levels of their body-part pain sentiments. The purpose of an audio noise reduction system is to remove noise from audio signals, such as speech. There are two types of these systems: Complementary and non-complementary. Before properly recording the audio stream, complementary noise reduction entails compressing it. Conversely, single-ended, or non-complementary, noise reduction works well for lowering noise levels [<xref rid="B52-sensors-25-01223" ref-type="bibr">52</xref>]. Digital and analogue devices are vulnerable to noise because of specific characteristics. It is possible to extract different aspects more precisely when noise is removed from audio signals. In this work, we have utilized filtered audio signals to extract audio features for pain sentiment analysis. Here, identifying pain sentiment levels is based on analyzing the types of emotions a person has during human&#x02013;computer-based interaction in the healthcare system. Fortunately, facial expressions are analyzed or recognized in image- or video-based emotion recognition. Still, in real-time scenarios, video-based media might record the vocal emotions of a patient in the online healthcare framework. Then, there will be the requirement for audio-based emotion recognition systems. Here, feature representation using the deep learning-based approach for audio-based classification is demonstrated in <xref rid="sensors-25-01223-f005" ref-type="fig">Figure 5</xref>.</p><p>This work utilizes speaker audio recordings for audio-based pain sentiment evaluation. The audio data are provided in waveform format, consisting of a sequence of bytes representing audio signals over time. Hence, for analyzing these sequences of bytes in terms of numeric feature representation, some feature extraction techniques are employed for feature computation from the waveform audio files. These feature extraction techniques are described as follows:<list list-type="bullet"><list-item><p>Statistical audio features: Here, the audio signal initially undergoes a frequency-domain signal analysis using fast Fourier transformation (FFT) [<xref rid="B53-sensors-25-01223" ref-type="bibr">53</xref>]. The computed frequencies are subsequently employed to calculate descriptive statistics, including mean, median, standard deviation, quartiles, and kurtosis. The magnitude of frequency components is used to calculate energy and root mean square (RMS) energy. Here, the use of FFT transforms the audio signals into frequency components, which are used to analyze audio characteristics, such as to measure tone, pitch, and spectra content. So, the measure of descriptive statistics gives a general representation of these audio characteristics, whereas energy and RMS energy distinguish low and high loudness intensities with variations in their magnitudes.</p></list-item><list-item><p>Mel-frequency cepstral coefficients (MFCCs): These represent a well-known audio-based feature computation technique [<xref rid="B54-sensors-25-01223" ref-type="bibr">54</xref>], where the derived frequency components from FFT undergo a log-amplitude transformation. Then, the mel scale is introduced to the logarithm of the amplitude spectrum, discrete cosine transformations (DCTs) are applied on the mel scale, and finally 2&#x02013;13 DCT coefficients are kept. The rest are discarded during feature computations. Here, the extracted final features of MFCCs represent the compactness (i.e., better for discriminating audio signals between subjects), and apply to domain relevance for diverse applications of audio-based recognition systems.</p></list-item><list-item><p>Spectral features: Various spectral features like spectral contrast, spectral centroid, and spectral bandwidth are analyzed from the audio files. These spectral features [<xref rid="B55-sensors-25-01223" ref-type="bibr">55</xref>] are related to the spectrograms of the audio files. The spectrogram represents the frequency intensities over time. It is measured from the squared magnitude of the STFT [<xref rid="B56-sensors-25-01223" ref-type="bibr">56</xref>], which is obtained by computing an FFT over successive signal frames. These types of features can extract richness and harmonicity from the audio signals.</p></list-item></list></p><p>So, using these above feature extraction techniques, <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>-dimensional statistical features (<inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>), <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>-dimensional MFCC features (<inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>), and <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>-dimensional spectral features (<inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>) are extracted from each speech audio file. Then, these extracted feature vectors are combined such that <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Now, <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, undergoes the proposed deep learning-based feature representation followed by classification for speech audio. During feature representation, the <italic toggle="yes">d</italic>-dimensional feature vector is transformed to a 512-dimensional feature, then 256- to 128-dimensional features. Finally, during classification, these are mapped into a three-class audio-of-pain problem. The list of parameters required for this audio-based deep learning technique is demonstrated in <xref rid="sensors-25-01223-t004" ref-type="table">Table 4</xref>.</p></sec></sec><sec id="sec4-sensors-25-01223"><title>4. Experiments</title><p>The experiments in this study are conducted in two phases. An image-based pain sentiment analysis system is developed in the first phase, while the second phase focuses on an audio-based pain sentiment analysis system. For the image-based or video-based system, facial expressions are analyzed by extracting the facial region and the area of interest during preprocessing. The extracted images are resized to <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>192</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>192</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to create a normalized facial region <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow></mml:math></inline-formula>. The <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> architecture processes images of size <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, while <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> handles images of size <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>192</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>192</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Both architectures employ a 50&#x02013;50% training&#x02013;testing split, where 50% of the images or videos from each pain class are used for training and the remaining 50% for testing. The hyperparameter settings for these CNN architectures are detailed in <xref rid="sensors-25-01223-t002" ref-type="table">Table 2</xref> and <xref rid="sensors-25-01223-t003" ref-type="table">Table 3</xref>, with both architectures utilizing the &#x02018;Softmax&#x02019; classifier and the &#x02018;Adam&#x02019; optimizer. Similarly, the audio-based pain sentiment analysis system (PSAS) employs a deep learning-based feature representation approach, as described in the above section, utilizing a one-dimensional CNN architecture. Like the image-based system, the PSAS follows a 50&#x02013;50% training test protocol to evaluate its performance. The proposed pain sentiment analysis system was implemented using Python 3.11.5 version for this research. The implementation was carried out on a system featuring an Intel Core i7 7th-generation processor (7700 K) (3.20 GHz), 64 GB DDR4 RAM, and 12 GB NVIDIA TITAN XP GPU running on the Windows 10 operating system. Key Python libraries, such as Keras [<xref rid="B57-sensors-25-01223" ref-type="bibr">57</xref>] and Theano [<xref rid="B58-sensors-25-01223" ref-type="bibr">58</xref>], were utilized to support the development of deep learning-based convolutional neural network (CNN) architectures and techniques, which form the foundation of the system.</p><sec id="sec4dot1-sensors-25-01223"><title>4.1. Used&#x000a0;Databases</title><p>The evaluation of the proposed system was conducted using one audio-based, two image-based, and one video-based dataset. The audio dataset, &#x0201c;Variably Intense Vocalizations of Affect and Emotion Corpus (VIVAE)&#x0201d; [<xref rid="B59-sensors-25-01223" ref-type="bibr">59</xref>], and the image datasets, the UNBC-McMaster Shoulder Pain Expression Archive [<xref rid="B60-sensors-25-01223" ref-type="bibr">60</xref>] and the 2D Face Set Database with Pain Expressions (D2) [<xref rid="B61-sensors-25-01223" ref-type="bibr">61</xref>], were utilized for testing.</p><p>The first dataset is the UNBC-McMaster Shoulder Pain Expression Archive dataset, which comprises recordings of 129 participants (66 females and 63 males), all diagnosed with shoulder pain by physiotherapists at a clinic. The videos were captured at McMaster University, documenting participants with various shoulder pain conditions such as rotator cuff injuries, bone spur, arthritis, subluxation, dislocation, tendinitis, bursitis, impingement syndromes, and capsulitis. Images were extracted from these videos and categorized by pain intensity into classes ranging from &#x02019;no pain&#x02019; to &#x02019;high-intensity pain.&#x02019; For this study, the dataset was organized into two-class and five-class problems of classification. For the three class problems of classification, the involved labels are no pain (<inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>), which indicates the non-aggressive (NAG) class of data; low pain (<inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>), for implying the covertly aggressive (CAG) class of data; and the last one is high pain (<inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>), for indicating the overtly aggressive (OAG) class of data. Representative samples from this dataset are displayed in <xref rid="sensors-25-01223-f006" ref-type="fig">Figure 6</xref>, and a detailed description of the dataset&#x02019;s two-class and three-class classifications is provided in <xref rid="sensors-25-01223-t005" ref-type="table">Table 5</xref>.</p><p>The second image-based dataset, the 2D Face Set Database with Pain Expressions (<inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) [<xref rid="B61-sensors-25-01223" ref-type="bibr">61</xref>], includes 599 images collected from 13 women and 10 men. This dataset primarily focuses on a two-class classification problem, where 298 images are labeled as &#x02018;no pain&#x02019; (<inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>) and the remaining 298 images as &#x02018;pain&#x02019; (<inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>). A comprehensive description of this database is available in <xref rid="sensors-25-01223-t005" ref-type="table">Table 5</xref>, with sample images shown in <xref rid="sensors-25-01223-f007" ref-type="fig">Figure 7</xref>.</p><p>The BioVid Heat Pain Database [<xref rid="B62-sensors-25-01223" ref-type="bibr">62</xref>] is the third dataset used in this research, and it is based on video samples. The dataset consists of twenty videos per class and five distinct classes for each of the 87 subjects. The five distinct data categories in the dataset are part A through part E, with part A selected for this work. From the list of 87 subjects, five subjects were chosen for part A, with 50 videos taken under consideration for each subject. The number of selected videos for every individual subject representing each label was ten; however, the total number of labels was five. As a whole, a total of 250 videos of a length of 5.5 s was considered; each of these videos has a total of 138 frames. The frames or images from each video were extracted and assigned one of five distinct labels, ranging from &#x0201c;no pain&#x0201d; to &#x0201c;extreme-intensity pain.&#x0201d; These labels correspond to the following pain intensity categories: baseline (<inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>), intensity of &#x02018;pain amount 1&#x02019; (<inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>), intensity of &#x02018;pain amount 2&#x02019; (<inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">S</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>), intensity of &#x02018;pain amount 3&#x02019; (<inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">S</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>), and intensity of &#x02018;pain amount 4&#x02019; (<inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">PI</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>). Example images from the BioVid Heat Pain Database are shown in <xref rid="sensors-25-01223-f008" ref-type="fig">Figure 8</xref>. A description of the BioVid Heat Pain dataset, including its correlation to image specimens for particular pain labels, is provided in <xref rid="sensors-25-01223-t005" ref-type="table">Table 5</xref>.</p><p>The fourth dataset utilized in this research is a speech audio database known as the Variably Intense Vocalizations of Affect and Emotion Corpus (VIVAE) [<xref rid="B59-sensors-25-01223" ref-type="bibr">59</xref>]. The VIVAE database comprises non-speech emotion vocalizations recorded from human participants. It contains 1085 audio recordings collected from 11 speakers, each expressing various emotions, including fear, anger, surprise, pain, achievements, and sexual pleasure. For each emotion, recordings were captured across four intensity levels: &#x02018;low&#x02019;, &#x02018;moderate&#x02019;, &#x02018;peak&#x02019;, and &#x02018;strong&#x02019;. This study focuses on detecting pain intensity, categorizing the recordings into three main classes: Non-aggressive (NAG), where the intensity of pain is zero (<inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>); covertly aggressive (CAG), representing a low pain intensity (<inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>); and overtly aggressive (OAG), corresponding to a high pain intensity (<inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>). For simplicity, recordings labeled with &#x02018;low&#x02019; and &#x02018;moderate&#x02019; intensities are grouped under the NAG class, &#x02018;peak&#x02019; is assigned to the CAG class, and &#x02018;strong&#x02019; is classified as OAG. The emotions in the dataset are thus distributed across these three pain intensity classes based on their intensity levels. Specifically, the database includes 530 audio files in the NAG class, 272 files in the CAG class, and 282 files in the OAG class. Experiments with the proposed system evaluate its performance using distinct training and testing sets derived from these recordings.</p></sec><sec sec-type="results" id="sec4dot2-sensors-25-01223"><title>4.2. Results for Image-Based&#x000a0;PSAS</title><p>The image-dependent pain sentiment analysis system (PSAS) consists of two main stages: (i) preprocessing of the image, and (ii) feature learning along with classification. In the preprocessing phase, the facial region, which is the area of interest, is first extracted. The images are then resized to <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>192</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for the normalized facial region <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow></mml:math></inline-formula>. After these transformations, the images with facial expressions from the training sample of the dataset are input to the proposed CNN architectures, as shown in <xref rid="sensors-25-01223-f003" ref-type="fig">Figure 3</xref> and <xref rid="sensors-25-01223-f004" ref-type="fig">Figure 4</xref>. Specifically, the <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> architecture processes input images of size <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, while the <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> architecture handles images of size <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>192</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>192</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Within these CNN architectures, feature extraction and pain sentiment classification are performed.</p><p>The primary focus of the training phase is to optimize the performance of these architectures. Once trained, the models <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are tested on the test samples to assess their performance. The experimental process begins by testing the <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> architecture with a 50&#x02013;50% training&#x02013;testing split, applied to both the 2DFPE database (two classes of pain level) and the UNBC database (three classes of pain level).</p><list list-type="bullet"><list-item><p><bold><inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> experiment:</bold> This experiment is centered around parameter learning, focusing on batch size and epoch count, which are critical factors for the performance of any CNN architecture. Both the batch size and the number of epochs have a substantial impact on the ability of the network to learn when training on the provided samples. One of the primary tasks in deep CNN models is optimizing the learning of weight parameters. To improve this process, this study identifies an optimal trade-off between a batch size of 16 and 1500 epochs, aiming to enhance the outcomes of the proposed system. The results of an experiment comparing the effects of batch size and epoch count on system performance are presented in <xref rid="sensors-25-01223-f009" ref-type="fig">Figure 9</xref> using the UNBC two-class dataset. As shown in the figure, performance improves with a batch size of 16, and the epoch count varies between 1000 and 1500.</p></list-item></list><fig position="anchor" id="sensors-25-01223-f009"><label>Figure 9</label><caption><p>Demonstration of utilization of <inline-formula><mml:math id="mm166" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> experiments, exploring the effect of batch size vs. epochs on the proposed system&#x02019;s performance.</p></caption><graphic xlink:href="sensors-25-01223-g009" position="float"/></fig><list list-type="simple"><list-item><p>In <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the experiments are conducted to incorporate multi-resolution facial images using progressive image resizing. This approach leverages multi-resolution image analysis to take advantage of progressive image scaling, allowing images of two different sizes to be processed by the <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> architectures. Compared to the standard <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> architecture performance, <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> offers several advantages: (i) networks can be trained with images having diverse dimensions, such as from low to high; (ii) representations of hierarchical features are obtained for each image, leading to enhanced texture pattern discrimination; and (iii) overfitting issues are minimized. The performance of multi-resolution image analysis in the proposed PSAS is illustrated in <xref rid="sensors-25-01223-f010" ref-type="fig">Figure 10</xref> for the 2DFPE database (2 classes of pain level), UNBC database (2 classes of pain level), UNBC database (3 classes of pain level), and BioVid Heat Pain dataset (5 classes of pain level). The figure demonstrates that the <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> model effectively addresses overfitting and enhances the proposed system&#x02019;s overall performance.</p></list-item></list><fig position="anchor" id="sensors-25-01223-f010"><label>Figure 10</label><caption><p>Demonstration of <inline-formula><mml:math id="mm167" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> experiments performing multi-resolution image analysis on the performance of the proposed system.</p></caption><graphic xlink:href="sensors-25-01223-g010" position="float"/></fig><list list-type="bullet"><list-item><p><bold><inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> experiment:</bold> This experiment investigates the influence of applying transfer learning with fine-tuning on the performance of the two new CNN models trained on the same image data domain. Fine-tuning involves adjusting the weights of an existing CNN architecture, which reduces training time and improves results. Transfer learning is an improvised version of machine learning, in which any model is trained to fulfill one specific task and utilized as the base for constructing a new framework for a similar task. In this experiment, two transfer learning strategies are tested: (i) the first approach trains both CNN architectures, <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, from scratch using the appropriate image sizes; and (ii) the second approach retrains the models using pre-trained weights from both architectures. The results indicate that the retrained models outperform those trained from scratch. <xref rid="sensors-25-01223-t006" ref-type="table">Table 6</xref> displays the outcome of the CNN architectures, where the evaluation is performed based on accuracy and <italic toggle="yes">F1-score</italic>. The findings suggest that approach 1 improves the performance of the proposed PSAS.</p></list-item></list><table-wrap position="anchor" id="sensors-25-01223-t006"><object-id pub-id-type="pii">sensors-25-01223-t006_Table 6</object-id><label>Table 6</label><caption><p>Proposed PSAS&#x02019;s performance when applying approach 1 (A1) &#x00026; approach 2 (A2) of fine-tuning and transfer learning.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th colspan="4" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Using <inline-formula><mml:math id="mm188" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-script">I</mml:mi><mml:mrow><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>128</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula></th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc. (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc. (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2DFPE (2 classes of pain level)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7563</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.22</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7041</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UNBC (2 classes of pain level)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8174</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7918</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UNBC (3 classes of pain level)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8437</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8213</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BioVid (5 classes of pain level)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3304</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3016</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dataset</td><td colspan="4" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Using <inline-formula><mml:math id="mm189" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">I</mml:mi><mml:mrow><mml:mn>192</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>192</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2DFPE (2 classes of pain level)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7533</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7524</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UNBC (2 classes of pain level)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7916</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7414</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UNBC (3 classes of pain level)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8334</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8103</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BioVid (5 classes of pain level)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3571</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3518</td></tr></tbody></table></table-wrap><list list-type="bullet"><list-item><p><bold><inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> experiment:</bold> In this experiment, various fusion techniques are applied to the classification outcome scores produced by the trained <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> architectures. The classification results are obtained from deep learning features, and a post-classification fusion approach is employed. The classification scores generated by each CNN model are then processed using the <italic toggle="yes">weighted-sum rule</italic>, <italic toggle="yes">product rule</italic>, and <italic toggle="yes">sum rule</italic> fusion methodologies. <xref rid="sensors-25-01223-t007" ref-type="table">Table 7</xref> presents the fused outcome performance of the proposed system based on different combinations of these fusion methods. The results show that the system performs optimally when using the <italic toggle="yes">product rule</italic> fusion method, which integrates both CNN architectures, surpassing the performance of the <italic toggle="yes">weighted-sum rule</italic>, <italic toggle="yes">sum rule</italic>, and other techniques. The proposed system achieves accuracy rates of 85.15% for the two-class UNBC dataset, 83.79% for the three-class UNBC dataset, and 77.41% for the two-class 2DFPE dataset, which will be referenced in the subsequent section.</p></list-item></list><table-wrap position="anchor" id="sensors-25-01223-t007"><object-id pub-id-type="pii">sensors-25-01223-t007_Table 7</object-id><label>Table 7</label><caption><p>Performance outcome of the proposed PSAS in terms of Acc. (%) when applying fusion methods to different CNN architectures; CPL means classes of pain levels.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">2DFPE (2-CPL)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UNBC (2-CPL)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">UNBC (3-CPL)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">BioVid (5-CPL)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm190" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (A1)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.11</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm191" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (A2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.22</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.73</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sum rule&#x000a0;(A1 + A2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.67</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Product rule&#x000a0;(A1 &#x000d7; A2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.13</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Weighted-sum rule&#x000a0;&#x000a0;(A1A2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.23</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm204" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (A1)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.45</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm205" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (A2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.89</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sum rule (A1 + A2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.74</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.89</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Product rule (A1 &#x000d7; A2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>77.41</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>85.15</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>83.38</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>38.04</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Weighted-sum rule (A1A2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.07</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.83</td></tr></tbody></table></table-wrap><list list-type="simple"><list-item><p>Facial expressions from people of different ethnicities influence the performance of facial expression recognition systems. To address this, the proposed system incorporates feature representation methods specifically designed for non-verbal communication, which aim to enhance performance across diverse ethnic groups and across age variations. While facial expressions are largely universal&#x02014;encompassing seven universally recognized basic expressions [<xref rid="B63-sensors-25-01223" ref-type="bibr">63</xref>]&#x02014;the challenge of ethnicity impacting recognition accuracy remains significant, primarily due to the scarcity of pain datasets explicitly designed to encompass diverse ethnicities across different age groups. Here, the proposed system&#x02019;s robustness in identifying facial expressions is validated across ethnicities with variations in age; an experiment was conducted using the challenging AffectNet [<xref rid="B64-sensors-25-01223" ref-type="bibr">64</xref>] facial expression dataset, which is composed of 29,042 image samples spanning eight facial expression categories: anger, contempt, disgust, fear, happy, neutral, sad, and surprise. <xref rid="sensors-25-01223-f011" ref-type="fig">Figure 11</xref> illustrates examples from this dataset. The facial expression recognition system is built using the same deep learning architectures (<xref rid="sensors-25-01223-f003" ref-type="fig">Figure 3</xref> and <xref rid="sensors-25-01223-f004" ref-type="fig">Figure 4</xref>) and training&#x02013;testing protocols employed in the proposed pain sentiment analysis system (PSAS). The system&#x02019;s performance is compared with competing methods used for PSA, and the results are summarized in <xref rid="sensors-25-01223-t008" ref-type="table">Table 8</xref>. The findings show that the proposed feature representation methods achieve superior performance, even on ethnically diverse data and data with variations in age in the facial expression dataset. This demonstrates the system&#x02019;s robustness and adaptability in accurately recognizing and analyzing facial expressions across varied demographic groups and age ranges, and also in the context of pain recognition systems.</p></list-item></list><fig position="anchor" id="sensors-25-01223-f011"><label>Figure 11</label><caption><p>Demonstration of some image samples of AffectNet dataset [<xref rid="B64-sensors-25-01223" ref-type="bibr">64</xref>] with ethnic diversity and variations in age among the subjects to validate the robustness of the proposed methodology.</p></caption><graphic xlink:href="sensors-25-01223-g011" position="float"/></fig><list list-type="bullet"><list-item><p><bold>Comparison with existing systems for the image-based PSAS:</bold> The effectiveness of the proposed system has been benchmarked against several state-of-the-art techniques across multiple datasets: the two-class 2DFPE database; the UNBC database considering the two-class and three-class classification problems; and the BioVid database considering the five-class classification problem. Feature extraction techniques, including local binary pattern (LBP) and Histogram of Oriented Gradients (HoG), as utilized in [<xref rid="B65-sensors-25-01223" ref-type="bibr">65</xref>,<xref rid="B66-sensors-25-01223" ref-type="bibr">66</xref>], were employed. For each image, the feature representation was generated using a size of <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>192</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>192</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels, and the images were divided into nine blocks for both the LBP and HoG methods. From each block, a 256-dimensional LBP feature vector [<xref rid="B65-sensors-25-01223" ref-type="bibr">65</xref>] and an 81-dimensional HoG feature vector [<xref rid="B66-sensors-25-01223" ref-type="bibr">66</xref>] were extracted, yielding 648 HoG features and 2304 LBP features per image. For deep feature extraction, methods such as ResNet50 [<xref rid="B67-sensors-25-01223" ref-type="bibr">67</xref>], Inception-v3 [<xref rid="B68-sensors-25-01223" ref-type="bibr">68</xref>], and VGG16 [<xref rid="B69-sensors-25-01223" ref-type="bibr">69</xref>] were employed, each using an image size of <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>192</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>192</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Transfer learning techniques were applied by training each network on samples from either the two-class or three-class problems. Features were then extracted from the trained networks, and neural network-based classifiers were used for the classification of test samples. Competing approaches from Lucey et al. [<xref rid="B60-sensors-25-01223" ref-type="bibr">60</xref>], Anay et al. [<xref rid="B70-sensors-25-01223" ref-type="bibr">70</xref>], and Werner et al. [<xref rid="B71-sensors-25-01223" ref-type="bibr">71</xref>] were also implemented, using the same image size of <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>192</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>192</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The results for these techniques were derived using the same training and testing protocol as the proposed system. These results are summarized in <xref rid="sensors-25-01223-t009" ref-type="table">Table 9</xref>, which shows that the proposed system outperforms the competing techniques.</p></list-item></list><table-wrap position="anchor" id="sensors-25-01223-t008"><object-id pub-id-type="pii">sensors-25-01223-t008_Table 8</object-id><label>Table 8</label><caption><p>Performance comparison of the image-based facial expression recognition system using the same deep learning techniques (<xref rid="sensors-25-01223-f003" ref-type="fig">Figure 3</xref> and <xref rid="sensors-25-01223-f004" ref-type="fig">Figure 4</xref>) and training&#x02013;testing protocols employed in the proposed pain sentiment analysis system (PSAS).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">8-Class AffectNet</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc. (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anay et al. [<xref rid="B70-sensors-25-01223" ref-type="bibr">70</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5352</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HoG&#x000a0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4018</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Inception-v3&#x000a0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">51.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4731</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LBP&#x000a0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3659</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lucey et al. [<xref rid="B60-sensors-25-01223" ref-type="bibr">60</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5345</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet50&#x000a0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3984</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG16&#x000a0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.26</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4221</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Werner et al. [<xref rid="B71-sensors-25-01223" ref-type="bibr">71</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5339</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>61.67</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.6034</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="anchor" id="sensors-25-01223-t009"><object-id pub-id-type="pii">sensors-25-01223-t009_Table 9</object-id><label>Table 9</label><caption><p>Performance comparison of image-based sentiment analysis system using 2-class, 3-class, and 5-class pain sentiment problems.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">3-Class UNBC-McMaster</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">2-Class UNBC-McMaster</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">2-Class 2DFPE</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">5-Class BioVid</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc. (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc. (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc. (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc. (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anay et al. [<xref rid="B70-sensors-25-01223" ref-type="bibr">70</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.54</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7962</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8193</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7391</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.89</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HoG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6087</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6967</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6364</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.17</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Inception-v3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6934</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7421</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6172</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.31</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LBP</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6217</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7156</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6386</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.81</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lucey et al. [<xref rid="B60-sensors-25-01223" ref-type="bibr">60</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7646</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7617</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.58</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6439</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.63</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7145</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7508</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6154</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.78</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7191</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.738</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.98</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5892</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.64</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.55</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Werner et al. [<xref rid="B71-sensors-25-01223" ref-type="bibr">71</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.98</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7233</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7531</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6148</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.61</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>83.38</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8174</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>85.07</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.8349</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>77.41</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.7528</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>37.42</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>35.84</bold>
</td></tr></tbody></table></table-wrap><list list-type="simple"><list-item><p><xref rid="sensors-25-01223-t009" ref-type="table">Table 9</xref> provides a comparison of the performance outcome of the sentiment analysis system based on images beyond both the two-class and three-class pain sentiment analysis tasks. All experiments were conducted using identical training and testing protocols. The proposed two-class classification model (A) was quantitatively compared to several competing methods: Anay et al. [<xref rid="B70-sensors-25-01223" ref-type="bibr">70</xref>] (B), HoG [<xref rid="B66-sensors-25-01223" ref-type="bibr">66</xref>] (C), Inception-v3 [<xref rid="B68-sensors-25-01223" ref-type="bibr">68</xref>] (D), LBP [<xref rid="B65-sensors-25-01223" ref-type="bibr">65</xref>] (E), Lucey et al. [<xref rid="B60-sensors-25-01223" ref-type="bibr">60</xref>] (F), ResNet50 [<xref rid="B67-sensors-25-01223" ref-type="bibr">67</xref>] (G), VGG16 [<xref rid="B69-sensors-25-01223" ref-type="bibr">69</xref>] (H), and Werner et al. [<xref rid="B71-sensors-25-01223" ref-type="bibr">71</xref>] (I). A comparative statistical evaluation of the proposed system (A) against the competing approaches ({B, C, D, E, F, G, H, I}) is provided in <xref rid="sensors-25-01223-t010" ref-type="table">Table 10</xref>. A one-tailed <italic toggle="yes">t</italic>-test [<xref rid="B72-sensors-25-01223" ref-type="bibr">72</xref>] was performed using the t-statistic to analyze the results. This test involved two hypotheses: (i) H0 (null hypothesis): <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (indicating that the proposed system (A) demonstrates comparable results to the competing approaches on average); and (ii) H1 (alternative hypothesis): <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (suggesting that the proposed system (A) performs better than the competing methods on average). Rejection of the null hypothesis occurs if the <italic toggle="yes">p</italic>-value is less than 0.05, signifying that the proposed system (A) demonstrates better performance than the competing methods. For the quantitative comparison, each test dataset from the employed databases was divided into two sets. For the <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> database, there were 298 test images (set 1: 149; set 2: 149). For the UNBC (two-class/three-class) problem, there were 24,199 test samples (set 1: 12,100; set 2: 12,099). The performance of the proposed and competing methods was evaluated using these test sets. The results, shown in <xref rid="sensors-25-01223-t010" ref-type="table">Table 10</xref> confirm that the alternative hypothesis is supported in all cases, and the null hypothesis is rejected, highlighting that the proposed system (A) consistently outshines the competing approaches.</p></list-item></list></sec><sec sec-type="results" id="sec4dot3-sensors-25-01223"><title>4.3. Results for Audio-Based&#x000a0;PSAS</title><p>The audio-based pain sentiment analysis system (PSAS) uses a deep learning-based feature representation approach, as described in the above sections (<xref rid="sensors-25-01223-f005" ref-type="fig">Figure 5</xref>). This method involves extracting various audio-based features, including statistical audio features, mel-frequency cepstral coefficients (MFCCs), and spectral features, all obtained from each speech audio using the Librosa Python package [<xref rid="B73-sensors-25-01223" ref-type="bibr">73</xref>]. The Librosa package is widely used for the analysis of audio, speech, and music. Specifically, 11 statistical features, 128 MFCC features, and 224 spectral features were derived from the audio data. During the experiments, each individual feature set, as well as combinations of these features, was tested by the use of machine learning classifiers such as Decision Tree, Logistic Regression, K-Nearest Neighbors, and Support Vector Machine (SVM), with 50&#x02013;50% and 75&#x02013;25% training&#x02013;testing splits. These feature sets, both individually and in combination, were also applied within the deep learning-based feature representation framework. <xref rid="sensors-25-01223-t011" ref-type="table">Table 11</xref> summarizes the performance results of the proposed audio-based pain sentiment recognition system when evaluated with these machine learning classifiers.</p><p>The performance results displayed in <xref rid="sensors-25-01223-t011" ref-type="table">Table 11</xref> indicate that, in most cases, the Decision Tree classifier achieved the highest accuracy across various feature types for both the 50&#x02013;50% and 75&#x02013;25% training&#x02013;testing splits. Moreover, it was found that MFCC and spectral features had a greater impact on performance than the statistical audio frequency features. Consequently, the 128 MFCC and 224 spectral features were combined to form a 352-dimensional feature vector for every speech audio sample. Then, this vector was utilized as input for the proposed deep learning architecture in the sentiment analysis system based on audio data. The deep neural network experiment was carried out with a batch size of 32, and the number of epochs ranged from 1 to 50. The performance outcome of the proposed PSAS is presented in <xref rid="sensors-25-01223-f012" ref-type="fig">Figure 12</xref>, where the results show that the optimal performance was achieved with a batch size of 32 and 100 epochs. Based on these results, these settings for batch size and epochs were selected for further experimentation with the audio-based PSAS.</p><p>The performance results shown in <xref rid="sensors-25-01223-t011" ref-type="table">Table 11</xref> and <xref rid="sensors-25-01223-f012" ref-type="fig">Figure 12</xref> reveal that, when using machine learning classifiers, the Decision Tree outperforms all other classifiers across the various audio-based features. However, when deep features are utilized, as illustrated in <xref rid="sensors-25-01223-f012" ref-type="fig">Figure 12</xref>a,b, the proposed sentiment analysis system based on audio data demonstrates enhanced performance when using a batch size of 32 and the number of epochs is 100. The results of these two experimental setups are further compared in <xref rid="sensors-25-01223-f013" ref-type="fig">Figure 13</xref>, where the performance of both the Decision Tree and deep learning-based methods is evaluated under different training&#x02013;testing protocols. As seen in <xref rid="sensors-25-01223-f013" ref-type="fig">Figure 13</xref>, the features extracted using deep learning models provide better performance outcomes for the proposed audio-based PSAS.</p></sec><sec sec-type="results" id="sec4dot4-sensors-25-01223"><title>4.4. Results for Multimodal&#x000a0;PSAS</title><p>Three experiments are conducted in the multimodal PSAS. In the case of the first experiment, the classification scores of 149 NAG-class image samples from the <inline-formula><mml:math id="mm159" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> database are fused with 149 NAG-class audio samples from the VIVAE database. Similarly, the classification scores of 136 OAG-class image samples from <inline-formula><mml:math id="mm160" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are fused with 136 OAG-class audio samples from VIVAE to asses the performance outcome of the proposed multimodal PSAS using the <inline-formula><mml:math id="mm161" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and VIVAE databases. In the second experiment, the classification scores of 265 NAG-class image specimens from the UNBC database are fused with 265 NAG-class audio samples from VIVAE; the classification scores of 136 CAG-class image samples from UNBC-McMaster are fused with 136 CAG-class audio samples from VIVAE; and the classification scores of 141 OAG-class image samples from UNBC-McMaster are fused with 141 OAG-class audio samples from VIVAE to asses the performance outcome of the three-class multimodal PSAS using the UNBC-McMaster and VIVAE databases. In the third experiment, the classification scores of 260 pain-level-1-class image samples from the BioVid database are fused with 260 low-intensity-class audio samples from VIVAE; the classification scores of 260 pain-level-2-class image samples from BioVid are fused with 260 moderate-intensity-class audio samples from VIVAE; the classification scores of 260 pain-level-3-class image samples from BioVid are fused with 260 strong-intensity-class audio samples from VIVAE; and the classification scores of 260 pain-level-4-class image samples from BioVid are fused with 260 peak-intensity-class audio samples from VIVAE. For all experiments, a 50&#x02013;50% training&#x02013;testing protocol is employed. The performance of these multimodal sentiment analysis systems is shown in <xref rid="sensors-25-01223-f014" ref-type="fig">Figure 14</xref>, <xref rid="sensors-25-01223-f015" ref-type="fig">Figure 15</xref>, and <xref rid="sensors-25-01223-f016" ref-type="fig">Figure 16</xref>, respectively. These figures clearly show that the proposed system performs exceptionally well when using the decision-level (majority voting) fusion technique.</p><p>A computational complexity analysis of the proposed system is presented in <xref rid="sensors-25-01223-t012" ref-type="table">Table 12</xref>. The analysis focuses on the different derived models for the image-based and audio-based pain sentiment analysis systems, including the number of parameters required for training these models and the time needed to detect pain based on the image and audio data of the patient. Pain sentiment analysis plays a crucial role in the context of electronic healthcare, which increasingly relies on e-mobile and smart healthcare systems. This research involves analyzing patients&#x02019; facial images and speech audio data to determine their pain levels. However, these data can be affected by various challenges. To improve performance, (i) the combined effect of image and audio data is utilized in the multimodal pain sentiment analysis system; and (ii) advanced technologies, such as deep learning algorithms and their variants, are employed in this work.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01223"><title>5. Conclusions</title><p>This paper introduces a multimodal sentiment analysis system designed for pain detection, which integrates facial expressions and speech-audio data within an IoT-enabled healthcare framework. The approach is organized into five key components: (i) preprocessing of the images for extracting the facial patterns as regions of interest; (ii) learning and classifying features from images using two separate CNN and ensembles of these CNN architectures; (iii) enhancement of CNN performance through transfer learning, fine-tuning, and progressive image resizing techniques; (iv) speech-audio feature learning and classification utilizing both handcrafted and deep features; and (v) the fusion of classification scores from both image-based and audio-based samples to improve the overall system performance. The experiments cover two classes (&#x02018;pain&#x02019; and &#x02018;no pain&#x02019;), using the 2DFPE and UNBC shoulder pain datasets; three classes (&#x02018;non-aggressive (NAG) (no pain)&#x02019;, &#x02018;covertly aggressive (CAG) (pain of low amount)&#x02019;, using the three-class UNBC dataset based on image data and the audio-based datasets, and &#x02018;overtly aggressive (OAG) (pain of high amount)&#x02019;), and five-class (&#x02018;no pain&#x02019;, &#x02018;pain amount 1&#x02019;, &#x02018;pain amount 2&#x02019;, &#x02018;pain amount 3&#x02019;, and &#x02018;pain amount 4&#x02019;) pain classification tasks using the BioVid heat pain dataset. The experimental results demonstrate that the proposed pain sentiment analysis system surpasses benchmark methods. Additionally, the fusion of image-based and audio-based samples significantly boosts performance for all categories of pain recognition classes under this study. This work also addresses challenges such as synchronizing data across images, videos of facial expressions, and audio by enabling simultaneous data acquisition from multiple modalities, which adds complexity to the system&#x02019;s implementation. While deploying image- and audio-based technologies over the web is feasible, implementing them on hardware presents a considerable challenge. The incorporation of the proposed system into a smart healthcare framework can significantly improve m-health, e-health, and telemedicine services, particularly in quarantine times, like during the COVID-19 pandemic or other infectious illnesses, for remote monitoring and evaluation of patients&#x02019; pain levels, which lowers the risk of disease transmission and will enable the provision of prompt therapies without the need for physical contact.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors would like to express their sincere gratitude to G. G. Md. Nawaz Ali for providing the funds necessary to publish the findings of this work in this journal. His guidance and encouragement were instrumental in the successful completion of this research.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, A.G. and S.U.; methodology, A.G., S.U. and B.C.D.; software, S.U.; validation, A.G., S.U., B.C.D. and G.G.M.N.A.; formal analysis, S.U. and G.G.M.N.A.; investigation, S.U.; resources, A.G.; data curation, A.G.; writing&#x02014;original draft preparation, A.G. and S.U.; writing&#x02014;review and editing, A.G. and B.C.D.; visualization, G.G.M.N.A.; supervision, S.U., B.C.D. and G.G.M.N.A.; project administration, G.G.M.N.A.; funding acquisition, G.G.M.N.A. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>The ethics committee or institutional review board approval is not required for this manuscript. This research respects all the sentiments, dignity, and intrinsic values of animals or humans.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>In this manuscript, the datasets used have been acquired under proper license agreements from the corresponding institutions, following the appropriate channels.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01223"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ande</surname><given-names>R.</given-names></name>
<name><surname>Adebisi</surname><given-names>B.</given-names></name>
<name><surname>Hammoudeh</surname><given-names>M.</given-names></name>
<name><surname>Saleem</surname><given-names>J.</given-names></name>
</person-group><article-title>Internet of Things: Evolution and technologies from a security perspective</article-title><source>Sustain. Cities Soc.</source><year>2020</year><volume>54</volume><fpage>101728</fpage><pub-id pub-id-type="doi">10.1016/j.scs.2019.101728</pub-id></element-citation></ref><ref id="B2-sensors-25-01223"><label>2.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Khang</surname><given-names>A.</given-names></name>
</person-group><source>AI and IoT Technology and Applications for Smart Healthcare Systems</source><publisher-name>CRC Press</publisher-name><publisher-loc>Boca Raton, FL, USA</publisher-loc><year>2024</year></element-citation></ref><ref id="B3-sensors-25-01223"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Aminizadeh</surname><given-names>S.</given-names></name>
<name><surname>Heidari</surname><given-names>A.</given-names></name>
<name><surname>Dehghan</surname><given-names>M.</given-names></name>
<name><surname>Toumaj</surname><given-names>S.</given-names></name>
<name><surname>Rezaei</surname><given-names>M.</given-names></name>
<name><surname>Navimipour</surname><given-names>N.J.</given-names></name>
<name><surname>Stroppa</surname><given-names>F.</given-names></name>
<name><surname>Unal</surname><given-names>M.</given-names></name>
</person-group><article-title>Opportunities and challenges of artificial intelligence and distributed systems to improve the quality of healthcare service</article-title><source>Artif. Intell. Med.</source><year>2024</year><volume>149</volume><fpage>102779</fpage><pub-id pub-id-type="doi">10.1016/j.artmed.2024.102779</pub-id><pub-id pub-id-type="pmid">38462281</pub-id>
</element-citation></ref><ref id="B4-sensors-25-01223"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Muhammad</surname><given-names>G.</given-names></name>
<name><surname>Alsulaiman</surname><given-names>M.</given-names></name>
<name><surname>Amin</surname><given-names>S.U.</given-names></name>
<name><surname>Ghoneim</surname><given-names>A.</given-names></name>
<name><surname>Alhamid</surname><given-names>M.F.</given-names></name>
</person-group><article-title>A facial-expression monitoring system for improved healthcare in smart cities</article-title><source>IEEE Access</source><year>2017</year><volume>5</volume><fpage>10871</fpage><lpage>10881</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2017.2712788</pub-id></element-citation></ref><ref id="B5-sensors-25-01223"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Williams</surname><given-names>A.d.</given-names></name>
</person-group><article-title>Facial expression of pain: An evolutionary account</article-title><source>Behav. Brain Sci.</source><year>2002</year><volume>25</volume><fpage>439</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1017/S0140525X02000080</pub-id><pub-id pub-id-type="pmid">12879700</pub-id>
</element-citation></ref><ref id="B6-sensors-25-01223"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Payen</surname><given-names>J.F.</given-names></name>
<name><surname>Bru</surname><given-names>O.</given-names></name>
<name><surname>Bosson</surname><given-names>J.L.</given-names></name>
<name><surname>Lagrasta</surname><given-names>A.</given-names></name>
<name><surname>Novel</surname><given-names>E.</given-names></name>
<name><surname>Deschaux</surname><given-names>I.</given-names></name>
<name><surname>Lavagne</surname><given-names>P.</given-names></name>
<name><surname>Jacquot</surname><given-names>C.</given-names></name>
</person-group><article-title>Assessing pain in critically ill sedated patients by using a behavioral pain scale</article-title><source>Crit. Care Med.</source><year>2001</year><volume>29</volume><fpage>2258</fpage><lpage>2263</lpage><pub-id pub-id-type="doi">10.1097/00003246-200112000-00004</pub-id><pub-id pub-id-type="pmid">11801819</pub-id>
</element-citation></ref><ref id="B7-sensors-25-01223"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>McGuire</surname><given-names>B.</given-names></name>
<name><surname>Daly</surname><given-names>P.</given-names></name>
<name><surname>Smyth</surname><given-names>F.</given-names></name>
</person-group><article-title>Chronic pain in people with an intellectual disability: Under-recognised and under-treated?</article-title><source>J. Intellect. Disabil. Res.</source><year>2010</year><volume>54</volume><fpage>240</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1111/j.1365-2788.2010.01254.x</pub-id><pub-id pub-id-type="pmid">20387264</pub-id>
</element-citation></ref><ref id="B8-sensors-25-01223"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Puntillo</surname><given-names>K.A.</given-names></name>
<name><surname>Morris</surname><given-names>A.B.</given-names></name>
<name><surname>Thompson</surname><given-names>C.L.</given-names></name>
<name><surname>Stanik-Hutt</surname><given-names>J.</given-names></name>
<name><surname>White</surname><given-names>C.A.</given-names></name>
<name><surname>Wild</surname><given-names>L.R.</given-names></name>
</person-group><article-title>Pain behaviors observed during six common procedures: Results from Thunder Project II</article-title><source>Crit. Care Med.</source><year>2004</year><volume>32</volume><fpage>421</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1097/01.CCM.0000108875.35298.D2</pub-id><pub-id pub-id-type="pmid">14758158</pub-id>
</element-citation></ref><ref id="B9-sensors-25-01223"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Herr</surname><given-names>K.</given-names></name>
<name><surname>Coyne</surname><given-names>P.J.</given-names></name>
<name><surname>Key</surname><given-names>T.</given-names></name>
<name><surname>Manworren</surname><given-names>R.</given-names></name>
<name><surname>McCaffery</surname><given-names>M.</given-names></name>
<name><surname>Merkel</surname><given-names>S.</given-names></name>
<name><surname>Pelosi-Kelly</surname><given-names>J.</given-names></name>
<name><surname>Wild</surname><given-names>L.</given-names></name>
</person-group><article-title>Pain assessment in the nonverbal patient: Position statement with clinical practice recommendations</article-title><source>Pain Manag. Nurs.</source><year>2006</year><volume>7</volume><fpage>44</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1016/j.pmn.2006.02.003</pub-id><pub-id pub-id-type="pmid">16730317</pub-id>
</element-citation></ref><ref id="B10-sensors-25-01223"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Twycross</surname><given-names>A.</given-names></name>
<name><surname>Voepel-Lewis</surname><given-names>T.</given-names></name>
<name><surname>Vincent</surname><given-names>C.</given-names></name>
<name><surname>Franck</surname><given-names>L.S.</given-names></name>
<name><surname>von Baeyer</surname><given-names>C.L.</given-names></name>
</person-group><article-title>A debate on the proposition that self-report is the gold standard in assessment of pediatric pain intensity</article-title><source>Clin. J. Pain</source><year>2015</year><volume>31</volume><fpage>707</fpage><lpage>712</lpage><pub-id pub-id-type="doi">10.1097/AJP.0000000000000165</pub-id><pub-id pub-id-type="pmid">25370143</pub-id>
</element-citation></ref><ref id="B11-sensors-25-01223"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Knox</surname><given-names>D.</given-names></name>
<name><surname>Beveridge</surname><given-names>S.</given-names></name>
<name><surname>Mitchell</surname><given-names>L.A.</given-names></name>
<name><surname>MacDonald</surname><given-names>R.A.</given-names></name>
</person-group><article-title>Acoustic analysis and mood classification of pain-relieving music</article-title><source>J. Acoust. Soc. Am.</source><year>2011</year><volume>130</volume><fpage>1673</fpage><lpage>1682</lpage><pub-id pub-id-type="doi">10.1121/1.3621029</pub-id><pub-id pub-id-type="pmid">21895104</pub-id>
</element-citation></ref><ref id="B12-sensors-25-01223"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Giordano</surname><given-names>V.</given-names></name>
<name><surname>Luister</surname><given-names>A.</given-names></name>
<name><surname>Reuter</surname><given-names>C.</given-names></name>
<name><surname>Czedik-Eysenberg</surname><given-names>I.</given-names></name>
<name><surname>Singer</surname><given-names>D.</given-names></name>
<name><surname>Steyrl</surname><given-names>D.</given-names></name>
<name><surname>Vettorazzi</surname><given-names>E.</given-names></name>
<name><surname>Deindl</surname><given-names>P.</given-names></name>
</person-group><article-title>Audio Feature Analysis for Acoustic Pain Detection in Term Newborns</article-title><source>Neonatology</source><year>2022</year><volume>119</volume><fpage>760</fpage><lpage>768</lpage><pub-id pub-id-type="doi">10.1159/000526209</pub-id><pub-id pub-id-type="pmid">36116434</pub-id>
</element-citation></ref><ref id="B13-sensors-25-01223"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Oshrat</surname><given-names>Y.</given-names></name>
<name><surname>Bloch</surname><given-names>A.</given-names></name>
<name><surname>Lerner</surname><given-names>A.</given-names></name>
<name><surname>Cohen</surname><given-names>A.</given-names></name>
<name><surname>Avigal</surname><given-names>M.</given-names></name>
<name><surname>Zeilig</surname><given-names>G.</given-names></name>
</person-group><article-title>Speech prosody as a biosignal for physical pain detection</article-title><source>Proceedings of the Conference on 8th Speech Prosody</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>31 May&#x02013;3 June 2016</conf-date><fpage>420</fpage><lpage>424</lpage></element-citation></ref><ref id="B14-sensors-25-01223"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ren</surname><given-names>Z.</given-names></name>
<name><surname>Cummins</surname><given-names>N.</given-names></name>
<name><surname>Han</surname><given-names>J.</given-names></name>
<name><surname>Schnieder</surname><given-names>S.</given-names></name>
<name><surname>Krajewski</surname><given-names>J.</given-names></name>
<name><surname>Schuller</surname><given-names>B.</given-names></name>
</person-group><article-title>Evaluation of the pain level from speech: Introducing a novel pain database and benchmarks</article-title><source>Proceedings of the Speech Communication; 13th ITG-Symposium</source><conf-loc>Oldenburg, Germany</conf-loc><conf-date>10&#x02013;12 October 2018</conf-date><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B15-sensors-25-01223"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ashraf</surname><given-names>A.B.</given-names></name>
<name><surname>Lucey</surname><given-names>S.</given-names></name>
<name><surname>Cohn</surname><given-names>J.F.</given-names></name>
<name><surname>Chen</surname><given-names>T.</given-names></name>
<name><surname>Ambadar</surname><given-names>Z.</given-names></name>
<name><surname>Prkachin</surname><given-names>K.M.</given-names></name>
<name><surname>Solomon</surname><given-names>P.E.</given-names></name>
</person-group><article-title>The painful face&#x02013;pain expression recognition using active appearance models</article-title><source>Image Vis. Comput.</source><year>2009</year><volume>27</volume><fpage>1788</fpage><lpage>1796</lpage><pub-id pub-id-type="doi">10.1016/j.imavis.2009.05.007</pub-id><pub-id pub-id-type="pmid">22837587</pub-id>
</element-citation></ref><ref id="B16-sensors-25-01223"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lucey</surname><given-names>P.</given-names></name>
<name><surname>Cohn</surname><given-names>J.</given-names></name>
<name><surname>Howlett</surname><given-names>J.</given-names></name>
<name><surname>Lucey</surname><given-names>S.</given-names></name>
<name><surname>Sridharan</surname><given-names>S.</given-names></name>
</person-group><article-title>Recognizing emotion with head pose variation: Identifying pain segments in video</article-title><source>IEEE Trans. Syst. Man Cybern.-Part B</source><year>2011</year><volume>41</volume><fpage>664</fpage><lpage>674</lpage><pub-id pub-id-type="doi">10.1109/TSMCB.2010.2082525</pub-id><pub-id pub-id-type="pmid">21097382</pub-id>
</element-citation></ref><ref id="B17-sensors-25-01223"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Littlewort-Ford</surname><given-names>G.</given-names></name>
<name><surname>Bartlett</surname><given-names>M.S.</given-names></name>
<name><surname>Movellan</surname><given-names>J.R.</given-names></name>
</person-group><article-title>Are your eyes smiling? Detecting genuine smiles with support vector machines and Gabor wavelets</article-title><source>Proceedings of the 8th Joint Symposium on Neural Computation</source><conf-loc>La Jolla, CA, USA</conf-loc><conf-date>19 May 2001</conf-date><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="B18-sensors-25-01223"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Umer</surname><given-names>S.</given-names></name>
<name><surname>Dhara</surname><given-names>B.C.</given-names></name>
<name><surname>Chanda</surname><given-names>B.</given-names></name>
</person-group><article-title>Face recognition using fusion of feature learning techniques</article-title><source>Measurement</source><year>2019</year><volume>146</volume><fpage>43</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/j.measurement.2019.06.008</pub-id></element-citation></ref><ref id="B19-sensors-25-01223"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bisogni</surname><given-names>C.</given-names></name>
<name><surname>Castiglione</surname><given-names>A.</given-names></name>
<name><surname>Hossain</surname><given-names>S.</given-names></name>
<name><surname>Narducci</surname><given-names>F.</given-names></name>
<name><surname>Umer</surname><given-names>S.</given-names></name>
</person-group><article-title>Impact of deep learning approaches on facial expression recognition in healthcare industries</article-title><source>IEEE Trans. Ind. Inform.</source><year>2022</year><volume>18</volume><fpage>5619</fpage><lpage>5627</lpage><pub-id pub-id-type="doi">10.1109/TII.2022.3141400</pub-id></element-citation></ref><ref id="B20-sensors-25-01223"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cunningham</surname><given-names>P.</given-names></name>
<name><surname>Delany</surname><given-names>S.J.</given-names></name>
</person-group><article-title>k-Nearest neighbour classifiers-A Tutorial</article-title><source>ACM Comput. Surv. (CSUR)</source><year>2021</year><volume>54</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1145/3459665</pub-id></element-citation></ref><ref id="B21-sensors-25-01223"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mohankumar</surname><given-names>N.</given-names></name>
<name><surname>Narani</surname><given-names>S.R.</given-names></name>
<name><surname>Asha</surname><given-names>S.</given-names></name>
<name><surname>Arivazhagan</surname><given-names>S.</given-names></name>
<name><surname>Rajanarayanan</surname><given-names>S.</given-names></name>
<name><surname>Padmanaban</surname><given-names>K.</given-names></name>
<name><surname>Murugan</surname><given-names>S.</given-names></name>
</person-group><article-title>Advancing chronic pain relief cloud-based remote management with machine learning in healthcare</article-title><source>Indones. J. Electr. Eng. Comput. Sci.</source><year>2025</year><volume>37</volume><fpage>1042</fpage><lpage>1052</lpage><pub-id pub-id-type="doi">10.11591/ijeecs.v37.i2.pp1042-1052</pub-id></element-citation></ref><ref id="B22-sensors-25-01223"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Anderson</surname><given-names>K.</given-names></name>
<name><surname>Stein</surname><given-names>S.</given-names></name>
<name><surname>Suen</surname><given-names>H.</given-names></name>
<name><surname>Purcell</surname><given-names>M.</given-names></name>
<name><surname>Belci</surname><given-names>M.</given-names></name>
<name><surname>McCaughey</surname><given-names>E.</given-names></name>
<name><surname>McLean</surname><given-names>R.</given-names></name>
<name><surname>Khine</surname><given-names>A.</given-names></name>
<name><surname>Vuckovic</surname><given-names>A.</given-names></name>
</person-group><article-title>Generalisation of EEG-Based Pain Biomarker Classification for Predicting Central Neuropathic Pain in Subacute Spinal Cord Injury</article-title><source>Biomedicines</source><year>2025</year><volume>13</volume><elocation-id>213</elocation-id><pub-id pub-id-type="doi">10.3390/biomedicines13010213</pub-id><pub-id pub-id-type="pmid">39857795</pub-id>
</element-citation></ref><ref id="B23-sensors-25-01223"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hu</surname><given-names>F.</given-names></name>
<name><surname>Xia</surname><given-names>G.S.</given-names></name>
<name><surname>Hu</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
</person-group><article-title>Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery</article-title><source>Remote Sens.</source><year>2015</year><volume>7</volume><fpage>14680</fpage><lpage>14707</lpage><pub-id pub-id-type="doi">10.3390/rs71114680</pub-id></element-citation></ref><ref id="B24-sensors-25-01223"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yadav</surname><given-names>A.</given-names></name>
<name><surname>Vishwakarma</surname><given-names>D.K.</given-names></name>
</person-group><article-title>A comparative study on bio-inspired algorithms for sentiment analysis</article-title><source>Clust. Comput.</source><year>2020</year><volume>23</volume><fpage>2969</fpage><lpage>2989</lpage><pub-id pub-id-type="doi">10.1007/s10586-020-03062-w</pub-id></element-citation></ref><ref id="B25-sensors-25-01223"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Nugroho</surname><given-names>H.</given-names></name>
<name><surname>Harmanto</surname><given-names>D.</given-names></name>
<name><surname>Al-Absi</surname><given-names>H.R.H.</given-names></name>
</person-group><article-title>On the development of smart home care: Application of deep learning for pain detection</article-title><source>Proceedings of the 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES)</source><conf-loc>Sarawak, Malaysia</conf-loc><conf-date>3&#x02013;6 December 2018</conf-date><fpage>612</fpage><lpage>616</lpage></element-citation></ref><ref id="B26-sensors-25-01223"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Haque</surname><given-names>M.A.</given-names></name>
<name><surname>Bautista</surname><given-names>R.B.</given-names></name>
<name><surname>Noroozi</surname><given-names>F.</given-names></name>
<name><surname>Kulkarni</surname><given-names>K.</given-names></name>
<name><surname>Laursen</surname><given-names>C.B.</given-names></name>
<name><surname>Irani</surname><given-names>R.</given-names></name>
<name><surname>Bellantonio</surname><given-names>M.</given-names></name>
<name><surname>Escalera</surname><given-names>S.</given-names></name>
<name><surname>Anbarjafari</surname><given-names>G.</given-names></name>
<name><surname>Nasrollahi</surname><given-names>K.</given-names></name>
<etal/>
</person-group><article-title>Deep multimodal pain recognition: A database and comparison of spatio-temporal visual modalities</article-title><source>Proceedings of the 2018 13th IEEE International Conference on Automatic Face &#x00026; Gesture Recognition (FG 2018)</source><conf-loc>Xi&#x02019;an, China</conf-loc><conf-date>15&#x02013;19 May 2018</conf-date><fpage>250</fpage><lpage>257</lpage></element-citation></ref><ref id="B27-sensors-25-01223"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Menchetti</surname><given-names>G.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Wilkie</surname><given-names>D.J.</given-names></name>
<name><surname>Ansari</surname><given-names>R.</given-names></name>
<name><surname>Yardimci</surname><given-names>Y.</given-names></name>
<name><surname>&#x000c7;etin</surname><given-names>A.E.</given-names></name>
</person-group><article-title>Pain detection from facial videos using two-stage deep learning</article-title><source>Proceedings of the 2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP)</source><conf-loc>Ottawa, ON, Canada</conf-loc><conf-date>11&#x02013;14 November 2019</conf-date><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B28-sensors-25-01223"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yu</surname><given-names>I.C.</given-names></name>
<name><surname>Guo</surname><given-names>J.M.</given-names></name>
<name><surname>Lin</surname><given-names>W.C.</given-names></name>
<name><surname>Fang</surname><given-names>J.T.</given-names></name>
</person-group><article-title>Development of nonverbal communication behavior model for nursing students based on deep learning facial expression recognition technology</article-title><source>Cogent Educ.</source><year>2025</year><volume>12</volume><fpage>2448059</fpage><pub-id pub-id-type="doi">10.1080/2331186X.2024.2448059</pub-id></element-citation></ref><ref id="B29-sensors-25-01223"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kasundra</surname><given-names>A.</given-names></name>
<name><surname>Chanchlani</surname><given-names>R.</given-names></name>
<name><surname>Lal</surname><given-names>B.</given-names></name>
<name><surname>Thanveeru</surname><given-names>S.K.</given-names></name>
<name><surname>Ratre</surname><given-names>G.</given-names></name>
<name><surname>Ahmad</surname><given-names>R.</given-names></name>
<name><surname>Sharma</surname><given-names>P.K.</given-names></name>
<name><surname>Agrawal</surname><given-names>A.</given-names></name>
<name><surname>Kasundra Sr</surname><given-names>A.</given-names></name>
</person-group><article-title>Role of Artificial Intelligence in the Assessment of Postoperative Pain in the Pediatric Population: A Systematic Review</article-title><source>Cureus</source><year>2025</year><volume>17</volume><fpage>e12345</fpage><pub-id pub-id-type="doi">10.7759/cureus.77074</pub-id><pub-id pub-id-type="pmid">39917118</pub-id>
</element-citation></ref><ref id="B30-sensors-25-01223"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>M. Al-Eidan</surname><given-names>R.</given-names></name>
<name><surname>Al-Khalifa</surname><given-names>H.</given-names></name>
<name><surname>Al-Salman</surname><given-names>A.</given-names></name>
</person-group><article-title>Deep-learning-based models for pain recognition: A systematic review</article-title><source>Appl. Sci.</source><year>2020</year><volume>10</volume><elocation-id>5984</elocation-id><pub-id pub-id-type="doi">10.3390/app10175984</pub-id></element-citation></ref><ref id="B31-sensors-25-01223"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gouverneur</surname><given-names>P.</given-names></name>
<name><surname>Li</surname><given-names>F.</given-names></name>
<name><surname>Adamczyk</surname><given-names>W.M.</given-names></name>
<name><surname>Szikszay</surname><given-names>T.M.</given-names></name>
<name><surname>Luedtke</surname><given-names>K.</given-names></name>
<name><surname>Grzegorzek</surname><given-names>M.</given-names></name>
</person-group><article-title>Comparison of feature extraction methods for physiological signals for heat-based pain recognition</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>4838</elocation-id><pub-id pub-id-type="doi">10.3390/s21144838</pub-id><pub-id pub-id-type="pmid">34300578</pub-id>
</element-citation></ref><ref id="B32-sensors-25-01223"><label>32.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Pikulkaew</surname><given-names>K.</given-names></name>
<name><surname>Boonchieng</surname><given-names>E.</given-names></name>
<name><surname>Boonchieng</surname><given-names>W.</given-names></name>
<name><surname>Chouvatut</surname><given-names>V.</given-names></name>
</person-group><article-title>Pain detection using deep learning with evaluation system</article-title><source>Proceedings of the Fifth International Congress on Information and Communication Technology</source><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2021</year><fpage>426</fpage><lpage>435</lpage></element-citation></ref><ref id="B33-sensors-25-01223"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ismail</surname><given-names>L.</given-names></name>
<name><surname>Waseem</surname><given-names>M.D.</given-names></name>
</person-group><article-title>Towards a Deep Learning Pain-Level Detection Deployment at UAE for Patient-Centric-Pain Management and Diagnosis Support: Framework and Performance Evaluation</article-title><source>Procedia Comput. Sci.</source><year>2023</year><volume>220</volume><fpage>339</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2023.03.044</pub-id><pub-id pub-id-type="pmid">37089761</pub-id>
</element-citation></ref><ref id="B34-sensors-25-01223"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Shi</surname><given-names>Y.</given-names></name>
<name><surname>Yan</surname><given-names>S.</given-names></name>
<name><surname>Yan</surname><given-names>H.M.</given-names></name>
</person-group><article-title>Global-Local Combined Features to Detect Pain Intensity from Facial Expression Images with Attention Mechanism</article-title><source>J. Electron. Sci. Technol.</source><year>2024</year><volume>23</volume><fpage>100260</fpage><pub-id pub-id-type="doi">10.1016/j.jnlest.2024.100260</pub-id></element-citation></ref><ref id="B35-sensors-25-01223"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Othman</surname><given-names>E.</given-names></name>
<name><surname>Werner</surname><given-names>P.</given-names></name>
<name><surname>Saxen</surname><given-names>F.</given-names></name>
<name><surname>Al-Hamadi</surname><given-names>A.</given-names></name>
<name><surname>Gruss</surname><given-names>S.</given-names></name>
<name><surname>Walter</surname><given-names>S.</given-names></name>
</person-group><article-title>Classification networks for continuous automatic pain intensity monitoring in video using facial expression on the X-ITE Pain Database</article-title><source>J. Vis. Commun. Image Represent.</source><year>2023</year><volume>91</volume><fpage>103743</fpage><pub-id pub-id-type="doi">10.1016/j.jvcir.2022.103743</pub-id></element-citation></ref><ref id="B36-sensors-25-01223"><label>36.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Thiam</surname><given-names>P.</given-names></name>
<name><surname>Kessler</surname><given-names>V.</given-names></name>
<name><surname>Walter</surname><given-names>S.</given-names></name>
<name><surname>Palm</surname><given-names>G.</given-names></name>
<name><surname>Schwenker</surname><given-names>F.</given-names></name>
</person-group><article-title>Audio-visual recognition of pain intensity</article-title><source>Proceedings of the IAPR Workshop on Multimodal Pattern Recognition of Social Signals in Human-Computer Interaction</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switerland</publisher-loc><year>2017</year><fpage>110</fpage><lpage>126</lpage></element-citation></ref><ref id="B37-sensors-25-01223"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hossain</surname><given-names>M.S.</given-names></name>
</person-group><article-title>Patient state recognition system for healthcare using speech and facial expressions</article-title><source>J. Med Syst.</source><year>2016</year><volume>40</volume><fpage>272</fpage><pub-id pub-id-type="doi">10.1007/s10916-016-0627-x</pub-id><pub-id pub-id-type="pmid">27757715</pub-id>
</element-citation></ref><ref id="B38-sensors-25-01223"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zeng</surname><given-names>Z.</given-names></name>
<name><surname>Pantic</surname><given-names>M.</given-names></name>
<name><surname>Roisman</surname><given-names>G.I.</given-names></name>
<name><surname>Huang</surname><given-names>T.S.</given-names></name>
</person-group><article-title>A survey of affect recognition methods: Audio, visual and spontaneous expressions</article-title><source>Proceedings of the 9th International Conference on Multimodal Interfaces</source><conf-loc>Nagoya, Japan</conf-loc><conf-date>12&#x02013;15 November 2007</conf-date><fpage>126</fpage><lpage>133</lpage></element-citation></ref><ref id="B39-sensors-25-01223"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Thiam</surname><given-names>P.</given-names></name>
<name><surname>Bellmann</surname><given-names>P.</given-names></name>
<name><surname>Kestler</surname><given-names>H.A.</given-names></name>
<name><surname>Schwenker</surname><given-names>F.</given-names></name>
</person-group><article-title>Exploring deep physiological models for nociceptive pain recognition</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>4503</elocation-id><pub-id pub-id-type="doi">10.3390/s19204503</pub-id><pub-id pub-id-type="pmid">31627305</pub-id>
</element-citation></ref><ref id="B40-sensors-25-01223"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Krizhevsky</surname><given-names>A.</given-names></name>
<name><surname>Sutskever</surname><given-names>I.</given-names></name>
<name><surname>Hinton</surname><given-names>G.E.</given-names></name>
</person-group><article-title>Imagenet classification with deep convolutional neural networks</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2012</year><volume>25</volume><fpage>1097</fpage><lpage>1105</lpage><pub-id pub-id-type="doi">10.1145/3065386</pub-id></element-citation></ref><ref id="B41-sensors-25-01223"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dashtipour</surname><given-names>K.</given-names></name>
<name><surname>Gogate</surname><given-names>M.</given-names></name>
<name><surname>Cambria</surname><given-names>E.</given-names></name>
<name><surname>Hussain</surname><given-names>A.</given-names></name>
</person-group><article-title>A novel context-aware multimodal framework for persian sentiment analysis</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2103.02636</pub-id><pub-id pub-id-type="doi">10.1016/j.neucom.2021.02.020</pub-id></element-citation></ref><ref id="B42-sensors-25-01223"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sagum</surname><given-names>R.A.</given-names></name>
</person-group><article-title>An Application of Emotion Detection in Sentiment Analysis on Movie Reviews</article-title><source>Turk. J. Comput. Math. Educ. (TURCOMAT)</source><year>2021</year><volume>12</volume><fpage>5468</fpage><lpage>5474</lpage><pub-id pub-id-type="doi">10.17762/turcomat.v12i3.2204</pub-id></element-citation></ref><ref id="B43-sensors-25-01223"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rustam</surname><given-names>F.</given-names></name>
<name><surname>Khalid</surname><given-names>M.</given-names></name>
<name><surname>Aslam</surname><given-names>W.</given-names></name>
<name><surname>Rupapara</surname><given-names>V.</given-names></name>
<name><surname>Mehmood</surname><given-names>A.</given-names></name>
<name><surname>Choi</surname><given-names>G.S.</given-names></name>
</person-group><article-title>A performance comparison of supervised machine learning models for Covid-19 tweets sentiment analysis</article-title><source>PLoS ONE</source><year>2021</year><volume>16</volume><elocation-id>e0245909</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0245909</pub-id><pub-id pub-id-type="pmid">33630869</pub-id>
</element-citation></ref><ref id="B44-sensors-25-01223"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Z.x.</given-names></name>
<name><surname>Zhang</surname><given-names>D.g.</given-names></name>
<name><surname>Luo</surname><given-names>G.z.</given-names></name>
<name><surname>Lian</surname><given-names>M.</given-names></name>
<name><surname>Liu</surname><given-names>B.</given-names></name>
</person-group><article-title>A new method of emotional analysis based on CNN&#x02013;BiLSTM hybrid neural network</article-title><source>Clust. Comput.</source><year>2020</year><volume>23</volume><fpage>2901</fpage><lpage>2913</lpage><pub-id pub-id-type="doi">10.1007/s10586-020-03055-9</pub-id></element-citation></ref><ref id="B45-sensors-25-01223"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ridouan</surname><given-names>A.</given-names></name>
<name><surname>Bohi</surname><given-names>A.</given-names></name>
<name><surname>Mourchid</surname><given-names>Y.</given-names></name>
</person-group><article-title>Improving Pain Classification using Spatio-Temporal Deep Learning Approaches with Facial Expressions</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="arxiv">2501.06787</pub-id></element-citation></ref><ref id="B46-sensors-25-01223"><label>46.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>X.</given-names></name>
<name><surname>Ramanan</surname><given-names>D.</given-names></name>
</person-group><article-title>Face detection, pose estimation, and landmark localization in the wild</article-title><source>Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Providence, RI, USA</conf-loc><conf-date>16&#x02013;21 June 2012</conf-date><fpage>2879</fpage><lpage>2886</lpage></element-citation></ref><ref id="B47-sensors-25-01223"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hossain</surname><given-names>S.</given-names></name>
<name><surname>Umer</surname><given-names>S.</given-names></name>
<name><surname>Rout</surname><given-names>R.K.</given-names></name>
<name><surname>Tanveer</surname><given-names>M.</given-names></name>
</person-group><article-title>Fine-grained image analysis for facial expression recognition using deep convolutional neural networks with bilinear pooling</article-title><source>Appl. Soft Comput.</source><year>2023</year><volume>134</volume><fpage>109997</fpage><pub-id pub-id-type="doi">10.1016/j.asoc.2023.109997</pub-id></element-citation></ref><ref id="B48-sensors-25-01223"><label>48.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Szegedy</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>W.</given-names></name>
<name><surname>Jia</surname><given-names>Y.</given-names></name>
<name><surname>Sermanet</surname><given-names>P.</given-names></name>
<name><surname>Reed</surname><given-names>S.</given-names></name>
<name><surname>Anguelov</surname><given-names>D.</given-names></name>
<name><surname>Erhan</surname><given-names>D.</given-names></name>
<name><surname>Vanhoucke</surname><given-names>V.</given-names></name>
<name><surname>Rabinovich</surname><given-names>A.</given-names></name>
</person-group><article-title>Going deeper with convolutions</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#x02013;12 June 2015</conf-date><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="B49-sensors-25-01223"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Umer</surname><given-names>S.</given-names></name>
<name><surname>Rout</surname><given-names>R.K.</given-names></name>
<name><surname>Pero</surname><given-names>C.</given-names></name>
<name><surname>Nappi</surname><given-names>M.</given-names></name>
</person-group><article-title>Facial expression recognition with trade-offs between data augmentation and deep learning features</article-title><source>J. Ambient. Intell. Humaniz. Comput.</source><year>2021</year><volume>13</volume><fpage>721</fpage><lpage>735</lpage><pub-id pub-id-type="doi">10.1007/s12652-020-02845-8</pub-id></element-citation></ref><ref id="B50-sensors-25-01223"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Saxena</surname><given-names>A.</given-names></name>
</person-group><article-title>Convolutional neural networks: An illustration in TensorFlow</article-title><source>XRDS: Crossroads, ACM Mag. Stud.</source><year>2016</year><volume>22</volume><fpage>56</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1145/2951024</pub-id></element-citation></ref><ref id="B51-sensors-25-01223"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lawrence</surname><given-names>S.</given-names></name>
<name><surname>Giles</surname><given-names>C.L.</given-names></name>
<name><surname>Tsoi</surname><given-names>A.C.</given-names></name>
<name><surname>Back</surname><given-names>A.D.</given-names></name>
</person-group><article-title>Face recognition: A convolutional neural-network approach</article-title><source>IEEE Trans. Neural Netw.</source><year>1997</year><volume>8</volume><fpage>98</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1109/72.554195</pub-id><pub-id pub-id-type="pmid">18255614</pub-id>
</element-citation></ref><ref id="B52-sensors-25-01223"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Martin</surname><given-names>E.</given-names></name>
<name><surname>d&#x02019;Autume</surname><given-names>M.d.M.</given-names></name>
<name><surname>Varray</surname><given-names>C.</given-names></name>
</person-group><article-title>Audio Denoising Algorithm with Block Thresholding</article-title><source>Image Process.</source><year>2012</year><volume>21</volume><fpage>105</fpage><lpage>1232</lpage></element-citation></ref><ref id="B53-sensors-25-01223"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fu</surname><given-names>Z.</given-names></name>
<name><surname>Lu</surname><given-names>G.</given-names></name>
<name><surname>Ting</surname><given-names>K.M.</given-names></name>
<name><surname>Zhang</surname><given-names>D.</given-names></name>
</person-group><article-title>A survey of audio-based music classification and annotation</article-title><source>IEEE Trans. Multimed.</source><year>2010</year><volume>13</volume><fpage>303</fpage><lpage>319</lpage><pub-id pub-id-type="doi">10.1109/TMM.2010.2098858</pub-id></element-citation></ref><ref id="B54-sensors-25-01223"><label>54.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Logan</surname><given-names>B.</given-names></name>
</person-group><article-title>Mel Frequency Cepstral Coefficients for Music Modeling</article-title><source>ISMIR</source><publisher-name>ISMIR</publisher-name><publisher-loc>Plymouth, MA, USA</publisher-loc><year>2000</year><volume>Volume 270</volume><fpage>11</fpage></element-citation></ref><ref id="B55-sensors-25-01223"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>C.H.</given-names></name>
<name><surname>Shih</surname><given-names>J.L.</given-names></name>
<name><surname>Yu</surname><given-names>K.M.</given-names></name>
<name><surname>Lin</surname><given-names>H.S.</given-names></name>
</person-group><article-title>Automatic music genre classification based on modulation spectral analysis of spectral and cepstral features</article-title><source>IEEE Trans. Multimed.</source><year>2009</year><volume>11</volume><fpage>670</fpage><lpage>682</lpage></element-citation></ref><ref id="B56-sensors-25-01223"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nawab</surname><given-names>S.</given-names></name>
<name><surname>Quatieri</surname><given-names>T.</given-names></name>
<name><surname>Lim</surname><given-names>J.</given-names></name>
</person-group><article-title>Signal reconstruction from short-time Fourier transform magnitude</article-title><source>IEEE Trans. Acoust. Speech Signal Process.</source><year>1983</year><volume>31</volume><fpage>986</fpage><lpage>998</lpage><pub-id pub-id-type="doi">10.1109/TASSP.1983.1164162</pub-id></element-citation></ref><ref id="B57-sensors-25-01223"><label>57.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Gulli</surname><given-names>A.</given-names></name>
<name><surname>Pal</surname><given-names>S.</given-names></name>
</person-group><source>Deep Learning with Keras</source><publisher-name>Packt Publishing Ltd.</publisher-name><publisher-loc>Birmingham, UK</publisher-loc><year>2017</year></element-citation></ref><ref id="B58-sensors-25-01223"><label>58.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Bergstra</surname><given-names>J.</given-names></name>
<name><surname>Breuleux</surname><given-names>O.</given-names></name>
<name><surname>Bastien</surname><given-names>F.</given-names></name>
<name><surname>Lamblin</surname><given-names>P.</given-names></name>
<name><surname>Pascanu</surname><given-names>R.</given-names></name>
<name><surname>Desjardins</surname><given-names>G.</given-names></name>
<name><surname>Turian</surname><given-names>J.</given-names></name>
<name><surname>Warde-Farley</surname><given-names>D.</given-names></name>
<name><surname>Bengio</surname><given-names>Y.</given-names></name>
</person-group><article-title>Theano: A CPU and GPU math expression compiler</article-title><source>Proceedings of the Python for Scientific Computing Conference (SciPy)</source><conf-loc>Austin, TX, USA</conf-loc><conf-date>28 June&#x02013;3 July 2010</conf-date><volume>Volume 4</volume><fpage>1</fpage><lpage>7</lpage></element-citation></ref><ref id="B59-sensors-25-01223"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Holz</surname><given-names>N.</given-names></name>
<name><surname>Larrouy-Maestri</surname><given-names>P.</given-names></name>
<name><surname>Poeppel</surname><given-names>D.</given-names></name>
</person-group><article-title>The variably intense vocalizations of affect and emotion (VIVAE) corpus prompts new perspective on nonspeech perception</article-title><source>Emotion</source><year>2022</year><volume>22</volume><fpage>213</fpage><pub-id pub-id-type="doi">10.1037/emo0001048</pub-id><pub-id pub-id-type="pmid">35129996</pub-id>
</element-citation></ref><ref id="B60-sensors-25-01223"><label>60.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lucey</surname><given-names>P.</given-names></name>
<name><surname>Cohn</surname><given-names>J.F.</given-names></name>
<name><surname>Prkachin</surname><given-names>K.M.</given-names></name>
<name><surname>Solomon</surname><given-names>P.E.</given-names></name>
<name><surname>Matthews</surname><given-names>I.</given-names></name>
</person-group><article-title>Painful data: The UNBC-McMaster shoulder pain expression archive database</article-title><source>Proceedings of the 2011 IEEE International Conference on Automatic Face &#x00026; Gesture Recognition (FG)</source><conf-loc>Santa Barbara, CA, USA</conf-loc><conf-date>21&#x02013;25 March 2011</conf-date><fpage>57</fpage><lpage>64</lpage></element-citation></ref><ref id="B61-sensors-25-01223"><label>61.</label><element-citation publication-type="webpage"><article-title>2D Face Dataset with Pain Expression</article-title><comment>Available online: <ext-link xlink:href="http://pics.psych.stir.ac.uk/2D_face_sets.htm" ext-link-type="uri">http://pics.psych.stir.ac.uk/2D_face_sets.htm</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2025-02-06">(accessed on 6 February 2025)</date-in-citation></element-citation></ref><ref id="B62-sensors-25-01223"><label>62.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Walter</surname><given-names>S.</given-names></name>
<name><surname>Gruss</surname><given-names>S.</given-names></name>
<name><surname>Ehleiter</surname><given-names>H.</given-names></name>
<name><surname>Tan</surname><given-names>J.</given-names></name>
<name><surname>Traue</surname><given-names>H.C.</given-names></name>
<name><surname>Werner</surname><given-names>P.</given-names></name>
<name><surname>Al-Hamadi</surname><given-names>A.</given-names></name>
<name><surname>Crawcour</surname><given-names>S.</given-names></name>
<name><surname>Andrade</surname><given-names>A.O.</given-names></name>
<name><surname>da Silva</surname><given-names>G.M.</given-names></name>
</person-group><article-title>The biovid heat pain database data for the advancement and systematic validation of an automated pain recognition system</article-title><source>Proceedings of the 2013 IEEE International Conference on Cybernetics (CYBCO)</source><conf-loc>Lausanne, Switzerland</conf-loc><conf-date>13&#x02013;15 June 2013</conf-date><fpage>128</fpage><lpage>131</lpage></element-citation></ref><ref id="B63-sensors-25-01223"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>P.</given-names></name>
</person-group><article-title>An Argument for Basic Emotions</article-title><source>Cogn. Emot.</source><year>1992</year><volume>6</volume><fpage>169</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1080/02699939208411068</pub-id></element-citation></ref><ref id="B64-sensors-25-01223"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mollahosseini</surname><given-names>A.</given-names></name>
<name><surname>Hasani</surname><given-names>B.</given-names></name>
<name><surname>Mahoor</surname><given-names>M.H.</given-names></name>
</person-group><article-title>Affectnet: A database for facial expression, valence, and arousal computing in the wild</article-title><source>IEEE Trans. Affect. Comput.</source><year>2017</year><volume>10</volume><fpage>18</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2017.2740923</pub-id></element-citation></ref><ref id="B65-sensors-25-01223"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Umer</surname><given-names>S.</given-names></name>
<name><surname>Dhara</surname><given-names>B.C.</given-names></name>
<name><surname>Chanda</surname><given-names>B.</given-names></name>
</person-group><article-title>An iris recognition system based on analysis of textural edgeness descriptors</article-title><source>IETE Tech. Rev.</source><year>2018</year><volume>35</volume><fpage>145</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1080/02564602.2016.1265904</pub-id></element-citation></ref><ref id="B66-sensors-25-01223"><label>66.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Umer</surname><given-names>S.</given-names></name>
<name><surname>Dhara</surname><given-names>B.C.</given-names></name>
<name><surname>Chanda</surname><given-names>B.</given-names></name>
</person-group><article-title>Biometric recognition system for challenging faces</article-title><source>Proceedings of the 2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)</source><conf-loc>Patna, India</conf-loc><conf-date>16&#x02013;19 December 2015</conf-date><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="B67-sensors-25-01223"><label>67.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Szegedy</surname><given-names>C.</given-names></name>
<name><surname>Vanhoucke</surname><given-names>V.</given-names></name>
<name><surname>Ioffe</surname><given-names>S.</given-names></name>
<name><surname>Shlens</surname><given-names>J.</given-names></name>
<name><surname>Wojna</surname><given-names>Z.</given-names></name>
</person-group><article-title>Rethinking the inception architecture for computer vision</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>2818</fpage><lpage>2826</lpage></element-citation></ref><ref id="B68-sensors-25-01223"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>McNeely-White</surname><given-names>D.</given-names></name>
<name><surname>Beveridge</surname><given-names>J.R.</given-names></name>
<name><surname>Draper</surname><given-names>B.A.</given-names></name>
</person-group><article-title>Inception and ResNet features are (almost) equivalent</article-title><source>Cogn. Syst. Res.</source><year>2020</year><volume>59</volume><fpage>312</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1016/j.cogsys.2019.10.004</pub-id></element-citation></ref><ref id="B69-sensors-25-01223"><label>69.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Simonyan</surname><given-names>K.</given-names></name>
<name><surname>Zisserman</surname><given-names>A.</given-names></name>
</person-group><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1409.1556</pub-id></element-citation></ref><ref id="B70-sensors-25-01223"><label>70.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ghosh</surname><given-names>A.</given-names></name>
<name><surname>Umer</surname><given-names>S.</given-names></name>
<name><surname>Khan</surname><given-names>M.K.</given-names></name>
<name><surname>Rout</surname><given-names>R.K.</given-names></name>
<name><surname>Dhara</surname><given-names>B.C.</given-names></name>
</person-group><article-title>Smart sentiment analysis system for pain detection using cutting edge techniques in a smart healthcare framework</article-title><source>Clust. Comput.</source><year>2023</year><volume>26</volume><fpage>119</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1007/s10586-022-03552-z</pub-id><pub-id pub-id-type="pmid">35125934</pub-id>
</element-citation></ref><ref id="B71-sensors-25-01223"><label>71.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Werner</surname><given-names>P.</given-names></name>
<name><surname>Al-Hamadi</surname><given-names>A.</given-names></name>
<name><surname>Limbrecht-Ecklundt</surname><given-names>K.</given-names></name>
<name><surname>Walter</surname><given-names>S.</given-names></name>
<name><surname>Gruss</surname><given-names>S.</given-names></name>
<name><surname>Traue</surname><given-names>H.C.</given-names></name>
</person-group><article-title>Automatic pain assessment with facial activity descriptors</article-title><source>IEEE Trans. Affect. Comput.</source><year>2016</year><volume>8</volume><fpage>286</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2016.2537327</pub-id></element-citation></ref><ref id="B72-sensors-25-01223"><label>72.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Gibbons</surname><given-names>J.D.</given-names></name>
</person-group><source>Nonparametric Statistics: An Introduction</source><publisher-name>Sage</publisher-name><publisher-loc>Thousand Oaks, CA, USA</publisher-loc><year>1993</year><volume>Volume 9</volume></element-citation></ref><ref id="B73-sensors-25-01223"><label>73.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>McFee</surname><given-names>B.</given-names></name>
<name><surname>Raffel</surname><given-names>C.</given-names></name>
<name><surname>Liang</surname><given-names>D.</given-names></name>
<name><surname>Ellis</surname><given-names>D.P.</given-names></name>
<name><surname>McVicar</surname><given-names>M.</given-names></name>
<name><surname>Battenberg</surname><given-names>E.</given-names></name>
<name><surname>Nieto</surname><given-names>O.</given-names></name>
</person-group><article-title>librosa: Audio and music signal analysis in python</article-title><source>Proceedings of the 14th Python in Science Conference (SCIPY 2015)</source><conf-loc>Austin, TX, USA</conf-loc><conf-date>6&#x02013;12 July 2015</conf-date><fpage>18</fpage><lpage>24</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01223-f001"><label>Figure 1</label><caption><p>Pictorial representation of the proposed multimodal pain sentiment analysis system (PSAS) for smart healthcare framework.</p></caption><graphic xlink:href="sensors-25-01223-g001" position="float"/></fig><fig position="float" id="sensors-25-01223-f002"><label>Figure 2</label><caption><p>Detecting facial regions in input images for the image-based PSAS.</p></caption><graphic xlink:href="sensors-25-01223-g002" position="float"/></fig><fig position="float" id="sensors-25-01223-f003"><label>Figure 3</label><caption><p>Demonstration of the <inline-formula><mml:math id="mm162" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> architecture for image-based PSAS.</p></caption><graphic xlink:href="sensors-25-01223-g003" position="float"/></fig><fig position="float" id="sensors-25-01223-f004"><label>Figure 4</label><caption><p>Illustration of the <inline-formula><mml:math id="mm163" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> architecture.</p></caption><graphic xlink:href="sensors-25-01223-g004" position="float"/></fig><fig position="float" id="sensors-25-01223-f005"><label>Figure 5</label><caption><p>Executed <inline-formula><mml:math id="mm164" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> framework.</p></caption><graphic xlink:href="sensors-25-01223-g005" position="float"/></fig><fig position="float" id="sensors-25-01223-f006"><label>Figure 6</label><caption><p>Examples of some image samples from UNBC-McMaster [<xref rid="B60-sensors-25-01223" ref-type="bibr">60</xref>] database.</p></caption><graphic xlink:href="sensors-25-01223-g006" position="float"/></fig><fig position="float" id="sensors-25-01223-f007"><label>Figure 7</label><caption><p>Examples of some image samples from <inline-formula><mml:math id="mm165" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B61-sensors-25-01223" ref-type="bibr">61</xref>] database.</p></caption><graphic xlink:href="sensors-25-01223-g007" position="float"/></fig><fig position="float" id="sensors-25-01223-f008"><label>Figure 8</label><caption><p>Samples of some image specimens from BioVid Heat Pain Database [<xref rid="B62-sensors-25-01223" ref-type="bibr">62</xref>].</p></caption><graphic xlink:href="sensors-25-01223-g008" position="float"/></fig><fig position="float" id="sensors-25-01223-f012"><label>Figure 12</label><caption><p>The performance outcome of the proposed pain SAS using audio features with (<bold>a</bold>) 50&#x02013;50% training&#x02013;testing, and (<bold>b</bold>) 75&#x02013;25% training&#x02013;testing sets.</p></caption><graphic xlink:href="sensors-25-01223-g012" position="float"/></fig><fig position="float" id="sensors-25-01223-f013"><label>Figure 13</label><caption><p>Performance of the proposed pain sentiment analysis system using the performance reported in <xref rid="sensors-25-01223-t011" ref-type="table">Table 11</xref> and <xref rid="sensors-25-01223-f012" ref-type="fig">Figure 12</xref>.</p></caption><graphic xlink:href="sensors-25-01223-g013" position="float"/></fig><fig position="float" id="sensors-25-01223-f014"><label>Figure 14</label><caption><p>Performance of the proposed multimodal pain SAS (<inline-formula><mml:math id="mm168" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) using 2-class <inline-formula><mml:math id="mm169" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and VIVAE databases.</p></caption><graphic xlink:href="sensors-25-01223-g014" position="float"/></fig><fig position="float" id="sensors-25-01223-f015"><label>Figure 15</label><caption><p>Performance of the proposed multimodal pain SAS (<inline-formula><mml:math id="mm170" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) using 3-Class UNBC-McMaster and VIVAE databases.</p></caption><graphic xlink:href="sensors-25-01223-g015" position="float"/></fig><fig position="float" id="sensors-25-01223-f016"><label>Figure 16</label><caption><p>Performance of the proposed multimodal pain SAS (<inline-formula><mml:math id="mm171" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) using 4-class BioVid and VIVAE databases.</p></caption><graphic xlink:href="sensors-25-01223-g016" position="float"/></fig><table-wrap position="float" id="sensors-25-01223-t001"><object-id pub-id-type="pii">sensors-25-01223-t001_Table 1</object-id><label>Table 1</label><caption><p>List of abbreviations employed in this paper.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Term</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Abbreviation</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2D Face Set Database with Pain Expression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2DFPE</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolutional neural network</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Covertly aggressive</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CAG</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fast Fourier transformation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FFT</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Internet Of Things</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IoT</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mel-frequency cepstral coefficients</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MFCCs</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multimodal pain sentiment analysis system</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MPSAS</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-aggressive</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NAG</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Overtly aggressive</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OAG</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pain sentiment analysis system</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PSAS</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tree-structured part model</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TSPM</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">University Of Northern British Columbia-McMaster<break/>
Shoulder Pain Expression Archive Database</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UNBC-McMaster</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Variably Intense Vocalizations of Affect And Emotion</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VIVAE</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01223-t002"><object-id pub-id-type="pii">sensors-25-01223-t002_Table 2</object-id><label>Table 2</label><caption><p>Detailed description of the <inline-formula><mml:math id="mm172" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> architecture, including layer-wise parameters, where <inline-formula><mml:math id="mm173" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm174" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm175" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm176" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm177" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm178" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm179" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>7</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <italic toggle="yes">P</italic> be the number of pain-classes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Layers</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Outputshape</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Layers</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Outputshape</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters</th></tr></thead><tbody><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block1 (Input image: (128, 128, 3))</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t, t, 30)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">840</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t4, t4, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">129,720</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t1, t1, 30)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t5, t5, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t1, t1, 30)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">120</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t5, t5, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">480</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t1, t1, 30)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t5, t5, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t1, t1, 30)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t5, t5, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block2</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t1, t1, 60)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16,260</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t5, t5, 240)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">259,440</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t2, t2, 60)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t6, t6, 240)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t2, t2, 60)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">240</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t6, t6, 240)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">960</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t2, t2, 60)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t6, t6, 240)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t2, t2, 60)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t6, t6, 240)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block3</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t2, t2, 90)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48,690</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t6, t6, 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">110,6432</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t3, t3, 90)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t7, t7, 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t3, t3, 90)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">360</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t7, t7, 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2048</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t3, t3, 90)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t7, t7, 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t3, t3, 90)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t7, t7, 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block4</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Flatten</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t3, t3, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97,320</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dense</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#x000d7; (t7 &#x000d7; t7 &#x000d7; 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">262,656</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t4, t4, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#x000d7; (t7 &#x000d7; t7 &#x000d7; 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2048</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t4, t4, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">480</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#x000d7; (t7 &#x000d7; t7 &#x000d7; 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t4, t4, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#x000d7; (t7 &#x000d7; t7 &#x000d7; 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t4, t4, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dense</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#x000d7; P (=5)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2565</td></tr><tr><td colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Total Parameters for The Input Image Size</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1,930,659</td></tr><tr><td colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Total Number of Trainable Parameters:</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1,927,291</td></tr><tr><td colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Non-trainable params:</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3368</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01223-t003"><object-id pub-id-type="pii">sensors-25-01223-t003_Table 3</object-id><label>Table 3</label><caption><p>Detailed description of the <inline-formula><mml:math id="mm180" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> architecture, including layer-wise parameters, where <inline-formula><mml:math id="mm181" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm182" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm183" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm184" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm185" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm186" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm187" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>7</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <italic toggle="yes">P</italic> be the number of pain-classes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Layers</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Outputshape</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Layers</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Outputshape</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters</th></tr></thead><tbody><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block1 (Input image: (192, 192, 3))</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t, t, 30)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">840</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t4, t4, 240)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">259,440</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t1, t1, 30)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t5, t5, 240)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t1, t1, 30)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">120</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t5, t5, 240)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">960</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t1, t1, 30)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t5, t5, 240)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t1, t1, 30)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t5, t5, 240)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block2</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t1, t1, 60)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16,260</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t5, t5, 480)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1,037,280</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t2, t2, 60)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t6, t6, 480)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t2, t2, 60)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">240</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t6, t6, 480)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1920</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t2, t2, 60)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t6, t6, 480)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t2, t2, 60)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t6, t6, 480)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block3</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t2, t2, 90)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">48,690</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t6, t6, 1024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4,424,704</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t3, t3, 90)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t7, t7, 1024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t3, t3, 90)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">360</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t7, t7, 1024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4096</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t3, t3, 90)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t7, t7, 1024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t3, t3, 90)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t7, t7, 1024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Block4</td><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Flatten</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Convolution (3 &#x000d7; 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t3, t3, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97,320</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dense</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#x000d7; (t7 &#x000d7; t7 &#x000d7; 1024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1,049,600</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maxpool (2 &#x000d7; 2)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t4, t4, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#x000d7; (t7 &#x000d7; t7 &#x000d7; 1024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4096</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatchNormalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t4, t4, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">480</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#x000d7; (t7 &#x000d7; t7 &#x000d7; 1024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Activation (ReLU)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t4, t4, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#x000d7; (t7 &#x000d7; t7 &#x000d7; 1024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(t4, t4, 120)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dense</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#x000d7; P (=5)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5125</td></tr><tr><td colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Total Parameters for The Input Image Size</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6,951,531</td></tr><tr><td colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Total Number of Trainable Parameters:</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6,945,395</td></tr><tr><td colspan="5" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Non-trainable params:</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6136</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01223-t004"><object-id pub-id-type="pii">sensors-25-01223-t004_Table 4</object-id><label>Table 4</label><caption><p>List of parameters required for the audio-based deep learning technique.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Layer</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Output Shape</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Feature Size</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Parameters</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Flatten</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, <italic toggle="yes">d</italic>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1,d)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dense</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1 + d) &#x000d7; 512</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatNorm</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2048</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ActRelu</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 512)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dense</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 256)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 256)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1 + 512) &#x000d7; 256 = 131,328</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatNorm</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 256)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 256)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1024</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ActRelu</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 256)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 256)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 256)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 256)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dense</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 128)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1,128)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1 + 256) &#x000d7; 128 = 32,896</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BatNorm</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 128)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1,128)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">512</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ActRelu</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 128</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 128)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 128)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 128)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dense</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(1, 3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(128 + 1) &#x000d7; 3 = 387</td></tr><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Total No. of Parameters</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">168,195 + ((1 + d) &#x000d7; 512)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01223-t005"><object-id pub-id-type="pii">sensors-25-01223-t005_Table 5</object-id><label>Table 5</label><caption><p>The description of employed databases concerning image samples for various pain intensity classes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Class</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Samples</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Demography</th></tr></thead><tbody><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">2DFPE Database (2 classes of pain level)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No Pain (<inline-formula><mml:math id="mm192" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">298</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">13 female and 10 male participants,<break/>
without mention, belongs to Scotland, UK</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pain (<inline-formula><mml:math id="mm193" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">298</td></tr><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">UNBC Database (2 classes of pain level)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No Pain (<inline-formula><mml:math id="mm194" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">Q</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40,029</td><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Same as for&#x000a0;<break/>
UNBC Database (3 classes of pain level)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pain (<inline-formula><mml:math id="mm195" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">Q</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8369</td></tr><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">UNBC Database (3 classes of pain level)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No (Zero Intensity) Pain (NP) (<inline-formula><mml:math id="mm196" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40,029</td><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">12 male and 13 female participants,<break/>
with ages ranging from 18 to 65 years,<break/>
from Ontario, Canada, regions</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low-Intensity Pain (LP) (<inline-formula><mml:math id="mm197" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2909</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High-Intensity Pain (HP) (<inline-formula><mml:math id="mm198" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">R</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5460</td></tr><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">BioVid Heat Pain dataset (5 classes of pain level)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No Pain (<inline-formula><mml:math id="mm199" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6900</td><td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">30 male and 30 female participants,<break/>
healthy adult individuals aged 18 and above,<break/>
diverse age range, Western,<break/>
European, and&#x000a0;German regions</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pain Amount 1 (<inline-formula><mml:math id="mm200" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6900</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pain Amount 2 (<inline-formula><mml:math id="mm201" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">S</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6900</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pain Amount 3 (<inline-formula><mml:math id="mm202" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">S</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6900</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pain Amount 4 (<inline-formula><mml:math id="mm203" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">S</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6900</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01223-t010"><object-id pub-id-type="pii">sensors-25-01223-t010_Table 10</object-id><label>Table 10</label><caption><p>A quantitative comparison (in Acc. (%)) of the proposed method with other competing methods for the image-based PSAS experiment is presented, maintaining the hypotheses <inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm154" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:math></inline-formula> denotes the mean performance, <inline-formula><mml:math id="mm155" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula> represents the standard deviation of performance, and <inline-formula><mml:math id="mm156" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the number of tests.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td colspan="7" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">3-Class UNBC-McMaster</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Set-1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Set-2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm157" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm158" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">t-statistic, <italic toggle="yes">p</italic>-value</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x000a0;Remarks</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed (A)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.1123</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">H0: Null hypothesis; H1: Alternative hypothesis</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anay et al. [<xref rid="B70-sensors-25-01223" ref-type="bibr">70</xref>] (B)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1942, <inline-formula><mml:math id="mm159555" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.8639</mml:mn><mml:mo>&#x0003e;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is accepted,&#x000a0;<inline-formula><mml:math id="mm165550" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HoG (C)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.74</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.31, <inline-formula><mml:math id="mm161555" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0092</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm5555162" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Inception-v3 (D)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.31</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.39</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.54, <inline-formula><mml:math id="mm165553" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0171</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm165554" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LBP (E)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.36</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">62.72</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.5674, <inline-formula><mml:math id="mm165555" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0351</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm15566" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lucey et al. [<xref rid="B60-sensors-25-01223" ref-type="bibr">60</xref>] (F)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.54, <inline-formula><mml:math id="mm155567" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0171</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm155568" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet50 (G)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.72</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.23, <inline-formula><mml:math id="mm169555" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0257</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm170555" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG16 (H)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.97, <inline-formula><mml:math id="mm171555" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0288</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm175552" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Werner et al. [<xref rid="B71-sensors-25-01223" ref-type="bibr">71</xref>] (I)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.64, <inline-formula><mml:math id="mm175553" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0216</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm175554" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td colspan="7" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">2-Class UNBC-McMaster</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Set-1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Set-2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm195551" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm155592" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">t-statistic, <italic toggle="yes">p</italic>-value</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x000a0;Remarks</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed (A)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.1123</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">H0: Null hypothesis; H1: Alternative hypothesis</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anay et al. [<xref rid="B70-sensors-25-01223" ref-type="bibr">70</xref>] (B)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.4314, <inline-formula><mml:math id="mm175555" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0371</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm175556" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HoG (C)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.31, <inline-formula><mml:math id="mm175557" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0092</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm1755558" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Inception-v3 (D)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.32</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.40, <inline-formula><mml:math id="mm175559" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0088</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm155580" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LBP (E)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.70</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.39, <inline-formula><mml:math id="mm1855551" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0178</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm182555" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lucey et al. [<xref rid="B60-sensors-25-01223" ref-type="bibr">60</xref>] (F)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.55, <inline-formula><mml:math id="mm1855553" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0154</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm1855554" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet50 (G)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.31</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.68</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.73, <inline-formula><mml:math id="mm185555" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0081</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm185556" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG16 (H)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.88, <inline-formula><mml:math id="mm5555187" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0301</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm185558" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Werner et al. [<xref rid="B71-sensors-25-01223" ref-type="bibr">71</xref>] (I)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.24, <inline-formula><mml:math id="mm1855559" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0024</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm190444" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td colspan="7" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">2-Class 2DFPE</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Set-1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Set-2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm1555591" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm5555192" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">t-statistic, <italic toggle="yes">p</italic>-value</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x000a0;Remarks</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed (A)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81.91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.12</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">H0: Null hypothesis; H1: Alternative hypothesis</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anay et al. [<xref rid="B70-sensors-25-01223" ref-type="bibr">70</xref>] (B)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.08</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.68, <inline-formula><mml:math id="mm195553" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.1174</mml:mn><mml:mo>&#x0003e;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is accepted,&#x000a0;<inline-formula><mml:math id="mm555194" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HoG (C)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.61</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.95, <inline-formula><mml:math id="mm195555" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0489</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm155596" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Inception-v3 (D)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.07</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.70</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.52, <inline-formula><mml:math id="mm155597" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0113</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm198555" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LBP (E)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.52, <inline-formula><mml:math id="mm195559" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0227</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm205550" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lucey et al. [<xref rid="B60-sensors-25-01223" ref-type="bibr">60</xref>] (F)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.58</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.80</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.29, <inline-formula><mml:math id="mm205551" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0743</mml:mn><mml:mo>&#x0003e;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is accepted,&#x000a0;<inline-formula><mml:math id="mm205552" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet50 (G)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.39</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.30, <inline-formula><mml:math id="mm2555503" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0121</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm2045555" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG16 (H)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.98</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.33</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.65, <inline-formula><mml:math id="mm2055555" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0109</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm2065555" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Werner et al. [<xref rid="B71-sensors-25-01223" ref-type="bibr">71</xref>] (I)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.69</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.73, <inline-formula><mml:math id="mm205557" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0324</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm205558" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td colspan="7" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">5-Class BioVid</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Method</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Set-1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Set-2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm255509" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm555210" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">t-statistic, <italic toggle="yes">p</italic>-value</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x000a0;Remarks</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed (A)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.42</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.62</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">H0: Null hypothesis; H1: Alternative hypothesis</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Anay et al. [<xref rid="B70-sensors-25-01223" ref-type="bibr">70</xref>] (B)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.96</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.4437, <inline-formula><mml:math id="mm2555511" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.00005</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm2555512" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HoG (C)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.62</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17.7968, <inline-formula><mml:math id="mm2555513" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.00002</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm5555214" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Inception-v3 (D)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.15, <inline-formula><mml:math id="mm215555" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0028</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm2155556" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LBP (E)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.31</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.58, <inline-formula><mml:math id="mm2555517" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0001</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm255518" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lucey et al. [<xref rid="B60-sensors-25-01223" ref-type="bibr">60</xref>] (F)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.78</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.64, <inline-formula><mml:math id="mm255519" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0007</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm555220" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet50 (G)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.28</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.26</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.049, <inline-formula><mml:math id="mm225551" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0008</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm255522" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG16 (H)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.64</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.59</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.03, <inline-formula><mml:math id="mm2555523" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0010</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm224" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Werner et al. [<xref rid="B71-sensors-25-01223" ref-type="bibr">71</xref>] (I)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.22, <inline-formula><mml:math id="mm225" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0.0024</mml:mn><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">H0 is rejected,&#x000a0;<inline-formula><mml:math id="mm226" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01223-t011"><object-id pub-id-type="pii">sensors-25-01223-t011_Table 11</object-id><label>Table 11</label><caption><p>The performance outcome of the proposed pain sentiment analysis system based on audio data using various machine learning classifiers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="4" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">50&#x02013;50% Training&#x02013;Testing Set</th><th colspan="4" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">75&#x02013;25% Training&#x02013;Testing Set</th></tr><tr><th colspan="9" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Statistical Features</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classifier</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Logistic Regression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.43</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">K-Nearest Neighbors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.22</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Decision Tree</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.47</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.43</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Support Vector Machine</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.43</td></tr><tr><td colspan="9" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">MFCC features</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Logistic Regression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.34</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.07</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.74</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">K-Nearest Neighbors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.66</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.22</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.39</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Decision Tree</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.52</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.38</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.57</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.81</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.08</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Support Vector Machine</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.63</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.48</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.72</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.69</td></tr><tr><td colspan="9" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Spectra features</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Logistic Regression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.77</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">K-Nearest Neighbors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.17</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.53</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.77</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Decision Tree</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.41</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.49</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.19</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Support Vector Machine</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.31</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.37</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.77</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01223-t012"><object-id pub-id-type="pii">sensors-25-01223-t012_Table 12</object-id><label>Table 12</label><caption><p>Computational complexity in <italic toggle="yes">sec.</italic> of the proposed pain detection systems.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Data Type</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Image Classification</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Audio Classification</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN Models</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm229" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm230" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm231" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Trainable Parameters</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1,947,431</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6,654,119</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">281,603</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-Trainable Parameters</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2944</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4736</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1792</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total Parameters</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1,944,487</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6,649,383</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">283,395</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.04</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.06</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.43</td></tr></tbody></table></table-wrap></floats-group></article>