<!--

File produced by pipelineRunner package (for JATS 2 SCJATS with pipeline SCJATS)
At: 2025-05-22T15:36:20.862Z

Version        : 1.16.1
Last update    : 2024-08-27
Modified by    : dunnm

-->
<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Soc Cogn Affect Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Soc Cogn Affect Neurosci</journal-id><journal-id journal-id-type="publisher-id">scan</journal-id><journal-title-group><journal-title>Social Cognitive and Affective Neuroscience</journal-title></journal-title-group><issn pub-type="ppub">1749-5016</issn><issn pub-type="epub">1749-5024</issn><publisher><publisher-name>Oxford University Press</publisher-name><publisher-loc>UK</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40036617</article-id><article-id pub-id-type="pmc">PMC11891439</article-id>
<article-id pub-id-type="doi">10.1093/scan/nsaf020</article-id><article-id pub-id-type="publisher-id">nsaf020</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Research &#x02013; Neuroscience</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI01880</subject></subj-group></article-categories><title-group><article-title>The contribution of body perception to self-identity: an event-related potential study</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lu</surname><given-names>Juanzhi</given-names></name><aff>
<institution content-type="department">Department of Cognitive Neuroscience, Faculty of Psychology and Neuroscience, Maastricht University</institution>, Maastricht, Limburg 6200 MD, <country country="NL">The Netherlands</country></aff><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation" degree-contribution="equal">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis" degree-contribution="equal">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="equal">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="equal">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration" degree-contribution="equal">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software" degree-contribution="equal">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization" degree-contribution="equal">Visualization</role></contrib><contrib contrib-type="author"><name><surname>Riecke</surname><given-names>Lars</given-names></name><aff>
<institution content-type="department">Department of Cognitive Neuroscience, Faculty of Psychology and Neuroscience, Maastricht University</institution>, Maastricht, Limburg 6200 MD, <country country="NL">The Netherlands</country></aff><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="equal">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision" degree-contribution="equal">Supervision</role></contrib><contrib contrib-type="author"><name><surname>Ryan</surname><given-names>Brenda E</given-names></name><aff>
<institution>Institut d&#x02019;Investigacions Biom&#x000e8;diques August Pi i Sunyer (IDIBAPS)</institution>, Carrer de Rosello 149, Barcelona 08036, <country country="ES">Spain</country></aff><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="equal">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources" degree-contribution="equal">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software" degree-contribution="equal">Software</role></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3373-8661</contrib-id><name><surname>de Gelder</surname><given-names>Beatrice</given-names></name><!--b.degelder@maastrichtuniversity.nl--><aff>
<institution content-type="department">Department of Cognitive Neuroscience, Faculty of Psychology and Neuroscience, Maastricht University</institution>, Maastricht, Limburg 6200 MD, <country country="NL">The Netherlands</country></aff><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition" degree-contribution="equal">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="equal">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision" degree-contribution="equal">Supervision</role><xref rid="COR0001" ref-type="corresp"/></contrib></contrib-group><author-notes><corresp id="COR0001">*Corresponding author. Department of Cognitive Neuroscience, Faculty of Psychology and Neuroscience, Maastricht University, P.O. Box 616, Maastricht 6200 MD, The Netherlands. E-mail: <email xlink:href="b.degelder@maastrichtuniversity.nl">b.degelder@maastrichtuniversity.nl</email></corresp></author-notes><pub-date pub-type="collection"><year>2025</year></pub-date><pub-date pub-type="epub" iso-8601-date="2025-02-27"><day>27</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>27</day><month>2</month><year>2025</year></pub-date><volume>20</volume><issue>1</issue><elocation-id>nsaf020</elocation-id><history><date date-type="received"><day>14</day><month>5</month><year>2024</year></date><date date-type="rev-recd"><day>05</day><month>11</month><year>2024</year></date><date date-type="accepted"><day>25</day><month>2</month><year>2025</year></date><date date-type="editorial-decision"><day>14</day><month>12</month><year>2024</year></date><date date-type="corrected-typeset"><day>10</day><month>3</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025. Published by Oxford University Press.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="nsaf020.pdf"/><abstract><title>Abstract</title><p>This study used electroencephalography (EEG) and personalized avatars to investigate the neural mechanisms underlying personal identity perception. Compound avatar images combining participants&#x02019; own faces and bodies, as well as those of others, were generated from photographs. Participants underwent an embodiment training for each avatar type in a virtual reality environment, where they controlled the avatar&#x02019;s actions during physical exercise tasks. Subjective assessments by participants confirmed a stronger identification with avatars representing their own identity compared to those representing others. Analysis of event-related potentials (ERPs) evoked by viewing the avatar revealed that avatars representing the participants&#x02019; self-identity elicited weaker N2 and P1 responses compared to avatars representing other identities. No significant effects on N170 responses were observed. Control conditions utilizing avatars with modified body characteristics confirmed that the reduction in N2 amplitude was specifically related to identity perception rather than variations in visual body size. These findings suggest that the perception of self-identity occurs rapidly, within &#x0223c;200&#x02009;ms, indicating the integration of visual face and body information into identity representation at an early stage.</p></abstract><kwd-group><kwd>body</kwd><kwd>face</kwd><kwd>self-identity</kwd><kwd>event-related potential</kwd><kwd>N2</kwd></kwd-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>ERC Synergy grant</institution></institution-wrap>
</funding-source><award-id>856495; Relevance</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Horizon 2020 Programme H2020-FETPROACT-2020-2</institution></institution-wrap>
</funding-source><award-id>101017884; GuestXR</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>China Scholarship Council</institution><institution-id institution-id-type="DOI">10.13039/501100004543</institution-id></institution-wrap>
</funding-source><award-id>CSC202008440538</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Research and Innovation Program H2020-EU.1.3.1</institution></institution-wrap>
</funding-source><award-id>721385; Socrates</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Horizon-CL4-2021-Human-01-21</institution></institution-wrap>
</funding-source><award-id>101070278; Re-Silence</award-id></award-group></funding-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>ERC Synergy grant</institution></institution-wrap>
</funding-source><award-id>856495; Relevance</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Horizon 2020 Programme H2020-FETPROACT-2020-2</institution></institution-wrap>
</funding-source><award-id>101017884; GuestXR</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>China Scholarship Council</institution><institution-id institution-id-type="DOI">10.13039/501100004543</institution-id></institution-wrap>
</funding-source><award-id>CSC202008440538</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Research and Innovation Program H2020-EU.1.3.1</institution></institution-wrap>
</funding-source><award-id>721385; Socrates</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Horizon-CL4-2021-Human-01-21</institution></institution-wrap>
</funding-source><award-id>101070278; Re-Silence</award-id></award-group></funding-group><counts><page-count count="8"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Understanding and processing someone&#x02019;s identity is fundamental to our social interactions. We effortlessly recognize familiar individuals and distinguish between acquaintances and strangers. But we care just as much about our own identity. We have a unique ability to recognize our own physical appearance, often scrutinizing our own facial and bodily features when faced with a mirror. Previous research on self-perception has primarily focused on facial stimuli to investigate the behavioral and neural processes involved in identity perception and recognition (<xref rid="R25" ref-type="bibr">Sui et&#x000a0;al. 2009</xref>, <xref rid="R13" ref-type="bibr">Maister et&#x000a0;al. 2013</xref>, <xref rid="R6" ref-type="bibr">Gonzalez-Franco et&#x000a0;al. 2016</xref>). The significance of self-face recognition has long been debated and recent meta-analyses confirm that individuals identify their own face more quickly than others&#x02019; faces (<xref rid="R3" ref-type="bibr">Bortolon and Raffard 2018</xref>).</p><p>EEG studies exploring the neural basis of self-facial identity perception have identified that the N2 component, an event-related negativity occurring at &#x0223c;200&#x02009;ms, is associated with self-identity and self-relevance (<xref rid="R26" ref-type="bibr">Tanaka et&#x000a0;al. 2006</xref>, <xref rid="R21" ref-type="bibr">Pierce et&#x000a0;al. 2011</xref>, <xref rid="R29" ref-type="bibr">Wo&#x0017a;niak et&#x000a0;al. 2018</xref>, <xref rid="R17" ref-type="bibr">Mu&#x000f1;oz et&#x000a0;al. 2020</xref>, <xref rid="R2" ref-type="bibr">Bola et&#x000a0;al. 2021</xref>). <xref rid="R29" ref-type="bibr">Wo&#x0017a;niak et&#x000a0;al. (2018)</xref> found that self-associated faces elicit smaller N2 amplitudes than faces associated with others, such as friends or strangers, indicating an early neural process for self-face perception. Consistent with this, <xref rid="R2" ref-type="bibr">Bola et&#x000a0;al. (2021)</xref> demonstrated that self-face stimuli, but not other-face stimuli, elicit the N2pc component within 200&#x02009;ms. <xref rid="R17" ref-type="bibr">Mu&#x000f1;oz et&#x000a0;al. (2020)</xref> found that personal objects elicited lower N2 amplitudes compared to nonpersonal objects, further supporting the role of the N2 in self-relevance processing. These findings collectively suggest that the attenuation of N2 associated with self-identity from self-relevant stimuli, such as one&#x02019;s own face, occurs rapidly within &#x0223c;200&#x02009;ms.</p><p>Much less is known about self-perception based on the body and the combined perception of the self-face and body. The importance of body perception for self-perception is illustrated by clinical studies showing that body perception-based self-awareness can sometimes lead to dissatisfaction with our physical appearance, contributing to conditions like eating disorders and body dissatisfaction, as evidenced by clinical studies (<xref rid="R9" ref-type="bibr">Karazsia et&#x000a0;al. 2017</xref>). But despite its importance, the neural mechanisms underlying self-body perception remain poorly understood. To date, only a few studies have delved into the perception of self-identity from bodies or used personalized body images. In a functional magnetic resonance imaging study, researchers utilized full-body photos of participants clad in dark bathing suits and found that photos conveying self-identity triggered responses in the extrastriate, parietal regions, and middle frontal gyrus in the right hemisphere (<xref rid="R7" ref-type="bibr">Hodzic et&#x000a0;al. 2009</xref>). An event-related potential (ERP) study employed participant photos, presenting naturalistic, enlarged, and reduced body images to investigate the perception of self-body versus other-body under various attention tasks (<xref rid="R28" ref-type="bibr">Uusberg et&#x000a0;al. 2018</xref>). Their findings indicate that the distinction between self-body and other-body influences the N170 component, renowned for its role in the initial stages of face or body perception (<xref rid="R23" ref-type="bibr">Stekelenburg and de Gelder 2004</xref>). However, our comprehension of the neural dynamics underlying the perception of self and other identities based on body images remains limited.</p><p>A related inquiry concerns the relative significance of the face and body. Based on the literature where studies on self-facial identity largely outnumber those on self-body identity, one might assume that face identity drives self-recognition, with the body playing only a minor role. If so, the presence of the personalized body would not make a difference to self-identity perception. However, studies on the perception of emotional expressions have shown that body expression can influence how facial emotion is processed already in the early processing stages as seen in the EEG on the P1 (<xref rid="R15" ref-type="bibr">Meeren et&#x000a0;al. 2005</xref>). Thus, it is conceivable that face and body information are integrated in early neural processes to form a self-identity percept.</p><p>To test this hypothesis in the current study, we explored the neural basis of self-identity perception using avatars synthesized from personalized face and body images. To enhance the participants&#x02019; sense of body ownership of the avatar, we devised an embodiment task employing a virtual reality (VR) environment for physical exercise. Participants faced a virtual mirror displaying their avatar and engaged in physical activities (e.g. moving their arms to catch bubbles and reaching for crystals). This VR session aimed to bolster the participants&#x02019; identification with the avatar and their sense of body ownership. During the experiment proper, participants viewed various categories of avatars and performed an oddball task, while EEG was recorded. EEG provides exceptional temporal precision, offering millisecond-level resolution, which is ideal for investigating the temporal dynamics of self-identity processing. Building upon previous findings with personalized face images and bodies, we anticipated that the perception of self-identity based on avatars with their own face and their own body would modulate N2 and N170 responses compared to the perception of other-identity avatars. As previous studies have linked P1 to sensory input such as image size (<xref rid="R4" ref-type="bibr">Busch et&#x000a0;al. 2004</xref>, <xref rid="R20" ref-type="bibr">Pfabigan et&#x000a0;al. 2015</xref>), we also included avatars with altered body sizes and analyzed P1 responses to control for visual image size differences between self and other avatars.</p></sec><sec id="s2"><title>Materials and methods</title><sec id="s2-s1"><title>Participants</title><p>Twenty-nine healthy participants [aged 18&#x02013;24&#x02009;years, mean&#x02009;=&#x02009;19.9; standard deviation (SD)&#x02009;=&#x02009;1.7; one left-handed] were recruited from the female student population at Maastricht University. All participants had a normal body mass index (BMI&#x02009;=&#x02009;18.5&#x02013;24.9), normal or corrected-to-normal vision, and no history of brain injury, psychiatric disorders, or current use of psychotropic medication. Before the experiment, participants provided written consent. They received compensation of 7.5 Euros or one credit point per hour for their participation. The Ethics Committee of Maastricht University approved the study, and all procedures adhered to the principles outlined in the Declaration of Helsinki.</p></sec><sec id="s2-s2"><title>Experimental design and stimuli</title><p>Personalized avatars for each participant were created from frontal photographs of the participant showing a full frontal view of body and face with a neutral posture and expression. These images served as the basis for generating individualized avatar body trunks, closely matching the participant&#x02019;s body shape, and avatar faces that accurately replicated the participant&#x02019;s facial features. We refer to these components as &#x0201c;self&#x0201d; or &#x0201c;own.&#x0201d; Additionally, we generated avatar body trunks and avatar faces from images of averaged Dutch body and face profiles. We refer to these components as &#x0201c;other.&#x0201d; We digitally manipulated all body images to generate additional, heavier-weighted versions of avatars with big body trunks.</p><p>To elicit a full self-identity percept, we combined the participants&#x02019; own avatar face with the participants&#x02019; own avatar body. Similarly, to elicit an other-identity percept, we combined the other avatar face with the other, big body trunk avatar. Examples of these two main avatars are illustrated in <xref rid="F1" ref-type="fig">Fig.&#x000a0;1a</xref>. As these avatars differed in both (putative) identity percept and body-trunk size, we included two altered-body conditions that served to assess potentially confounding body-size effects in the absence of strong identity differences. These control avatars combined the participants&#x02019; own avatar face with the participants&#x02019; big body trunk avatar (<xref rid="F1" ref-type="fig">Fig.&#x000a0;1b</xref>, left) and the other avatar face with the other, medium body trunk avatar (<xref rid="F1" ref-type="fig">Fig.&#x000a0;1b</xref>, right). All avatars were generated using the Character Creator 3 software.</p><fig position="float" id="F1" fig-type="figure"><label>Figure&#x000a0;1.</label><caption><p>Experimental design. Four example avatars are shown (one per condition). (a) It depicts the main conditions, featuring the participant&#x02019;s self-identity avatar (composed of the participant&#x02019;s own face and own body) on the left, and the other-identity avatar (composed of others&#x02019; face and body) on the right. These avatars were expected to elicit strong self-identity percepts and a clear other-identity percept, respectively. (b) It depicts the control conditions, featuring an avatar composed of the participant&#x02019;s own face with a big body and another avatar composed of other-face with a medium body. The latter avatars were expected to elicit body-size effects in the absence of strong identity differences. Avatar orientation and shirt color were pseudorandomly varied within each condition. This figure is used with permission of the participant resembled in the self-identity condition images. (c) It depicts the experimental protocol. The order of runs was randomized across participants.</p></caption><graphic xlink:href="nsaf020f1" position="float"/></fig><p>In order to increase the naturalness and variability of the avatar images during the EEG recordings, we pseudorandomly varied the color of the avatar&#x02019;s T-shirt (blue, purple, or green) and the avatar&#x02019;s orientation (frontal, 90&#x000b0; left, or 90&#x000b0; right), resulting in nine unique avatar images for each condition. The avatar was presented on a transparent background (size: 700&#x02009;&#x000d7;&#x02009;1000 pixels) covering &#x0223c;3.2&#x000b0;&#x02009;&#x000d7;&#x02009;4.5&#x000b0; of participants&#x02019; visual angle in the experiment. In order to make participants focus on the whole body, we centered a white fixation cross onto the avatar&#x02019;s body near the waist (<xref rid="F1" ref-type="fig">Fig.&#x000a0;1</xref>).</p></sec><sec id="s2-s3"><title>Experimental procedure</title><p>Before admitting participants to the EEG study, their ability to recognize and distinguish their own avatar image from other avatar images was assessed with an online pre-questionnaire. When participants passed this initial screening, they were invited to the lab, where the experimenters introduced the experiment, prepared the EEG (see the &#x0201c;EEG acquisition&#x0201d; section), and seated them in a comfortable chair. Participants underwent four runs of the experiment lasting in total &#x0223c;1&#x02009;h. Each run consisted of three parts that directly followed each other: VR task, EEG task, and post-questionnaire. In the VR task, participants performed a behavioral task in a VR environment while being embodied in an avatar corresponding to one of the four conditions. The purpose of this task was to create embodiment in the current avatar condition. In the subsequent EEG task, participants performed a behavioral task on 2D images of the same avatar as in the VR task; the images were shown on a computer screen, while EEG was recorded. In the subsequent post-questionnaire, participants answered three questions related to the VR and avatar. The order of runs (i.e. conditions) was randomized across participants. Details about pre-questionnaire, VR task, EEG task, and post-questionnaire are described in the next subsections.</p><sec id="s2-s3-s1"><title>Pre-questionnaire</title><p>To assess whether participants could recognize their personalized self-identity avatar and distinguish it from other avatars, we presented them with images of the self-identity avatar and images of the control avatar composed of other-face with medium body. We asked participants to rate whether the avatar looked similar to themselves (&#x0201c;similarity&#x0201d;) and whether they could recognize the avatar (&#x0201c;recognizability&#x0201d;) on a seven-point Likert scale (1: not agree at all, 7: strongly agree) before the experiment (<xref rid="T1" ref-type="table">Table&#x000a0;1</xref>, light shading).</p><table-wrap position="float" id="T1"><label>Table&#x000a0;1.</label><caption><p>Pre-questionnaire and post-questionnaire.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Questionnaire</th><th align="left" rowspan="1" colspan="1">Aim</th><th valign="bottom" align="left" rowspan="1" colspan="1">Question</th><th valign="bottom" align="left" rowspan="1" colspan="1">Rating</th></tr></thead><tbody><tr><td rowspan="2" align="left" colspan="1">Pre-questionnaire</td><td align="left" rowspan="1" colspan="1">Similarity</td><td align="left" rowspan="1" colspan="1">The avatar looks like me.</td><td align="left" rowspan="1" colspan="1">1 (do not agree at all) to 7 (strongly agree)</td></tr><tr><td align="left" rowspan="1" colspan="1">Recognizability</td><td align="left" rowspan="1" colspan="1">I can recognize myself in this avatar.</td><td align="left" rowspan="1" colspan="1">1 (do not agree at all) to 7 (strongly agree)</td></tr><tr><td rowspan="3" align="left" colspan="1">Post-questionnaire</td><td align="left" rowspan="1" colspan="1">Embodiment</td><td align="left" rowspan="1" colspan="1">I felt that the virtual body I saw when I was in VR was my own body.</td><td align="left" rowspan="1" colspan="1">1 (do not agree at all) to 7 (strongly agree)</td></tr><tr><td align="left" rowspan="1" colspan="1">Identification</td><td align="left" rowspan="1" colspan="1">The body I saw physically looked like me.</td><td align="left" rowspan="1" colspan="1">1 (do not agree at all) to 7 (strongly agree)</td></tr><tr><td align="left" rowspan="1" colspan="1">Liking</td><td align="left" rowspan="1" colspan="1">How did you feel about the avatar?</td><td align="left" rowspan="1" colspan="1">1 (do not like it at all) to 7 (I like it very much)</td></tr></tbody></table></table-wrap></sec><sec id="s2-s3-s2"><title>VR embodiment task</title><p>The VR scenario was programmed in Unity Software in collaboration with Virtual Bodyworks Company (Barcelona, Spain). Participants stood upright wearing a VR headset (Oculus Quest 2). In the VR environment, they viewed a given avatar (depending on the experimental condition) in a mirror from a first-person perspective and performed a target catching task. This task required them to manually catch bubbles and reach crystals through different circles in the VR by executing corresponding arm movements in the physical world.</p></sec><sec id="s2-s3-s3"><title>EEG task</title><p>In the EEG experiment, each trial consisted of the presentation of an avatar image for 1000&#x02009;ms, preceded by a fixation cross for 500&#x02009;&#x000b1;&#x02009;100&#x02009;ms. To make participants focus on the avatar, an oddball task was used. This task required them to press a button when they detected a deviant avatar, which consisted of different physical features (hair style, skin color, and facial features). The deviant avatars were also presented in nine variations (3 shirt colors&#x02009;&#x000d7;&#x02009;3 orientations) and they never occurred on two consecutive trials. We instructed participants to press the button when they saw the deviant avatar, and we emphasized response accuracy (we did not encourage response speed; therefore, we did not analyze reaction time below). In each run, each standard image was repeated 42 times and each deviant image was repeated 4 times, resulting in 378 standard trials and 36 deviant trials per run (i.e. condition). Each EEG run lasted &#x0223c;10&#x02009;min.</p></sec><sec id="s2-s3-s4"><title>Post-questionnaire</title><p>To assess the participants&#x02019; subjective feelings about their embodiment, avatar identification, and avatar liking (<xref rid="R14" ref-type="bibr">Marchiondo et&#x000a0;al. 2015</xref>, <xref rid="R16" ref-type="bibr">Mello et&#x000a0;al. 2022</xref>), we asked them three questions after each EEG run. These questions are described later (<xref rid="T1" ref-type="table">Table&#x000a0;1</xref>, dark shading).</p></sec></sec><sec id="s2-s4"><title>EEG acquisition</title><p>EEG data were recorded using an elastic cap with 64 electrodes placed according to the international 10-20 system and sampled at a rate of 1000&#x02009;Hz (BrainVison Products, Munich, Germany). Electrode Cz was used as the reference during recording, and the forehead electrode (Fp1) was used as a ground electrode. Two electrodes were used to measure the vertical and horizontal electrooculogram. The remaining 58 electrodes included FPz, AFz, Fz, FCz, CPz, Pz, POz, Oz, AF7, AF8, AF3, AF4, F7, F8, F5, F6, F3, F4, F1, F2, FC5, FC6, FC3, FC4, FC1, FC2, T7, T8, C5, C6, C3, C4, C1, C2, TP9, TP10, TP7, TP8, CP5, CP6, CP3, CP4, CP1, CP2, P7, P8, P5, P6, P3, P4, P1, P2, PO7, PO8, PO3, PO4, O1, and O2. Impedances for reference and ground were maintained &#x0003c;5&#x02009;kOhm and for all other electrodes &#x0003c;10&#x02009;kOhm.</p></sec><sec id="s2-s5"><title>EEG data preprocessing</title><p>EEG data were preprocessed and analyzed using FieldTrip version 20220104 (<xref rid="R19" ref-type="bibr">Oostenveld et&#x000a0;al. 2011</xref>) in Matlab R2021b (MathWorks, USA). Recordings were first segmented into epochs from 500&#x02009;ms pre-stimulus (the avatar image) to 1500&#x02009;ms post-stimulus and then filtered with a 0.3&#x02013;30-Hz band-pass filter. EEG data at each electrode were re-referenced to the average of all electrodes. Artifact rejection was done using independent component analysis (logistic infomax ICA algorithm, <xref rid="R1" ref-type="bibr">Bell and Sejnowski 1995</xref>); on average, 1.56&#x02009;&#x000b1;&#x02009;0.70 (mean&#x02009;&#x000b1;&#x02009;SD) components were visually identified as artifacts and removed per participant. Moreover, single epochs during which the EEG peak amplitude exceeded 3 SD above/below the mean amplitude were rejected. On average, 270.03&#x02009;&#x000b1;&#x02009;39.02 trials were preserved and statistically analyzed per participant.</p></sec><sec id="s2-s6"><title>ERP analyses</title><p>Epochs from 200&#x02009;ms before until 1000&#x02009;ms after stimulus onset were extracted from the preprocessed data. Baseline correction was applied and involved subtracting the average amplitude in the baseline interval (&#x02212;200 to 0&#x02009;ms) from the overall epoch. Trials were averaged for each experimental condition, resulting in ERPs used for further statistical analyses, which were performed using IBM SPSS Statistics 27 (IBM Corp., Armonk, NY, USA). Only trials comprising standard stimuli were analyzed. We spatially separated the EEG electrodes into an occipital cluster (POz, Oz, PO3, PO4, O1, and O2), temporal cluster (P7, P8, TP7, TP8, CP5, CP6, P5, and P6), and frontal cluster (F1, F2, Fz, FC1, FC2, and FCz) and averaged the channels within each cluster. For each cluster, we pooled all conditions and visually identified a prominent ERP component based on visual inspection of the overall ERP waveform, topographical distribution of grand-averaged ERP, and previous studies (<xref rid="R30" ref-type="bibr">Xu et&#x000a0;al. 2017</xref>, <xref rid="R17" ref-type="bibr">Mu&#x000f1;oz et&#x000a0;al. 2020</xref>, <xref rid="R12" ref-type="bibr">Lu et&#x000a0;al. 2023</xref>). The identified ERP components and their associated time windows are as follows: P1 (80&#x02013;130&#x02009;ms) in the occipital cluster; N170 (140&#x02013;190&#x02009;ms) in the temporal cluster; and N2 (200&#x02013;250&#x02009;ms) in the frontal cluster. The mean amplitude was computed as the average of all electrodes within each cluster within the specific time window.</p><p>Outliers (data points further than 3 SD away from the mean) were rejected; this affected one participant&#x02019;s dataset in the analyses of N170. All other datasets were normally distributed as assessed with the Shapiro&#x02013;Wilk test. Paired <italic toggle="yes">t</italic>-tests were applied to compare the mean ERP amplitudes (or differences between these amplitudes) across conditions. Statistical results were considered as significant given a <italic toggle="yes">P</italic>-value of &#x0003c;.05.</p></sec></sec><sec id="s3"><title>Results</title><sec id="s3-s1"><title>Pre-questionnaire results</title><p>Rating scores for similarity and recognizability were significantly larger for the self-identity avatar than the control avatar, which was composed of other-face with medium body (similarity: self-identity avatar: 5.45&#x02009;&#x000b1;&#x02009;1.40, altered-body avatar: 2.72&#x02009;&#x000b1;&#x02009;1.46, <italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;&#x02212;10.21, <italic toggle="yes">P&#x02009;</italic>&#x0003c;&#x02009;.001; recognizability: self-identity avatar: 5.86&#x02009;&#x000b1;&#x02009;1.27, altered-body avatar: 2.59&#x02009;&#x000b1;&#x02009;1.48, <italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;&#x02212;9.96, <italic toggle="yes">P&#x02009;</italic>&#x0003c;&#x02009;.001). These observations indicate that participants could successfully identify and discriminate their personalized avatar.</p></sec><sec id="s3-s2"><title>Post-questionnaire results</title><p>The rating scores of embodiment showed a significant difference between self-identity and other-identity conditions (<italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;13.15, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001). Specifically, the embodiment scores for self-identity (5.03&#x02009;&#x000b1;&#x02009;1.37) were substantially higher than those for other-identity (1.79&#x02009;&#x000b1;&#x02009;0.98). Additionally, a significant difference was observed between the control conditions (<italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;3.58, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.001), with higher embodiment scores for other-faces with medium bodies (3.13&#x02009;&#x000b1;&#x02009;1.55) than self-faces with big bodies (2.44&#x02009;&#x000b1;&#x02009;1.32). Notably, the difference between main conditions was significantly larger than that between control conditions [<italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;7.23, <italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, difference between main conditions (self-identity minus other-identity): 3.24&#x02009;&#x000b1;&#x02009;1.33, difference between control conditions (other-faces with medium bodies minus self-faces with big bodies): 0.69&#x02009;&#x000b1;&#x02009;1.04]. Applying the same analyses to identification scores and liking scores yielded results qualitatively identical to those mentioned earlier, except that the identity had no significant effect on identification scores in the control conditions. Overall, the subjective rating data indicate that participants identified more strongly with the self-identity avatar than the other-identity avatar (<xref rid="F2" ref-type="fig">Fig.&#x000a0;2</xref>).</p><fig position="float" id="F2" fig-type="figure"><label>Figure&#x000a0;2.</label><caption><p>Post-questionnaire results. Means and standard error (SE) of rating scores for embodiment (left), identification (middle), and liking (right) per condition. ***<italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.001, **<italic toggle="yes">P</italic> &#x0003c;0.01, *<italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.05, n.s.: non-significant.</p></caption><graphic xlink:href="nsaf020f2" position="float"/></fig></sec><sec id="s3-s3"><title>Event-related potentials</title><sec id="s3-s3-s1"><title>Effect of self- versus other-identity perception</title><p>To investigate the temporal dynamics of brain activity during the perception of self-identity versus other-identity, we compared ERPs elicited by viewing participants&#x02019; own avatar (self-identity condition) versus another avatar (other-identity condition). <xref rid="F3" ref-type="fig">Figure&#x000a0;3a</xref> shows that the self-identity condition (4.40&#x02009;&#x000b1;&#x02009;2.97&#x02009;&#x000b5;V) elicited smaller P1 amplitudes than the other-identity condition (5.34&#x02009;&#x000b1;&#x02009;3.36&#x02009;&#x000b5;V). This observation was confirmed by a significant effect of identity on P1 amplitude (<italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;&#x02212;2.09, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.046). In line with our hypothesis, we found a similar effect of identity on N2 (<italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;2.82, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.009; self-identity: &#x02212;0.46&#x02009;&#x000b1;&#x02009;1.54&#x02009;&#x000b5;V, other-identity: &#x02212;1.08&#x02009;&#x000b1;&#x02009;1.59&#x02009;&#x000b5;V). We found no effect on N170 (<italic toggle="yes">t</italic>(27)&#x02009;=&#x02009; &#x02212;1.20, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.240).</p><fig position="float" id="F3" fig-type="figure"><label>Figure&#x000a0;3.</label><caption><p>(a&#x02013;c) EEG results. Grand averaged ERPs are depicted per condition for P1, N170, and N2 components separately (middle). The shaded rectangle visualizes the time window from which the average ERP amplitude was extracted. The highlighted white dots on the topographic map (left) represent the electrodes from which the grand-averaged ERP for each component was extracted. Bar plots (right) illustrate the mean and standard error (SE) across participants of each component&#x02019;s amplitude per condition. **<italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.01, *<italic toggle="yes">P</italic>&#x02009;&#x0003c;&#x02009;.05, n.s.: non-significant.</p></caption><graphic xlink:href="nsaf020f3" position="float"/></fig></sec><sec id="s3-s3-s2"><title>Effect of identity perception versus effect of body size</title><p>To exclude that the observed effect on P1 and N2 reflected merely visual body differences (rather than identity, i.e. the integration of the own face with the own body), we compared the other-face, medium body avatar versus the self-face, big body avatar. Compared to the main conditions, these control conditions significantly reduced overall identity differences (see the &#x0201c;Post-questionnaire results&#x0201d; section) while preserving differences in physical body size. Unlike the main results above, the comparison of these control conditions yielded no effect on P1 or N2 (P1: <italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;&#x02212;1.45, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.158; N2: <italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;&#x02212;0.24, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.815), but on N170 (<italic toggle="yes">t</italic>(27)&#x02009;=&#x02009;2.67, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.013; other-face with medium body: &#x02212;1.17&#x02009;&#x000b1;&#x02009;1.46 &#x000b5;, self-face with big body: &#x02212;1.38&#x02009;&#x000b1;&#x02009;1.46&#x02009;&#x000b5;V), suggesting that the main results above do not reflect merely visual body differences.</p><p>To confirm this notion, we statistically compared the observed effect of identity perception versus the observed (null) effect of body size. We found a significant difference only for the N2 component, such that the difference between the main conditions was larger than the difference between the control conditions (<italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;2.55, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.017; self-identity minus other-identity: 0.62&#x02009;&#x000b1;&#x02009;1.19&#x02009;&#x000b5;V; other-face with medium body minus self-face with big body: &#x02212;0.03&#x02009;&#x000b1;&#x02009;0.72&#x02009;&#x000b5;V). This result suggested that the effect on N2 can be attributed to identity perception, rather than body size. We found no significant difference for P1 (<italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;&#x02212;1.64, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.113), suggesting that the effect on P1 cannot be attributed to identity perception alone, but also to body size.</p></sec><sec id="s3-s3-s3"><title>Comparison of self-identity condition versus control conditions</title><p>We further explored differences between the self-identity condition and the control conditions, for which participants gave reduced identification ratings as stated earlier (see the &#x0201c;Post-questionnaire results&#x0201d; section). Consistent with the identification-rating results, we found that the self-identity condition elicited smaller N2 amplitudes than the self-face with big body (<italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;2.13, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.042) and the other-face with medium body (<italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;2.04, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.051). Applying the same analyses to P1 yielded a significant difference to the self-face with big body (<italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;&#x02212;2.13, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.042), but not to the other-face with medium body (<italic toggle="yes">t</italic>(28)&#x02009;=&#x02009;&#x02212;1.73, <italic toggle="yes">P</italic>&#x02009;=&#x02009;.095), further supporting the above notion that P1 was modulated by body rather than identity. Overall, the N2 responses and identification ratings showed a strikingly similar pattern across conditions (<xref rid="F3" ref-type="fig">Fig.&#x000a0;3c</xref> versus <xref rid="F2" ref-type="fig">Fig.&#x000a0;2</xref>, middle). These observations further support the notion that N2 was modulated by identity perception, whereas P1 was affected more by visual body size.</p></sec></sec></sec><sec id="s4"><title>Discussion</title><p>We investigated neural responses to self-identity, other-identity, and altered-body avatar images. In line with our hypothesis on the importance of body identity, we found a clear effect of self-identity based on the face and the body on the N2 component: avatars with whom participants could identify more strongly elicited significantly smaller N2 amplitudes than avatars with whom participants identified less. Importantly, this suppressive effect on N2 was caused by self-identity as defined &#x0201c;jointly by the face and the body&#x0201d;: self-face avatars elicited smaller N2 amplitudes only when the avatar&#x02019;s body also resembled the participant. In contrast, the earlier P1 component showed an effect of visual body size in the main condition (and a corresponding trend in the control condition): avatars with bigger bodies elicited larger P1 amplitudes than avatars with smaller bodies. We further observed that self-face avatars elicited stronger N170 amplitudes than other-face avatars; however, like the P1 results, this observation showed no systematic link to self-identity.</p><sec id="s4-s1"><title>Effect of face and body on subjective ratings of identity, embodiment, and liking</title><p>Behavioral results showed that participants identified more strongly with avatars that contained both their own face and body compared to avatars whose faces and bodies were not generated from themselves. Avatars with a face and/or a body that did not resemble the participant received significantly lower identification ratings. Surprisingly, the self-face avatar with the big body was rated similarly as the other-face avatar with the medium body. This suggests that the presence of facial identity alone is insufficient for triggering self-perception, regardless of body self-identity. Instead, self-identity seems to be perceived based on a combination of the body and the face. In line with this, it has been shown that a task-irrelevant body expression can influence face identity (<xref rid="R24" ref-type="bibr">Stock and Gelder 2014</xref>). Our finding contrasts with the traditional emphasis on the role of the face for identity perception. A review of previous studies about combined face&#x02013;body perception for person identity found that observers tend to attend selectively to the face and ignore the body, unless the facial information is insufficient to provide identity information (<xref rid="R8" ref-type="bibr">Hu et&#x000a0;al. 2020</xref>). But previous studies did not use personalized avatar images that the participants had become acquainted with through VR embodiment. Still, we cannot exclude that the instructions to fixate on the center of the image (which was in facet on the body) may have contributed to a reduction in the impact of the face component.</p><p>Unlike the identification ratings, participants&#x02019; ratings of embodiment and liking showed a significant difference between the control conditions. Participants embodied and liked more the other-face avatar with the medium body than the self-face avatar with the big body. The main conditions showed a similar effect, with higher ratings for the self-face, self-body avatar than the other-face avatar with the big body. Together, this shows that participants embodied and liked more the medium (more self-like) body than the bigger body, regardless of face identity. Face identity alone may not be not as important for embodiment and liking as one may have expected.</p><p>Overall, these subjective rating data indicate that embodiment and liking seem to be driven primarily by the perception of the body, whereas identification may be driven more by a combination of face and body perception. However, the three measures turned out to yield overall very similar results, suggesting that they measure partially the same construct.</p></sec><sec id="s4-s2"><title>Effect of self-identity on N2</title><p>Our main finding is that the perception of self-identity based on the face and the body reduces N2 responses. In the main conditions, we observed that self-face, self-body avatars elicited smaller N2 amplitudes than other-face, big-body avatars. In contrast, in the control conditions, we found no significant N2 difference between self-face, big-body avatars and other-face, medium-body avatars. Moreover, N2 responses showed a significantly larger difference between the two main conditions than between the two control conditions. These observations rule out that the effect observed in the main conditions reflects body-size differences and instead support a genuine effect of self-identity perception. The latter is further supported by our observation that the N2 responses followed a pattern across conditions that closely resembled participants&#x02019; identification ratings. This strongly suggests that both measures reflect the same phenomenon.</p><p>Our finding that self-identity perception reduces the N2 amplitude is consistent with results from previous studies that showed N2 reduction related to self-relatedness, familiarity, and ownership (<xref rid="R5" ref-type="bibr">Carver et&#x000a0;al. 2006</xref>, <xref rid="R29" ref-type="bibr">Wo&#x0017a;niak et&#x000a0;al. 2018</xref>, <xref rid="R17" ref-type="bibr">Mu&#x000f1;oz et&#x000a0;al. 2020</xref>, <xref rid="R11" ref-type="bibr">Kotlewska et&#x000a0;al. 2023</xref>). Noteworthy, the reduction of N2 has been attributed to a rather automatic process (<xref rid="R27" ref-type="bibr">Todd et&#x000a0;al. 2008</xref>), suggesting that self-identity perception as reflected by N2 suppression might emerge with relatively little cognitive effort.</p><p>We further observed that self-identity avatars elicited smaller N2 amplitudes than avatars combining the self-face with a big body or the other-face with a medium body. This indicates that the N2 reduction is sustained by the identity of not only the face but also the body. Previous studies reported that self-faces elicit smaller N2 amplitudes than other-faces including famous faces (<xref rid="R11" ref-type="bibr">Kotlewska et&#x000a0;al. 2023</xref>), indicating that N2 is sensitive to self-identity. While consistent with this interpretation, our results further reveal that when a body is present, its identity &#x0201c;needs to fit that of the face.&#x0201d; Thus, our findings extend the previous N2 findings by showing that the previously observed suppressive, self-face related effect on N2 is largely attenuated when the self-face is combined with a big body. This further suggests that the N2 suppression does not reflect the self-face alone, but the self-identity as defined by the combination of face and body.</p><p>It should be noted that the observed N2 reduction might reflect a process that is more general than self-perception alone. <xref rid="R17" ref-type="bibr">Mu&#x000f1;oz et&#x000a0;al. (2020)</xref> found significantly smaller N2 response to personal objects than nonpersonal objects (<xref rid="R17" ref-type="bibr">Mu&#x000f1;oz et&#x000a0;al. 2020</xref>). Similarly, unfamiliar objects have been found to elicit more negative N2 responses than familiar objects (<xref rid="R5" ref-type="bibr">Carver et&#x000a0;al. 2006</xref>). Therefore, the N2 reduction might reflect more general percepts of personal identity and/or familiarity.</p></sec><sec id="s4-s3"><title>Effect of body size on P1</title><p>We found that the self-identity avatar elicited significantly smaller P1 amplitudes than the other-identity avatar with the big body. A similar trend was observed in the control conditions: other-face avatars with medium-sized body elicited smaller P1 amplitudes than self-face avatars with a big body. Therefore, unlike the N2 results, the effect on P1 reflected visual differences in body size, rather than self-identity. Several previous studies have shown that the P1 component is related to the visual stimulus size (<xref rid="R4" ref-type="bibr">Busch et&#x000a0;al. 2004</xref>, <xref rid="R18" ref-type="bibr">Niu et&#x000a0;al. 2008</xref>, <xref rid="R20" ref-type="bibr">Pfabigan et&#x000a0;al. 2015</xref>, <xref rid="R22" ref-type="bibr">Schindler et&#x000a0;al. 2019</xref>), with larger-sized stimuli eliciting larger P1 amplitudes. Our results confirm these previous findings and further reveal that the size sensitivity of P1 persists even when visual input elicits distinct identity percepts.</p></sec><sec id="s4-s4"><title>Effect of face, but not self-identity, on N170</title><p>In contradiction with our hypothesis, we found no clear effect of self-identity (face and body) on the N170 component. We observed that self-face avatars with big bodies elicited significantly larger N170 amplitudes than other-face avatars with medium bodies. A similar trend was observed in the main conditions: self-face avatars with self-bodies elicited slightly larger N170 amplitudes than other-face avatars with big bodies. Therefore, unlike the N2 results, the effect on N170 reflected differences between self-face and other-face, with little relation to the body or overall identity. This interpretation is in line with previous findings showing larger N170 responses to self-faces versus faces of friends or strangers (<xref rid="R10" ref-type="bibr">Keyes et&#x000a0;al. 2010</xref>).</p><p>In summary, our data highlight the critical role of body expression in perceiving self-identity. Interestingly, participants embodied and liked medium bodies (closer to their own body size) more than bigger bodies, regardless of the face-identity. Participants preferred the medium body over the bigger body even when the former had someone else&#x02019;s face, suggesting that body size plays a significant role in identity perception. These findings have important social and clinical implications, particularly for understanding attitudes toward obesity. Moreover, VR technology could serve as a tool to help individuals with obesity engage in physical exercise or training by allowing them to embody avatars of varying body sizes in realistic, immersive environments.</p><p>Drawing these conclusions from our study requires some cautionary remarks. First, to eliminate potential confounds related to gender matching between participants and the female avatars, we only recruited female participants, implying that our findings may not generalize to males. Second, our sample consisted exclusively of young adults and Caucasian participants, and further research is needed to understand to which extent cultural differences (e.g. among Asian populations) might affect our results. Finally, the individualization of the stimuli came with the drawback that participants&#x02019; bodies could not be systematically matched to the control condition. Although participants&#x02019; bodies had normal BMI, some were bigger, and others were smaller than the Dutch medium one (which also had normal BMI). Consequently, size might have affected the individual results, but a systematic size effect at the group level is unlikely.</p></sec></sec><sec id="s5"><title>Conclusion</title><p>The perception of self-identity (as defined jointly by the face and the body) leads to a reduced amplitude of the N2. This indicates that self-identity perception emerges rapidly in the brain within 200&#x02009;ms, suggesting that at this stage visual face and body information have been integrated into person identity. Conversely, image size and face identity are processed in earlier stages as reflected by increases in P1 and N170 responses, respectively.</p></sec></body><back><ack id="ack1"><title>Acknowledgements</title><p>We thank Jane Chesley for helping with data collection and Virtual Bodyworks company for designing the VR software.</p></ack><sec sec-type="COI-statement" id="s6"><title>Conflict of interest:</title><p>The authors have declared no conflict of interest for this article.</p></sec><sec id="s7"><title>Funding</title><p>This work was supported by the European Research Council (ERC) Synergy grant (grant agreement no. 856495; Relevance), the Horizon 2020 Programme H2020-FETPROACT-2020-2 (grant agreement no. 101017884; GuestXR), the Research and Innovation Program H2020-EU.1.3.1 (grant agreement no. 721385; Socrates), the Horizon-CL4-2021-Human-01-21 (grant agreement no. 101070278; Re-Silence), and China Scholarship Council (no. CSC202008440538).</p></sec><sec sec-type="data-availability" id="s8"><title>Data availability</title><p>The data underlying this article will be shared on reasonable request to the corresponding author.</p></sec><ref-list id="ref1"><title>References</title><ref id="R1"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bell</surname> &#x000a0;<given-names>AJ</given-names></string-name>, <string-name><surname>Sejnowski</surname> &#x000a0;<given-names>TJ</given-names></string-name></person-group>. <article-title>An information-maximization approach to blind separation and blind deconvolution</article-title>. <source><italic toggle="yes">Neural Comput</italic></source> &#x000a0;<year>1995</year>;<volume>7</volume>:<fpage>1129</fpage>&#x02013;<lpage>59</lpage>.<pub-id pub-id-type="pmid">7584893</pub-id>
</mixed-citation></ref><ref id="R2"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bola</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Pa&#x0017a;</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Doradzi&#x00144;ska</surname> &#x000a0;<given-names>&#x00141;</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The self&#x02010;face captures attention without consciousness: evidence from the N2pc ERP component analysis</article-title>. <source><italic toggle="yes">Psychophysiology</italic></source> &#x000a0;<year>2021</year>;<volume>58</volume>:<page-range>e13759</page-range>.</mixed-citation></ref><ref id="R3"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bortolon</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Raffard</surname> &#x000a0;<given-names>S</given-names></string-name></person-group>. <article-title>Self-face advantage over familiar and unfamiliar faces: a three-level meta-analytic approach</article-title>. <source><italic toggle="yes">Psychon Bull Rev</italic></source> &#x000a0;<year>2018</year>;<volume>25</volume>:<fpage>1287</fpage>&#x02013;<lpage>300</lpage>.<pub-id pub-id-type="pmid">29799093</pub-id>
</mixed-citation></ref><ref id="R4"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Busch</surname> &#x000a0;<given-names>NA</given-names></string-name>, <string-name><surname>Debener</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Kranczioch</surname> &#x000a0;<given-names>C</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Size matters: effects of stimulus size, duration and eccentricity on the visual gamma-band response</article-title>. <source><italic toggle="yes">Clin Neurophysiol</italic></source> &#x000a0;<year>2004</year>;<volume>115</volume>:<fpage>1810</fpage>&#x02013;<lpage>20</lpage>.<pub-id pub-id-type="pmid">15261860</pub-id>
</mixed-citation></ref><ref id="R5"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Carver</surname> &#x000a0;<given-names>LJ</given-names></string-name>, <string-name><surname>Meltzoff</surname> &#x000a0;<given-names>AN</given-names></string-name>, <string-name><surname>Dawson</surname> &#x000a0;<given-names>G</given-names></string-name></person-group>. <article-title>Event&#x02010;related potential (ERP) indices of infants&#x02019; recognition of familiar and unfamiliar objects in two and three dimensions</article-title>. <source><italic toggle="yes">Dev Sci</italic></source> &#x000a0;<year>2006</year>;<volume>9</volume>:<fpage>51</fpage>&#x02013;<lpage>62</lpage>.<pub-id pub-id-type="pmid">16445396</pub-id>
</mixed-citation></ref><ref id="R6"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Gonzalez-Franco</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Bellido</surname> &#x000a0;<given-names>AI</given-names></string-name>, <string-name><surname>Blom</surname> &#x000a0;<given-names>KJ</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The neurological traces of look-alike avatars</article-title>. <source><italic toggle="yes">Front Human Neurosci</italic></source> &#x000a0;<year>2016</year>;<volume>10</volume>:<page-range>392</page-range>.</mixed-citation></ref><ref id="R7"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hodzic</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Muckli</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Singer</surname> &#x000a0;<given-names>W</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Cortical responses to self and others</article-title>. <source><italic toggle="yes">Human Brain Mapp</italic></source> &#x000a0;<year>2009</year>;<volume>30</volume>:<fpage>951</fpage>&#x02013;<lpage>62</lpage>.</mixed-citation></ref><ref id="R8"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hu</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Baragchizadeh</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>O&#x02019;Toole</surname> &#x000a0;<given-names>AJ</given-names></string-name></person-group>. <article-title>Integrating faces and bodies: psychological and neural perspectives on whole person perception</article-title>. <source><italic toggle="yes">Neurosci Biobehav Rev</italic></source> &#x000a0;<year>2020</year>;<volume>112</volume>:<fpage>472</fpage>&#x02013;<lpage>86</lpage>.<pub-id pub-id-type="pmid">32088346</pub-id>
</mixed-citation></ref><ref id="R9"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Karazsia</surname> &#x000a0;<given-names>BT</given-names></string-name>, <string-name><surname>Murnen</surname> &#x000a0;<given-names>SK</given-names></string-name>, <string-name><surname>Tylka</surname> &#x000a0;<given-names>TL</given-names></string-name></person-group>. <article-title>Is body dissatisfaction changing across time? A cross-temporal meta-analysis</article-title>. <source><italic toggle="yes">Psychol Bull</italic></source> &#x000a0;<year>2017</year>;<volume>143</volume>:<page-range>293</page-range>.</mixed-citation></ref><ref id="R10"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Keyes</surname> &#x000a0;<given-names>H</given-names></string-name>, <string-name><surname>Brady</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Reilly</surname> &#x000a0;<given-names>RB</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>My face or yours? Event-related potential correlates of self-face processing</article-title>. <source><italic toggle="yes">Brain Cogn</italic></source> &#x000a0;<year>2010</year>;<volume>72</volume>:<fpage>244</fpage>&#x02013;<lpage>54</lpage>.<pub-id pub-id-type="pmid">19854553</pub-id>
</mixed-citation></ref><ref id="R11"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kotlewska</surname> &#x000a0;<given-names>I</given-names></string-name>, <string-name><surname>Panek</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Nowicka</surname> &#x000a0;<given-names>A</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Posterior theta activity reveals an early signal of self-face recognition</article-title>. <source><italic toggle="yes">Sci Rep</italic></source> &#x000a0;<year>2023</year>;<volume>13</volume>:<page-range>13823</page-range>.</mixed-citation></ref><ref id="R12"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lu</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Kemmerer</surname> &#x000a0;<given-names>SK</given-names></string-name>, <string-name><surname>Riecke</surname> &#x000a0;<given-names>L</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Early threat perception is independent of later cognitive and behavioral control. A virtual reality-EEG-ECG study</article-title>. <source><italic toggle="yes">Cereb Cortex</italic></source> &#x000a0;<year>2023</year>;<volume>33</volume>:<fpage>8748</fpage>&#x02013;<lpage>58</lpage>.<pub-id pub-id-type="pmid">37197766</pub-id>
</mixed-citation></ref><ref id="R13"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Maister</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Banissy</surname> &#x000a0;<given-names>MJ</given-names></string-name>, <string-name><surname>Tsakiris</surname> &#x000a0;<given-names>M</given-names></string-name></person-group>. <article-title>Mirror-touch synaesthesia changes representations of self-identity</article-title>. <source><italic toggle="yes">Neuropsychologia</italic></source> &#x000a0;<year>2013</year>;<volume>51</volume>:<fpage>802</fpage>&#x02013;<lpage>08</lpage>.<pub-id pub-id-type="pmid">23391559</pub-id>
</mixed-citation></ref><ref id="R14"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Marchiondo</surname> &#x000a0;<given-names>LA</given-names></string-name>, <string-name><surname>Myers</surname> &#x000a0;<given-names>CG</given-names></string-name>, <string-name><surname>Kopelman</surname> &#x000a0;<given-names>S</given-names></string-name></person-group>. <article-title>The relational nature of leadership identity construction: how and when it influences perceived leadership and decision-making</article-title>. <source><italic toggle="yes">Leadersh Q</italic></source> &#x000a0;<year>2015</year>;<volume>26</volume>:<fpage>892</fpage>&#x02013;<lpage>908</lpage>.</mixed-citation></ref><ref id="R15"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Meeren</surname> &#x000a0;<given-names>HK</given-names></string-name>, <string-name><surname>Van Heijnsbergen</surname> &#x000a0;<given-names>CC</given-names></string-name>, <string-name><surname>De Gelder</surname> &#x000a0;<given-names>B</given-names></string-name></person-group>. <article-title>Rapid perceptual integration of facial expression and emotional body language</article-title>. <source><italic toggle="yes">Proc Natl Acad Sci</italic></source> &#x000a0;<year>2005</year>;<volume>102</volume>:<fpage>16518</fpage>&#x02013;<lpage>23</lpage>.<pub-id pub-id-type="pmid">16260734</pub-id>
</mixed-citation></ref><ref id="R16"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Mello</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Dupont</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Engelen</surname> &#x000a0;<given-names>T</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The influence of body expression, group affiliation and threat proximity on interactions in virtual reality</article-title>. <source><italic toggle="yes">Curr Res Behav Sci</italic></source> &#x000a0;<year>2022</year>;<volume>3</volume>:<page-range>100075</page-range>.</mixed-citation></ref><ref id="R17"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Mu&#x000f1;oz</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Casado</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Hern&#x000e1;ndez-Guti&#x000e9;rrez</surname> &#x000a0;<given-names>D</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Neural dynamics in the processing of personal objects as an index of the brain representation of the self</article-title>. <source><italic toggle="yes">Brain Topogr</italic></source> &#x000a0;<year>2020</year>;<volume>33</volume>:<fpage>86</fpage>&#x02013;<lpage>100</lpage>.<pub-id pub-id-type="pmid">31776831</pub-id>
</mixed-citation></ref><ref id="R18"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Niu</surname> &#x000a0;<given-names>Y-N</given-names></string-name>, <string-name><surname>Wei</surname> &#x000a0;<given-names>J-H</given-names></string-name>, <string-name><surname>Luo</surname> &#x000a0;<given-names>Y-J</given-names></string-name></person-group>. <article-title>Early ERP effects on the scaling of spatial attention in visual search</article-title>. <source><italic toggle="yes">Prog Nat Sci</italic></source> &#x000a0;<year>2008</year>;<volume>18</volume>:<fpage>381</fpage>&#x02013;<lpage>86</lpage>.</mixed-citation></ref><ref id="R19"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Oostenveld</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Fries</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Maris</surname> &#x000a0;<given-names>E</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source><italic toggle="yes">Comput Intell Neurosci</italic></source> &#x000a0;<year>2011</year>;<volume>2011</volume>:<fpage>1</fpage>&#x02013;<lpage>9</lpage>.<pub-id pub-id-type="pmid">21837235</pub-id>
</mixed-citation></ref><ref id="R20"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Pfabigan</surname> &#x000a0;<given-names>DM</given-names></string-name>, <string-name><surname>Sailer</surname> &#x000a0;<given-names>U</given-names></string-name>, <string-name><surname>Lamm</surname> &#x000a0;<given-names>C</given-names></string-name></person-group>. <article-title>Size does matter! Perceptual stimulus properties affect event&#x02010;related potentials during feedback processing</article-title>. <source><italic toggle="yes">Psychophysiology</italic></source> &#x000a0;<year>2015</year>;<volume>52</volume>:<fpage>1238</fpage>&#x02013;<lpage>47</lpage>.<pub-id pub-id-type="pmid">26059201</pub-id>
</mixed-citation></ref><ref id="R21"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Pierce</surname> &#x000a0;<given-names>LJ</given-names></string-name>, <string-name><surname>Scott</surname> &#x000a0;<given-names>LS</given-names></string-name>, <string-name><surname>Boddington</surname> &#x000a0;<given-names>S</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The N250 brain potential to personally familiar and newly learned faces and objects</article-title>. <source><italic toggle="yes">Front Human Neurosci</italic></source> &#x000a0;<year>2011</year>;<volume>5</volume>:<page-range>111</page-range>.</mixed-citation></ref><ref id="R22"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schindler</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Bruchmann</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Bublatzky</surname> &#x000a0;<given-names>F</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Modulation of face- and emotion-selective ERPs by the three most common types of face image manipulations</article-title>. <source><italic toggle="yes">Soc Cogn Affect Neurosci</italic></source> &#x000a0;<year>2019</year>;<volume>14</volume>:<fpage>493</fpage>&#x02013;<lpage>503</lpage>.<pub-id pub-id-type="pmid">30972417</pub-id>
</mixed-citation></ref><ref id="R23"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Stekelenburg</surname> &#x000a0;<given-names>JJ</given-names></string-name>, <string-name><surname>de Gelder</surname> &#x000a0;<given-names>B</given-names></string-name></person-group>. <article-title>The neural correlates of perceiving human bodies: an ERP study on the body-inversion effect</article-title>. <source><italic toggle="yes">NeuroReport</italic></source> &#x000a0;<year>2004</year>;<volume>15</volume>:<fpage>777</fpage>&#x02013;<lpage>80</lpage>.<pub-id pub-id-type="pmid">15073513</pub-id>
</mixed-citation></ref><ref id="R24"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Stock</surname> &#x000a0;<given-names>JVD</given-names></string-name>, <string-name><surname>Gelder</surname> &#x000a0;<given-names>BD</given-names></string-name></person-group>. <article-title>Face identity matching is influenced by emotions conveyed by face and body</article-title>. <source><italic toggle="yes">Front Human Neurosci</italic></source> &#x000a0;<year>2014</year>;<volume>8</volume>:<page-range>53</page-range>.</mixed-citation></ref><ref id="R25"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sui</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Liu</surname> &#x000a0;<given-names>CH</given-names></string-name>, <string-name><surname>Han</surname> &#x000a0;<given-names>S</given-names></string-name></person-group>. <article-title>Cultural difference in neural mechanisms of self-recognition</article-title>. <source><italic toggle="yes">Soc Neurosci</italic></source> &#x000a0;<year>2009</year>;<volume>4</volume>:<fpage>402</fpage>&#x02013;<lpage>11</lpage>.<pub-id pub-id-type="pmid">19739032</pub-id>
</mixed-citation></ref><ref id="R26"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Tanaka</surname> &#x000a0;<given-names>JW</given-names></string-name>, <string-name><surname>Curran</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Porterfield</surname> &#x000a0;<given-names>AL</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Activation of preexisting and acquired face representations: the N250 event-related potential as an index of face familiarity</article-title>. <source><italic toggle="yes">J Cogn Neurosci</italic></source> &#x000a0;<year>2006</year>;<volume>18</volume>:<fpage>1488</fpage>&#x02013;<lpage>97</lpage>.<pub-id pub-id-type="pmid">16989550</pub-id>
</mixed-citation></ref><ref id="R27"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Todd</surname> &#x000a0;<given-names>RM</given-names></string-name>, <string-name><surname>Lewis</surname> &#x000a0;<given-names>MD</given-names></string-name>, <string-name><surname>Meusel</surname> &#x000a0;<given-names>L-A</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The time course of social-emotional processing in early childhood: ERP responses to facial affect and personal familiarity in a Go-Nogo task</article-title>. <source><italic toggle="yes">Neuropsychologia</italic></source> &#x000a0;<year>2008</year>;<volume>46</volume>:<fpage>595</fpage>&#x02013;<lpage>613</lpage>.<pub-id pub-id-type="pmid">18061633</pub-id>
</mixed-citation></ref><ref id="R28"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Uusberg</surname> &#x000a0;<given-names>H</given-names></string-name>, <string-name><surname>Peet</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Uusberg</surname> &#x000a0;<given-names>A</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Attention biases in preoccupation with body image: an ERP study of the role of social comparison and automaticity when processing body size</article-title>. <source><italic toggle="yes">Biol Psychol</italic></source> &#x000a0;<year>2018</year>;<volume>135</volume>:<fpage>136</fpage>&#x02013;<lpage>48</lpage>.<pub-id pub-id-type="pmid">29559352</pub-id>
</mixed-citation></ref><ref id="R29"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wo&#x0017a;niak</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Kourtis</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Knoblich</surname> &#x000a0;<given-names>G</given-names></string-name></person-group>. <article-title>Prioritization of arbitrary faces associated to self: an EEG study</article-title>. <source><italic toggle="yes">PLoS One</italic></source> &#x000a0;<year>2018</year>;<volume>13</volume>:<page-range>e0190679</page-range>.</mixed-citation></ref><ref id="R30"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Xu</surname> &#x000a0;<given-names>Q</given-names></string-name>, <string-name><surname>Yang</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Tan</surname> &#x000a0;<given-names>Q</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Facial expressions in context: electrophysiological correlates of the emotional congruency of facial expressions and background scenes</article-title>. <source><italic toggle="yes">Front Psychol</italic></source> &#x000a0;<year>2017</year>;<volume>8</volume>:<page-range>308212</page-range>.</mixed-citation></ref></ref-list></back></article>