<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Med Inform Decis Mak</journal-id><journal-id journal-id-type="iso-abbrev">BMC Med Inform Decis Mak</journal-id><journal-title-group><journal-title>BMC Medical Informatics and Decision Making</journal-title></journal-title-group><issn pub-type="epub">1472-6947</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11786435</article-id><article-id pub-id-type="publisher-id">2889</article-id><article-id pub-id-type="doi">10.1186/s12911-025-02889-w</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Towards unbiased skin cancer classification using deep feature fusion</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Abdulredah</surname><given-names>Ali Atshan</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Fadhel</surname><given-names>Mohammed A.</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Alzubaidi</surname><given-names>Laith</given-names></name><address><email>l.alzubaidi@qut.edu.au</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Duan</surname><given-names>Ye</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Kherallah</surname><given-names>Monji</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><name><surname>Charfi</surname><given-names>Faiza</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04d4sd432</institution-id><institution-id institution-id-type="GRID">grid.412124.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2323 5644</institution-id><institution>National School of Electronics and Telecoms of Sfax, </institution><institution>University of Sfax, </institution></institution-wrap>Sfax, Tunisia </aff><aff id="Aff2"><label>2</label>College of Computer Science and Information Technology, University of Sumer, Thi-Qar, Iraq </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03pnv4752</institution-id><institution-id institution-id-type="GRID">grid.1024.7</institution-id><institution-id institution-id-type="ISNI">0000 0000 8915 0953</institution-id><institution>School of Mechanical, Medical, and Process Engineering, </institution><institution>Queensland University of Technology, </institution></institution-wrap>Brisbane, Australia </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/037s24f05</institution-id><institution-id institution-id-type="GRID">grid.26090.3d</institution-id><institution-id institution-id-type="ISNI">0000 0001 0665 0280</institution-id><institution>School of Computing, </institution><institution>Clemson University, </institution></institution-wrap>Clemson, SC USA </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04d4sd432</institution-id><institution-id institution-id-type="GRID">grid.412124.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2323 5644</institution-id><institution>Faculty of Science of Sfax, </institution><institution>University of Sfax, </institution></institution-wrap>Sfax, Tunisia </aff></contrib-group><pub-date pub-type="epub"><day>31</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>31</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>25</volume><elocation-id>48</elocation-id><history><date date-type="received"><day>12</day><month>9</month><year>2024</year></date><date date-type="accepted"><day>21</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">This paper introduces SkinWiseNet (SWNet), a deep convolutional neural network designed for the detection and automatic classification of potentially malignant skin cancer conditions. SWNet optimizes feature extraction through multiple pathways, emphasizing network width augmentation to enhance efficiency. The proposed model addresses potential biases associated with skin conditions, particularly in individuals with darker skin tones or excessive hair, by incorporating feature fusion to assimilate insights from diverse datasets. Extensive experiments were conducted using publicly accessible datasets to evaluate SWNet&#x02019;s effectiveness.This study utilized four datasets-Mnist-HAM10000, ISIC2019, ISIC2020, and Melanoma Skin Cancer-comprising skin cancer images categorized into benign and malignant classes. Explainable Artificial Intelligence (XAI) techniques, specifically Grad-CAM, were employed to enhance the interpretability of the model&#x02019;s decisions. Comparative analysis was performed with three pre-existing deep learning networks-EfficientNet, MobileNet, and Darknet. The results demonstrate SWNet&#x02019;s superiority, achieving an accuracy of 99.86% and an F1 score of 99.95%, underscoring its efficacy in gradient propagation and feature capture across various levels. This research highlights the significant potential of SWNet in advancing skin cancer detection and classification, providing a robust tool for accurate and early diagnosis. The integration of feature fusion enhances accuracy and mitigates biases associated with hair and skin tones. The outcomes of this study contribute to improved patient outcomes and healthcare practices, showcasing SWNet&#x02019;s exceptional capabilities in skin cancer detection and classification.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Deep learning</kwd><kwd>Skin cancer classification</kwd><kwd>Transfer learning</kwd><kwd>Explainable AI</kwd><kwd>Grad-CAM</kwd><kwd>Feature fusion</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000923</institution-id><institution>Australian Research Council</institution></institution-wrap></funding-source><award-id>IC190100020</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; BioMed Central Ltd., part of Springer Nature 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">The most important organ of the human body is the skin. It protects the body from extreme temperatures, UV rays, and chemicals and acts as a waterproof shield. The abnormal growth of skin cells may cause skin cancer [<xref ref-type="bibr" rid="CR1">1</xref>]. It occurs mainly outside the body, but can extend to other areas, including the eyes, nose, and neck [<xref ref-type="bibr" rid="CR2">2</xref>]. The epidermis, which consists of layers, is usually the target of such malignant disease [<xref ref-type="bibr" rid="CR3">3</xref>]. Skin cancers begin as precancerous lesions that are not malignant but become malignant over time. This disease has many causes that may sometimes lead to death. Therefore, early detection, regular skin examination procedures, correct diagnosis, and treatment are the keys to preventing skin cancer [<xref ref-type="bibr" rid="CR4">4</xref>&#x02013;<xref ref-type="bibr" rid="CR6">6</xref>]. Skin cancer is typically detected by doctors who identify suspicious areas on the skin. However, recent studies [<xref ref-type="bibr" rid="CR7">7</xref>&#x02013;<xref ref-type="bibr" rid="CR9">9</xref>] have shown that the use of advanced artificial intelligence techniques, such as Convolutional Neural Networks (CNN), can aid doctors diagnose diseases earlier stage [<xref ref-type="bibr" rid="CR10">10</xref>]. This has inspired researchers to develop advanced technological tools for diagnosing skin cancer diseases.</p><p id="Par3">The diagnosis of skin cancer involves various methods such as visual examination, dermoscopy, and biopsy. Dermoscopy and the expertise of a physician have significantly improved the accuracy of the identification. However, manual diagnosis poses challenges, prompting the adoption of computer-assisted diagnosis (CAD) when expert consultation is limited [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>]. Machine learning (ML), particularly deep learning utilizing convolutional neural networks (CNNs), has transformed the classification of skin cancer. Traditional ML methods are less prevalent, with deep learning models as effective as dermatologists in image recognition. Despite challenges like insufficient training data, deep learning improves accuracy, with a 15&#x02013;20% enhancement in cancer prediction over the past two decades [<xref ref-type="bibr" rid="CR13">13</xref>]. While some deep CNN models increase processing costs, integrating ML into computer-aided design (CAD) systems improves tumour disease identification and treatment, making the process more cost-effective. The fusion of machine learning and computer vision in CAD systems significantly enhances skin cancer detection, particularly in its early stages [<xref ref-type="bibr" rid="CR14">14</xref>]. AI in medicine is gaining prominence for skin cancer diagnosis, providing swift, precise, and consistent disease recognition. Computer-aided design streamlines the identification and treatment of tumour diseases, complementing traditional imaging methods like MRI, PET, and X-rays. AI-driven advancements seek to enhance outcomes and mortality rates by improving the early detection of skin cancer, addressing the constraints of subjective and time-consuming diagnostic procedures [<xref ref-type="bibr" rid="CR15">15</xref>].</p><p id="Par4">The following is a summary of the numerous intriguing observations revealed by our contributions. <list list-type="order"><list-item><p id="Par5">Developed a robust Deep Convolutional Neural Network, SWNet, from scratch, emphasizing network width expansion and global average pooling. Established the efficacy of the architecture by incorporating discrete layers of features at each stage, enhancing feature fusion.</p></list-item><list-item><p id="Par6">Implementation of SWNet algorithm for skin cancer classification achieved a 99.95% F1-score distinguishing between benign and malignant cases, surpassing the CNN model.</p></list-item><list-item><p id="Par7">Explainable Artificial Intelligence (XAI): We applied XAI techniques to interpret model outcomes. Enhanced understanding and confidence in results through XAI, contributing to the transparency of the model&#x02019;s decision-making process.</p></list-item><list-item><p id="Par8">We conducted benchmarking tests, evaluated model robustness, and applied bias mitigation techniques for skin cancer classification. We used fine-tuned and pre-trained EfficientNet, MobileNet, and Darknet models as benchmarks for comparison. Our results showed that SWNet outperformed the other models, highlighting its effectiveness in the field. We also demonstrated the significance of feature fusion in mitigating biases. We assessed the robustness of our models across diverse datasets, including HAM10000 and ISIC2019_2020. These tests established the reliability of SWNet and its contribution to bias reduction through feature fusion.</p></list-item></list></p><p id="Par9">The article is systematically structured, starting with a discussion related to the work in the field in &#x0201c;<xref rid="Sec2" ref-type="sec">Related works</xref>&#x0201d;&#x000a0;section. This sets the contextual backdrop for the proposed research. &#x0201c;<xref rid="Sec4" ref-type="sec">Proposed methodology</xref>&#x0201d;&#x000a0;section comprehensively explains the method employed in this study, elucidating the novel approach and methodology applied. &#x0201c;<xref rid="Sec11" ref-type="sec">Experimental result</xref>&#x0201d;&#x000a0;section presents the results, revealing the results and insights gleaned from implementing the proposed method. Finally, &#x0201c;<xref rid="Sec13" ref-type="sec">Limitations and future directions</xref>&#x0201d;&#x000a0;section concludes the article by offering conclusions drawn from the study and providing recommendations for further avenues of research in the domain. This organization ensures a logical flow of information.</p></sec><sec id="Sec2"><title>Related works</title><p id="Par10">Although non-automated medical communication systems have shown impressive results, assessing a patient&#x02019;s condition still requires the presence of professional medical experts. There is a clear and pressing need for automatic skin cancer detection. In this section, we will discuss the literature related to skin cancer detection and classification using machine learning and deep learning.</p><p id="Par11">In [<xref ref-type="bibr" rid="CR16">16</xref>], CNN techniques and SVM and KNN machine learning classifiers were applied for image feature extraction to show the skin lesion image&#x02019;s borders, texture, and color. The accuracy rates for the SVM and KNN classifiers were 77.8% and 57.3%, respectively. When using deep learning, the accuracy grows up to 85.5%. Despite the excellent result, splitting the image into parts can miss some relevant information to predict the class correctly. In another study, A. Demir et al. [<xref ref-type="bibr" rid="CR17">17</xref>] employed the ResNet-101 and Inception-v3 transfer learning models. Additionally, they implemented a data augmentation approach to address the issue of overfitting that may arise from training the model on a limited dataset. The accuracy achieved by the models was 84.09% and 87.42%, respectively. T. Emara et al. [<xref ref-type="bibr" rid="CR18">18</xref>] used the Inception V4 model pre-trained on ImageNet on the HAM10000 dataset to diagnose skin cancer disorders. The data set displays an imbalance, which led to the use of a data sampling strategy to address this problem. Their model has a rating accuracy of 94.7%.</p><p id="Par12">In 2020, C.N. Vasconcelos et al. [<xref ref-type="bibr" rid="CR19">19</xref>] trained the ISBI 2016 dataset with GoogLeNet. The researchers used standard data augmentation during sample processing to address the issue of an imbalanced training dataset influencing CNN performance. Maximum accuracy was 83.6%. Qasim et al. [<xref ref-type="bibr" rid="CR20">20</xref>] used the same model to leverage knowledge transfer effectively to classify eight distinct categories of skin lesions in the ISIC 2019 dataset. The model achieved a level of accuracy of 94%. Hosny et al. [<xref ref-type="bibr" rid="CR21">21</xref>] their method based on deep convolutional neural network (DCNN) for skin image classification. The methodology includes pre-processing to segment regions of interest (ROIs), augmenting ROI images with rotations and translations, and using different DCNN architectures. Deep convolutional neural networks (DCNNs) replace the last three layers to improve lesion classification. Three datasets were used to evaluate and fine-tune this technique. GoogleNet performed well on the MEDNODE, DermIS &#x00026; DermQuest, and ISIC 2017 datasets, achieving classification accuracies of 99.29%, 99.15%, and 98.14%, respectively. The model may miss important information when medical images are segmented. Diame et al. [<xref ref-type="bibr" rid="CR22">22</xref>] Classified seborrheic keratosis and melanoma using three deep-learning models: DenseNet161, Inception-v4, and ResNet-152. These models&#x02019; accuracy was 86.3%, 82.0%, and 88.7%.</p><p id="Par13">In 2022, Ahmadi et al. [<xref ref-type="bibr" rid="CR23">23</xref>] pre-trained the Inception-ResNet-v2 CNN on 57,536 lesions. Pre-training helped the model identify melanoma in lesions. For classification, the model included patient-specific parameters, particularly lesion location, age, and sex, in addition to the lesion image. The model accuracy was 94.5%. Also, Li, Z. [<xref ref-type="bibr" rid="CR24">24</xref>] Used the pre-trained technique to transfer learning Inception-ResNetV1 with the SVM classifier; the model was tested on ISIC 2019. A Classification accuracy rate of 98% helps diagnose clinical melanoma.</p><p id="Par14">In 2023, K. Mridha, J. Shin, et al. [<xref ref-type="bibr" rid="CR25">25</xref>] developed a deep learning (DL) prediction model for melanoma classification on the HAM10000 dataset, and using a data augmentation technique, they also used the resulting model interpretation technique (Grad-CAM, Grad-CAM++) with 82% classification accuracy. Thanka, M. Roshni, and colleagues [<xref ref-type="bibr" rid="CR26">26</xref>] used the VGG16 model as a transfer learning technique for feature extraction. These features are subsequently fed into the XGBoost classifier and optical gradient boosting machine for severity assessment and classification of benign and malignant conditions. The integration shows an accuracy level of 99.1%. The XGBoost and LightGBM models achieve a classification accuracy of 99.1% and 97.2%, respectively.</p><p id="Par15">In 2023, B. Tasar presented a modified CNN framework that employed transfer learning to categorize skin lesions in dermoscopy images. The model utilized imageNet-pre-trained CNN architectures and underwent training on the HAM10000 skin lesions dataset. The classification accuracy for the modified DenseNet121, VGGNet16, ResNet50, MobileNet, and Xception models was 94.29%, 93.28%, 87.10%, 83.10%, and 80.05%, respectively. These findings suggest that the proposed transfer learning framework surpassed the performance of traditional deep learning architectures in classifying skin lesion types [<xref ref-type="bibr" rid="CR27">27</xref>].</p><p id="Par16">M. Tahir et al. [<xref ref-type="bibr" rid="CR28">28</xref>] suggested DSCC_Net, a CNN-based deep-learning network for classifying melanoma. Three data sets were used to evaluate it. They struggled with the problem of an unequal distribution of categories across the data set. DSCC_Net performs better than six core models (ResNet-152, Vgg-16, Vgg-19, Inception-V3, EfficientNet-B0, and MobileNet), with accuracy scores of 94.17%, 93.76% retrieval, 94.28%, and 93.93%. F1 among the four categories of skin cancer.</p><p id="Par17">G. Qasim et al. [<xref ref-type="bibr" rid="CR29">29</xref>] used Deep spiking neural networks with surrogate gradient descent to classify 3670 ISIC 2019 melanoma pictures and 3323 non-melanoma photos. Spiking VGG-13 outperformed both VGG-13 and AlexNet models with an accuracy of 89.57% and an F1 score of 90.07%. This improvement occurred with less trainable parameters. The study by S. Waheed et al. [<xref ref-type="bibr" rid="CR30">30</xref>] utilized dermoscopy images to demonstrate the application of a deep learning system for melanoma identification. The researchers achieved an F1 score of 90.87% and sensitivity, specificity, and precision scores of 92.46%, 92.23%, and 92.46%, respectively. In another study, Y. Dahdouh et al. [<xref ref-type="bibr" rid="CR31">31</xref>] used a watershed algorithm in their proposal that combines deep learning and reinforcement learning techniques to detect and classify skin cancer. The proposed system achieved up to 80% accuracy on the HAM10000 data set. Using a watershed algorithm for image segmentation may only capture relevant image information if image gradation is calculated to identify potential regions of interest. It can also lead to excessive fragmentation if not used carefully.</p><p id="Par18">R. Maalej et al. [<xref ref-type="bibr" rid="CR32">32</xref>] used Mobilenet as a transfer learning model to extract features for classifying breast cancer histopathological images. This model was trained on the Breakhis dataset, and they treated the data imbalance using a data augmentation technique; the accuracy of the proposed model reached 90.0%.</p><sec id="Sec3"><title>Deep learning</title><p id="Par19">Recently, the domain of deep learning has demonstrated significant efficacy in solving a range of issues related to pattern identification and the field of computer vision, including image classification [<xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>]. This technique has been effectively showcased in numerous biomedical image analysis challenges. Classical machine learning involves a series of methodologies that require pre-processing and identifying relevant features from input data, such as extracting features like texture and intensity and forming descriptors from images meticulously. Subsequently, the models are trained to generate predictions using features. However, this methodology is limited by the fact that it requires domain expertise for the selection of features. A process that can be laborious and may not exhaustively extract intricate patterns within complex medical images [<xref ref-type="bibr" rid="CR35">35</xref>]. Figure <xref rid="Fig1" ref-type="fig">1</xref> illustrates the distinction between deep learning and machine learning.<fig id="Fig1"><label>Fig. 1</label><caption><p>Illustrating the difference between ML and DL</p></caption><graphic xlink:href="12911_2025_2889_Fig1_HTML" id="MO1"/></fig></p><p id="Par20">On the other hand, the significant advancement of deep learning networks, a branch of machine learning that utilizes neural networks, particularly those with intricate structures comprising numerous layers, has facilitated the automated acquisition of characteristics from unprocessed data. Convolutional neural networks, a prevalent kind of deep learning, demonstrate exceptional proficiency in analyzing medical pictures through their ability to acquire significant information directly from the images hierarchically. Deep learning models can have the capability to identify intricate spatial and contextual details through an end-to-end learning process. This leads to enhanced precision and reliability in making predictions and managing complex and vital tasks such as image assimilation at different scales [<xref ref-type="bibr" rid="CR35">35</xref>]. Implementing this approach has brought about a significant transformation in medical image analysis. It has effectively minimized the reliance on subjective and manually crafted features, leading to a substantial enhancement in diagnostic precision and the ability to detect diseases. Nevertheless, this challenge still exists due to various factors, including low image resolution, overlapping elements, intricate shapes, etc.</p><p id="Par21">In summary, traditional machine learning methods in medical image analysis are heavily dependent on manual features and traditional algorithms. In contrast, deep learning techniques, particularly Convolutional Neural Networks (CNNs), autonomously acquire hierarchical features directly from raw images. This characteristic of deep learning contributes to enhanced accuracy and performance in medical image analysis. The primary distinction resides in the automation of feature extraction and representation, rendering deep learning highly advantageous for collecting nuanced patterns seen in intricate medical images [<xref ref-type="bibr" rid="CR36">36</xref>]. A CNN is a feed-forward neural network that, as depicted in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, consists of one or more convolutional layers, followed by pooling layers.</p><p id="Par22">AlexNet is a prominent CNN because it was first developed in 2012 [<xref ref-type="bibr" rid="CR37">37</xref>]. This development includes many improvements. First, I used rectified linear unit (ReLU) activation to increase the non-linearity of the network, which helped solve gradient descent problems. To avoid overfitting, it also applied a dropout technique, similar to regularization, which involves stochastic activation and deactivation of neurons across all layers. By activating neurons in various ways, we can force data down new paths, improving the network&#x02019;s generalization ability. Finally, data augmentation strengthens the network by emphasizing image properties rather than the images themselves. It is implemented by providing images that have been arbitrarily cropped, rotated, and translated before being fed into the network. Finally, additional convolutional layers are placed before pooling layers to improve classification accuracy.<fig id="Fig2"><label>Fig. 2</label><caption><p>The basic structure of CNN</p></caption><graphic xlink:href="12911_2025_2889_Fig2_HTML" id="MO2"/></fig></p><p id="Par23">Afterwards, the VGG16 network was introduced, with some improvements included [<xref ref-type="bibr" rid="CR38">38</xref>]. It has improved the depth of the network, allowing it to learn more complex features. It also used 3 x 3 convolutional filters and a maximum of 2 x 2 pooling layers throughout the network. This simplicity made it easy to understand and iterate. Small convolutional filters enabled it to capture more detailed features in early layers. It also used dropout regulation, which helped reduce overfitting by randomly turning off a portion of a neuron during training. A deep convolutional neural network architecture is GoogleNet. It was introduced in 2014 and used numerous parallel convolutional layers using different filter sizes to capture features in photos at various scales. This framework architecture makes image recognition tasks more accurate and efficient. ResNet (Residual Network) is a deep neural network architecture that uses skip connections to solve the problem of vanishing gradients and make very deep network training possible [<xref ref-type="bibr" rid="CR39">39</xref>]. The DenseNet network has been designed to incorporate a highly intricate structure wherein the blocks of layers are interconnected [<xref ref-type="bibr" rid="CR40">40</xref>]. However, MobileNet is a lightweight convolutional neural network model for efficient inference on mobile and embedded devices [<xref ref-type="bibr" rid="CR37">37</xref>]. The networks mentioned above have been developed to categorize various medical image classification tasks efficiently [<xref ref-type="bibr" rid="CR41">41</xref>]. We aim to design an extended convolutional neural network to take different features from multiple levels and integrate them at each network stage, facilitating efficient gradient propagation and bringing significant benefits.</p></sec></sec><sec id="Sec4"><title>Proposed methodology</title><p id="Par24">Early skin cancer detection can save patients&#x02019; lives and increase their chances of survival. This section contains several steps, which are described below.</p><sec id="Sec5"><title> Data gathering</title><p id="Par25">Data acquisition includes the data collection process obtained via the electronic platform. We used four data sets containing images of skin cancer, divided into benign and malignant categories. The first dataset is taken from (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/datasets/hasnainjaved/melanoma-skin-cancer">https://www.kaggle.com/datasets/hasnainjaved/melanoma-skin-cancer</ext-link>). The skin cancer melanoma dataset contains 10,605 images. Skin cancer of melanoma is a deadly cancer. This data set will be useful for developing deep learning models for accurate classification of melanoma. Images were 300 pixels on the longest side and saved in JPEG file format. Figure <xref rid="Fig3" ref-type="fig">3</xref> shows a sample of the data set. The second dataset is from the International Skin Imaging Collaboration (ISIC) (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/datasets/%20nodoubttome/skin-cancer9-classesisic">https://www.kaggle.com/datasets/ nodoubttome/skin-cancer9-classesisic</ext-link>). This collection consists of 2357 images of malignant and benign tumours. The third dataset is from Skin Cancer MNIST: HAM10000 (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000">https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000</ext-link>), which is a folder named HAM10000_images_part_1 containing a set of images with names. An Excel file containing data related to those images is in the folder. We extracted them from the folder so that the df, bkl, and nv images are in the DX column in Excel and put them in a folder on the desktop called Benign. Other images within the same dx column, namely mel, bcc, ak, and akiec, are also placed in a folder on the desktop that we call malicious. This collection consists of 10,015 images of benign and malignant tumors. The fourth dataset is from (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/datasets/qikangdeng/isic-2019-and-2020-melanoma-dataset">https://www.kaggle.com/datasets/qikangdeng/isic-2019-and-2020-melanoma-dataset</ext-link>); this dataset is a set of melanoma images from the ISIC2019 and ISIC2020 challenge datasets; consisting of 11449 images for malignant and benign tumours. In this study, the same data are used. The dataset consists of images divided into two parts: 80% were used for training and 20% for testing. This study uses publicly available datasets. Per the dataset guidelines, no additional ethics approval is needed for secondary use.<fig id="Fig3"><label>Fig. 3</label><caption><p>Samples of patches with labels from the dataset</p></caption><graphic xlink:href="12911_2025_2889_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec6"><title>Data pre-processing</title><p id="Par26">After collecting the preprocessing is done for the images received from the dataset. Thus, all images were resized to a uniform size of 224 pixels in width and 224 pixels in height before being input into the deep learning model,order to be compatible with the inputs of our proposed model and with the pre-trained model.</p></sec><sec id="Sec7"><title>Pre-trained CNN architectures</title><p id="Par27">Leveraging pre-trained convolutional neural networks (CNNs) offers the potential for refinement through fine-tuning with ImageNet, which is particularly advantageous in medical image datasets where expansive networks can adeptly learn task-specific features [<xref ref-type="bibr" rid="CR42">42</xref>]. Extensive research [<xref ref-type="bibr" rid="CR43">43</xref>] underscores the efficacy of transfer learning in augmenting the performance of medical image classification. In our exploration, we selected EfficientNet, MobileNet, and Darknet CNN models with careful consideration of the number of layers. The procedural stages of our transfer learning approach are outlined in Fig. <xref rid="Fig4" ref-type="fig">4</xref>.</p><p id="Par28">EfficientNet, distinguished by a compound scaling method, dynamically adjusts its layer count based on model variants (e.g., B0, B1, B2). Introducing a balancing factor for efficient scaling of depth, width, and resolution, this architecture is celebrated for its outstanding performance with diminished parameters and computational demands, aligning seamlessly with the demands of medical image classification tasks.</p><p id="Par29">DarkNet, initially crafted for YOLO (You Only Look Once) object detection, showcases an impressive array of 53 convolutional layers. Operating with an input size of 224x224x3, Darknet employs deep layers to capture intricate features, establishing itself as a compelling choice for the classification of skin pathology.</p><p id="Par30">MobileNet, now equipped with 28 convolutional layers and maintaining a standardized input size of 224x224x3, optimizes computational efficiency through depthwise separable convolutions. This technique partitions regular convolutions into depth and pointwise layers, diminishing parameters and computational intricacies. MobileNet is particularly well-suited for the classification of skin pathology, especially in resource-constrained environments. Retraining these pre-existing networks on our dataset shifted our focus to discerning between normal and pathological skin conditions. The prominent instances of transfer learning for classification now encompass EfficientNet, MobileNet, and Darknet networks.<fig id="Fig4"><label>Fig. 4</label><caption><p>Procedures were performed using our method of knowledge transfer</p></caption><graphic xlink:href="12911_2025_2889_Fig4_HTML" id="MO4"/></fig></p></sec><sec id="Sec8"><title> Proposed approach- SWNet model</title><p id="Par31">This research paper presents a new design for a deep convolutional neural network called SWNet, which aims to improve the identification of critical elements related to the categorization of melanoma, a type of skin cancer. The system is designed based on a directed acyclic graph (DAG). The classification of melanoma requires a network with a more complex structure to extract more features, which helps to distinguish between normal and abnormal classes. The proposed model has an advantage in expanding its width without increasing computational cost significantly, which increases the amount of information that can be acquired and improves precision. The overall process of the classification methodology is shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. The SWNet architecture comprises multiple layers, namely:</p><p id="Par32">1. Initially, an Input layer consists of cropping the input image into 224&#x000d7;224 pixels. Cropping the image to a reasonable size helps to retain relevant information while reducing computational overhead and ensuring compatibility with the proposed model, standard, efficient processing, and improved generalizability across different tasks. This task aims to classify these patches into normal and abnormal categories. It makes it possible to analyze and treat the selected regions separately, which can be useful in different computer vision tasks.</p><p id="Par33">2. The convolutional layer is a fundamental component of convolutional neural networks (CNNs). This layer applies convolutions to the output of the preceding layer. The set of filters is learnable because the convolution filter is defined by its weights. The two-dimensional activation maps of the corresponding filters are generated by varying the height and width of the input volume. It is essential to observe that each filter possesses a comparable depth to entries [<xref ref-type="bibr" rid="CR44">44</xref>]. Furthermore, the output&#x02019;s dimensions can be controlled by adjusting three hyperparameters: zero padding, stride, and depth. Zero padding involves adding zeroes around the boundaries of the input to maintain its size. Stride pertains to the number of pixels the filter skips across the image. Depth, on the other hand, indicates the quantity of filters applied to the input image. These filters can identify different structures, including blobs, corners, edges, etc. This research utilizes a model of 33 convolutional layers, and increasing the numbe of layers in convolutional neural networks (CNNs) makes it possible to extract hierarchical features, which improves the model&#x02019;s ability to capture complex patterns. Deeper networks provide better representation and increased ability to learn complex data, each featuring 3 &#x000d7; 3 filters. Following each convolutional layer, subsequent layers of Batch Normalization (BN) and the Rectified Linear Unit (ReLU) are incorporated. 2. The convolutional layer is a fundamental component of convolutional neural networks (CNNs). This layer applies convolutions to the output of the preceding layer. The set of filters is learnable because the convolution filter is defined by its weights. The two-dimensional activation maps of the corresponding filters are generated by varying the height and width of the input volume. It is essential to observe that each filter possesses a comparable depth to entries [<xref ref-type="bibr" rid="CR44">44</xref>]. Furthermore, the output&#x02019;s dimensions can be controlled by adjusting three hyperparameters: zero padding, stride, and depth. Zero padding involves adding zeroes around the boundaries of the input to maintain its size. Stride pertains to the number of pixels the filter skips across the image. Depth, on the other hand, indicates the quantity of filters applied to the input image. These filters can identify different structures, including blobs, corners, edges, etc. This research utilizes a model of 33 convolutional layers, and increasing the numbe of layers in convolutional neural networks (CNNs) makes it possible to extract hierarchical features, which improves the model&#x02019;s ability to capture complex patterns. Deeper networks provide better representation and increased ability to learn complex data, each featuring 3 &#x000d7; 3 filters. Following each convolutional layer, subsequent layers of Batch Normalization (BN) and the Rectified Linear Unit (ReLU) are incorporated. SWNet&#x02019;s complex architecture achieves high accuracy but comes with significant computational demands, which can be challenging in resource-constrained clinical settings. Trade-offs between model complexity, inference time, and deployment feasibility must be carefully considered. Strategies such as model pruning, quantization, and lightweight versions can optimize SWNet for real-world applications. Balancing performance and computational cost is essential for effective clinical integration.</p><p id="Par34">3. Batch normalization is a method employed in machine learning and deep learning models to normalize the activations of a neural network layer by adjusting and scaling them. This layer is used to normalize every input channel within a mini-batch. This technique accelerates the Convolutional Neural Networks (CNNs) training process and reduces the network initialization&#x02019;s sensitivity [<xref ref-type="bibr" rid="CR45">45</xref>]. This study places the batch normalization layer between the convolutional and ReLU layers. This work incorporates 37 batch normalization layers; these layers help stabilize the training process in deeper models. The mechanism of the batch normalization layer involves the normalization of channel activations through the subtraction of the mini-batch average and division by the standard deviation of the mini-batch to enhance stability, elevate learning rates, decrease reliance on explicit regularization, and enhance the management of gradient issues.</p><p id="Par35">4. The Rectified Linear Unit (ReLU) layer filters information by using the max (0, x) function, where x represents the neuron&#x02019;s input [<xref ref-type="bibr" rid="CR46">46</xref>]. ReLU benefits neural networks by introducing nonlinearity, addressing the vanishing gradient problem, and providing computational efficiency.<fig id="Fig5"><label>Fig. 5</label><caption><p>illustrates our classification pipeline</p></caption><graphic xlink:href="12911_2025_2889_Fig5_HTML" id="MO5"/></fig></p><p id="Par36">5. The Addition Layer combines the inputs from two or more neural network layers. All inputs must have identical dimensions for this layer to function.</p><p id="Par37">6. The average pooling layer divides its input into rectangular pooling zones of various sizes, such as 2 &#x000d7; 2, 3 x 3, etc. It calculates the average values in each smaller spatial block [<xref ref-type="bibr" rid="CR47">47</xref>]. Average pooling extracts normalized feature information from significant and insignificant pixel data. Max polling emphasizes edges, corners, and lines to improve an image. In the final stage of the network, we use global average pooling instead of max pooling to avoid losing some attributes to max pooling. All of these qualities, large or small, help differentiate classes.</p><p id="Par38">7. Dropout layers prevent overfitting and improve model performance. This layer activates and deactivates neurons randomly [<xref ref-type="bibr" rid="CR48">48</xref>]. In our work, two dropout layers are employed between fully connected layers with a dropout probability of <italic>p</italic> = 0.5.</p><p id="Par39">8. Fully Connected (FC) Layer: All of the neurons from the previous layer are connected to all of the neurons in this layer. This layer mixes the attributes that can be used to divide skin patches into two groups: normal and abnormal [<xref ref-type="bibr" rid="CR47">47</xref>]. Our suggested SWNet comprises three FC layers, which led to performance improvements.</p><p id="Par40">Figure <xref rid="Fig6" ref-type="fig">6</xref> demonstrates the utilization of the Softmax function for categorization. The output layer resides at the highest location within the complete and ultimate connected layer, as illustrated in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. The total number of SWNet layers is 113. The image classification task requires a deep architecture to better capture complex and essential image patterns through a deeper network. This provided us with better accuracy and performance on other metrics, as shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.<fig id="Fig6"><label>Fig. 6</label><caption><p>Overview of the network training procedures</p></caption><graphic xlink:href="12911_2025_2889_Fig6_HTML" id="MO6"/></fig></p><p id="Par41">SWNet&#x02019;s architecture, based on CNNs, is highly generalizable and can be adapted to various image classification tasks, including medical and non-medical applications. Its feature extraction techniques, using multiple paths, enhance versatility across different datasets. Robust performance metrics ensure easy adaptation to new tasks, while its compatibility with transfer learning enables leveraging learned features for other domains. These attributes make SWNet a scalable framework beyond skin cancer detection.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Presents the architectural components of the proposed SWNet model</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Name of layer</th><th align="left">Kernel size and stride</th><th align="left">Activations</th></tr></thead><tbody><tr><td align="left">Input layer</td><td align="left"/><td align="left">224&#x000d7;224&#x000d7;3</td></tr><tr><td align="left">Conv1, BN1, ReLU1</td><td align="left">Conv1: Kernel size=3&#x000d7;3</td><td align="left">224&#x000d7;224&#x000d7;32</td></tr><tr><td align="left">Conv2, BN2, ReLU2</td><td align="left">Conv2: Kernel size=3&#x000d7;3, stride=1</td><td align="left">224&#x000d7;224&#x000d7;32</td></tr><tr><td align="left">Conv3, BN3, ReLU3</td><td align="left">Conv3: Kernel size=3&#x000d7;3, stride=2</td><td align="left">112&#x000d7;112&#x000d7;32</td></tr><tr><td align="left">Conv4, BN4, ReLU4</td><td align="left">Conv4: Kernel size=3&#x000d7;3, stride=1</td><td align="left">224&#x000d7;224&#x000d7;32</td></tr><tr><td align="left">Conv5, BN5, ReLU5</td><td align="left">Conv5: Kernel size=3&#x000d7;3, stride=2</td><td align="left">112&#x000d7;112&#x000d7;32</td></tr><tr><td align="left">Conv6, BN6, ReLU6</td><td align="left">Conv6: Kernel size=3&#x000d7;3, stride=1</td><td align="left">224&#x000d7;224&#x000d7;32</td></tr><tr><td align="left">Conv7, BN7, ReLU7</td><td align="left">Conv7: Kernel size=3&#x000d7;3, stride=2</td><td align="left">112&#x000d7;112&#x000d7;32</td></tr><tr><td align="left">Conv8, BN8, ReLU8</td><td align="left">Conv8: Kernel size=3&#x000d7;3, stride=1</td><td align="left">224&#x000d7;224&#x000d7;32</td></tr><tr><td align="left">Conv9, BN9, ReLU9</td><td align="left">Conv9: Kernel size=3&#x000d7;3, stride=2</td><td align="left">112&#x000d7;112&#x000d7;32</td></tr><tr><td align="left">Concat1, BNConcat1</td><td align="left">Concatenation of four inputs</td><td align="left">112&#x000d7;112&#x000d7;128</td></tr><tr><td align="left">Conv10,BN10,ReLU10</td><td align="left">Conv10: Kernel size=3&#x000d7;3, stride=1</td><td align="left">112&#x000d7;112&#x000d7;64</td></tr><tr><td align="left">Conv11, BN11, ReLU11</td><td align="left">Conv11: Kernel size=3&#x000d7;3, stride=2</td><td align="left">56&#x000d7;56&#x000d7;64</td></tr><tr><td align="left">Conv12, BN12, ReLU12</td><td align="left">Conv12: Kernel size=3&#x000d7;3, stride=1</td><td align="left">112&#x000d7;112&#x000d7;64</td></tr><tr><td align="left">Conv13, BN13, ReLU13</td><td align="left">Conv13: Kernel size=3&#x000d7;3, stride=2</td><td align="left">56&#x000d7;56&#x000d7;64</td></tr><tr><td align="left">Conv14, BN14, ReLU14</td><td align="left">Conv14: Kernel size=3&#x000d7;3, stride=1</td><td align="left">112&#x000d7;112&#x000d7;64</td></tr><tr><td align="left">Conv15, BN15, ReLU15</td><td align="left">Conv15: Kernel size=3&#x000d7;3, stride=2</td><td align="left">56&#x000d7;56&#x000d7;64</td></tr><tr><td align="left">Conv16, BN16, ReLU16</td><td align="left">Conv16: Kernel size=3&#x000d7;3, stride=1</td><td align="left">112&#x000d7;112&#x000d7;64</td></tr><tr><td align="left">Conv17, BN17, ReLU17</td><td align="left">Conv17: Kernel size=3&#x000d7;3, stride=2</td><td align="left">56&#x000d7;56&#x000d7;64</td></tr><tr><td align="left">Concat2, BNConcat2</td><td align="left">Concatenation of four inputs</td><td align="left">56&#x000d7;56&#x000d7;64</td></tr><tr><td align="left">Conv18, BN18, ReLU18</td><td align="left">Conv18: Kernel size=3&#x000d7;3, stride=1</td><td align="left">56&#x000d7;56&#x000d7;64</td></tr><tr><td align="left">Conv19, BN19, ReLU19</td><td align="left">Conv19: Kernel size=3&#x000d7;3, stride=2</td><td align="left">28&#x000d7;28&#x000d7;128</td></tr><tr><td align="left">Conv20, BN20, ReLU20</td><td align="left">Conv20: Kernel size=3&#x000d7;3, stride=1</td><td align="left">56&#x000d7;56&#x000d7;128</td></tr><tr><td align="left">Conv21, BN21, ReLU21</td><td align="left">Conv21: Kernel size=3&#x000d7;3, stride=2</td><td align="left">28&#x000d7;28&#x000d7;128</td></tr><tr><td align="left">Conv22, BN22, ReLU22</td><td align="left">Conv22: Kernel size=3&#x000d7;3, stride=1</td><td align="left">56&#x000d7;56&#x000d7;128</td></tr><tr><td align="left">Conv23, BN23, ReLU23</td><td align="left">Conv23: Kernel size=3&#x000d7;3, stride=2</td><td align="left">28&#x000d7;28&#x000d7;128</td></tr><tr><td align="left">Conv24, BN24, ReLU24</td><td align="left">Conv24: Kernel size=3&#x000d7;3, stride=1</td><td align="left">56&#x000d7;56&#x000d7;128</td></tr><tr><td align="left">Conv25, BN25, ReLU25</td><td align="left">Conv25: Kernel size=3&#x000d7;3, stride=2</td><td align="left">28&#x000d7;28&#x000d7;128</td></tr><tr><td align="left">Concat3, BNConcat3</td><td align="left">Concatenation of four inputs</td><td align="left">28&#x000d7;28&#x000d7;512</td></tr><tr><td align="left">Conv26, BN26, ReLU26</td><td align="left">Conv26: Kernel size=3&#x000d7;3, stride=1</td><td align="left">28&#x000d7;28&#x000d7;256</td></tr><tr><td align="left">Conv27, BN27, ReLU27</td><td align="left">Conv27: Kernel size=3&#x000d7;3, stride=2</td><td align="left">14&#x000d7;14&#x000d7;256</td></tr><tr><td align="left">Conv28, BN28, ReLU28</td><td align="left">Conv28: Kernel size=3&#x000d7;3, stride=1</td><td align="left">28&#x000d7;28&#x000d7;256</td></tr><tr><td align="left">Conv29, BN29, ReLU29</td><td align="left">Conv29: Kernel size=3&#x000d7;3, stride=2</td><td align="left">14&#x000d7;14&#x000d7;256</td></tr><tr><td align="left">Conv30, BN30, ReLU30</td><td align="left">Conv30: Kernel size=3&#x000d7;3, stride=1</td><td align="left">28&#x000d7;28&#x000d7;256</td></tr><tr><td align="left">Conv31, BN31, ReLU31</td><td align="left">Conv31: Kernel size=3&#x000d7;3, stride=2</td><td align="left">14&#x000d7;14&#x000d7;256</td></tr><tr><td align="left">Conv32, BN32, ReLU32</td><td align="left">Conv32: Kernel size=3&#x000d7;3, stride=1</td><td align="left">28&#x000d7;28&#x000d7;256</td></tr><tr><td align="left">Conv33, BN33, ReLU33</td><td align="left">Conv33: Kernel size=3&#x000d7;3, stride=2</td><td align="left">14&#x000d7;14&#x000d7;256</td></tr><tr><td align="left">Concat4, BNConcat4</td><td align="left">Concatenation of four inputs</td><td align="left">14&#x000d7;14&#x000d7;1024</td></tr><tr><td align="left">Average polling layer</td><td align="left"/><td align="left">1&#x000d7;1&#x000d7;1024</td></tr><tr><td align="left">Fc1</td><td align="left">300 fully connected</td><td align="left">1&#x000d7;1&#x000d7;300</td></tr><tr><td align="left">Drop1</td><td align="left">Dropout layer with learning rate:0.5</td><td align="left">1&#x000d7;1&#x000d7;300</td></tr><tr><td align="left">Fc2</td><td align="left">64 fully connected</td><td align="left">1&#x000d7;1&#x000d7;64</td></tr><tr><td align="left">Drop2</td><td align="left">Dropout layer with learning rate:0.5</td><td align="left">1&#x000d7;1&#x000d7;300</td></tr><tr><td align="left">Fc3 (Softmax layer)</td><td align="left">0= Benign, 1= Malignant</td><td align="left">1&#x000d7;1&#x000d7;2</td></tr></tbody></table></table-wrap></p><p id="Par42">
<fig id="Fig7"><label>Fig. 7</label><caption><p>SWNet architecture. Where: CONV = Convolutional layer; BN = Batch Normalization layer; Relu = Rectified Linear Unit layer; Concat = Concatenation Layer (Combines the inputs from two or more neural network layers); <sc>BNC</sc>oncat = Batch Normalization Concatenation layer; Drop = Dropout layer; <sc>FC</sc> = Fully Connected Layer</p></caption><graphic xlink:href="12911_2025_2889_Fig7_HTML" id="MO7"/></fig>
</p><p id="Par43">The new SWNet architecture helps to obtain a multi-level advantage at each step. With each wrapping layer, there are more differentiating attributes. Figure <xref rid="Fig8" ref-type="fig">8</xref> shows the activation for both normal and diseased epidermis classes. The model and pre-trained models utilized in this study were trained on a dataset consisting of 10605 images for 100 epochs until the learning process reached a point of convergence. SWNet is a network proven to be the best for classifying skin cancer. SWNet architecture has been built entirely from scratch. For this experiment, we used a RAM of 16 GB and a GPU of 8 GB. The experiments were carried out using Matlab R2023a.<fig id="Fig8"><label>Fig. 8</label><caption><p><bold>a</bold>, <bold>b</bold> Features map for benign and malignant</p></caption><graphic xlink:href="12911_2025_2889_Fig8_HTML" id="MO8"/></fig></p></sec><sec id="Sec9"><title>Feature fusion</title><p id="Par44">Feature fusion is a methodology employed to enhance the effectiveness of machine learning models by combining features sourced from diverse origins. In the specific context of skin cancer detection, this approach proves advantageous as it amalgamates features from disparate datasets of skin cancer images, improving the model&#x02019;s accuracy. Various methods exist for implementing feature fusion [<xref ref-type="bibr" rid="CR49">49</xref>&#x02013;<xref ref-type="bibr" rid="CR51">51</xref>].</p><p id="Par45">One conventional method entails the simple concatenation of features from different datasets. Although this approach is straightforward, it may need to be more efficient when dealing with datasets that differ in the quantity of features. Alternatively, more advanced techniques like canonical correlation analysis (CCA) or support vector machines (SVMs) can be utilized for feature fusion. These sophisticated methods excel in capturing intricate relationships between features from diverse datasets, ultimately enhancing the model&#x02019;s overall performance.</p><p id="Par46">In skin cancer detection, multi-dataset feature fusion emerges as a promising strategy to address the challenge of data scarcity. The scarcity issue arises due to the limited availability of labeled examples of skin cancer, hindering the training of accurate and adaptable machine learning models for new data. By amalgamating features from multiple datasets, multidataset feature fusion expands the pool of labeled data accessible to the model. This augmentation plays a crucial role in improving both the accuracy of the model and its ability to adapt to new data [<xref ref-type="bibr" rid="CR52">52</xref>]. Figure <xref rid="Fig9" ref-type="fig">9</xref> shows the fusion structure.<fig id="Fig9"><label>Fig. 9</label><caption><p>Feature fusion diagram</p></caption><graphic xlink:href="12911_2025_2889_Fig9_HTML" id="MO9"/></fig></p><p id="Par47">Integrating multi-dataset feature fusion into skin cancer detection brhas several benefits. First, it improves the accuracy of skin cancer detection models. Second, it improves generalizability, allowing these models to perform effectively on new and unseen data. Third, multi-dataset feature fusion addresses overfitting in skin cancer detection models.</p><p id="Par48">However, adopting multi-dataset feature fusion in skin cancer detection is challenging. Ensuring compatibility between datasets in terms of feature representation and labeling poses a significant obstacle. The computational complexity of specific feature fusion methods also presents a potential drawback. Furthermore, interpreting the outcomes of models utilizing multi-dataset feature fusion can be intricate.</p><p id="Par49">Despite these challenges, multi-dataset feature fusion remains a promising avenue for advancing skin cancer detection. With careful design and implementation, it holds the potential to significantly enhance the accuracy and adaptability of skin cancer detection models.</p></sec><sec id="Sec10"><title>Explainable artificial intelligence(XAI)</title><p id="Par50">Deep learning networks are commonly characterized as &#x0201c;black boxes&#x0201d; due to their lack of transparency on the underlying rationale behind the network&#x02019;s decision-making process [<xref ref-type="bibr" rid="CR53">53</xref>, <xref ref-type="bibr" rid="CR54">54</xref>]. The utilization of deep learning networks is increasingly prevalent across numerous disciplines, including medical care, so understanding why the network makes a particular decision is critical.</p><p id="Par51">A subset of interpretability techniques called visualization methods uses visual illustrations to describe what the network is looking at and its predictions. This topic focuses on post-training techniques that annotate predictions made by a network trained on image data using test images [<xref ref-type="bibr" rid="CR55">55</xref>, <xref ref-type="bibr" rid="CR56">56</xref>].</p><p id="Par52">We incorporated interpretable artificial intelligence (XAI) techniques into our proposed model to ensure transparency and interpretability. Specifically, we used Grad-CAM to identify features of interest, generate interpretations of the trained model&#x02019;s predictions, and evaluate model reliability. The Grad-CAM heatmap is a visual representation that identifies the specific regions within an image that have the most significant influence on the prediction of a target. By superimposing a Grad-CAM heat map onto the original image, it becomes possible to observe how parts of the image or input data that influence the model&#x02019;s decision are highlighted and to assign relevance scores to different features by analyzing the gradients or heat maps produced by Grad-CAM, thus giving us a clear understanding of the model&#x02019;s predictions. The classification score hierarchy concerning convolutional features generated by the network is used to ascertain which regions of the image hold the highest importance for classification purposes. The following Fig. <xref rid="Fig10" ref-type="fig">10</xref> illustrations are part of our target applications for test images by using Grad-CAM for 9 layers.<fig id="Fig10"><label>Fig. 10</label><caption><p>Images taken using Grad-CAM</p></caption><graphic xlink:href="12911_2025_2889_Fig10_HTML" id="MO10"/></fig></p></sec></sec><sec id="Sec11"><title>Experimental result</title><p id="Par53">The dataset was partitioned into two phases: a training phase and a testing phase. We conducted a series of experiments on the dataset to evaluate the classification performance of our network, as well as the fine-tuned networks that were employed. Applying interpretable artificial intelligence (XAI) techniques significantly improved the interpretability of our proposed model. The generated interpretations allowed us to identify the main factors that influence the model predictions and understand their reasons. By analyzing the explanations provided, we could confirm the model&#x02019;s consistency with domain knowledge and identify potential areas for improvement.</p><p id="Par54">An F1 score is used to evaluate the performance of the proposed and finetuned models. F1_score represents the balance between recall (R) and precision (P), two crucial characteristics for assessing the proposed technique. Precision, recall, and F1_score are computed using Eqs.&#x000a0;(<xref rid="Equ1" ref-type="disp-formula">1</xref>),&#x000a0;(<xref rid="Equ2" ref-type="disp-formula">2</xref>), and&#x000a0;(<xref rid="Equ3" ref-type="disp-formula">3</xref>).<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Precision} = \frac{\text {TP}}{\text {TP} + \text {FP}} \end{aligned}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12911_2025_2889_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Recall} = \frac{\text {TP}}{\text {TP} + \text {FN}} \end{aligned}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Recall</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12911_2025_2889_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {F1\_Score} = \frac{\text {Precision} \times \text {Recall}}{\text {Precision} + \text {Recall}} \end{aligned}$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>F1\_Score</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mo>&#x000d7;</mml:mo><mml:mtext>Recall</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mo>+</mml:mo><mml:mtext>Recall</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12911_2025_2889_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par55">The number of images that the network correctly classifies as relevant is TP (true positive). The number of images that the network correctly classifies as irrelevant is TN (True Negative). The number of images that the network mistakenly classifies as relevant is FP (False Positive). The number of relevant images that the network cannot recognize is FN (False Negative), as seen in the general confusion matrix Fig. <xref rid="Fig11" ref-type="fig">11</xref>a, and b represents our data confusion matrix.<fig id="Fig11"><label>Fig. 11</label><caption><p>The proposed confusion matrix of SWNet</p></caption><graphic xlink:href="12911_2025_2889_Fig11_HTML" id="MO11"/></fig></p><p id="Par56">The model was trained using the SGD optimizer with a learning rate 0.001 and other specified hyperparameters such as batch size. The optimal outcome was reached by executing 100 epochs, resulting in an accuracy metric of 99.86%. Figure <xref rid="Fig12" ref-type="fig">12</xref> illustrates the process of our training.<fig id="Fig12"><label>Fig. 12</label><caption><p>Illustration of training process</p></caption><graphic xlink:href="12911_2025_2889_Fig12_HTML" id="MO12"/></fig></p><p id="Par57">In Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, a comparison is presented between the current study and previous research. The metrics of the proposed SWNet were reported using the Softmax classifier, demonstrating its superior performance. SWNet achieved the highest standards of precision, recall, and F1-score at 100%, 99.90%, and 99.95%, respectively. These results conclude that SWNet outperforms existing models as a robust tool for skin cancer detection.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Comparison of SWNet with similar studies</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Net work</th><th align="left">Year</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-Score</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td align="left">S. Waheed et at. [<xref ref-type="bibr" rid="CR30">30</xref>]</td><td align="left">2023</td><td align="left">92.46%</td><td align="left">92.46%</td><td align="left">90.87%</td><td align="left">91%</td></tr><tr><td align="left">Y. Dahdouh, et al. [<xref ref-type="bibr" rid="CR31">31</xref>]</td><td align="left">2023</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">80%</td></tr><tr><td align="left">M. Tahir, et al. [<xref ref-type="bibr" rid="CR28">28</xref>]</td><td align="left">2023</td><td align="left">94.17</td><td align="left">93.76</td><td align="left">93.93</td><td align="left">94.28</td></tr><tr><td align="left">Proposed SWNet</td><td align="left">2024</td><td align="left">100%</td><td align="left">99.90</td><td align="left">99.95</td><td align="left">99.86</td></tr></tbody></table></table-wrap></p><p id="Par58">Figure <xref rid="Fig13" ref-type="fig">13</xref> displays the prediction of some test images using SWNet with Explainable AI (XAI) technique. Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> compares SWNet with contemporary networks for classification, assessing their performance across various metrics. The evaluated networks include EfficientNet, MobileNet, Darknet, and the proposed SWNet. The metrics considered are Precision, Recall, Specificity, F1-Score, and Accuracy, providing a holistic view of the classification capabilities of each network.</p><p id="Par59">EfficientNet achieves a Precision of 89.05%, recall of 89.77%, Specificity of 88.03%, F1-Score of 86.39%, and Accuracy of 91.88%. MobilNet follows closely with a Precision of 89.64%, recall of 90.22%, specificity of 87.60%, F1-Score of 92.23%, and an Accuracy of 93.2%. Darknet outperforms both with a Precision of 91.66%, Recall of 93.10%, specificity of 90.18%, F1-Score of 92.17%, and an Accuracy of 94.44%.</p><p id="Par60">Notably, the proposed SWNet surpasses all, achieving exceptional results across all metrics. SWNet attains a perfect precision of 100%, an impressive recall of 99.90%, a perfect specificity of 100%, an outstanding F1-Score of 99.95%, and an accuracy of 99.86%. These results highlight the superior classification performance of SWNet compared to the other modern networks, making it a compelling choice for the task.</p><p id="Par61">SWNet&#x02019;s perfect precision and specificity indicate its ability to minimize false positives, which is essential in applications where misclassifying a positive instance is critical. The high recall underscores its effectiveness in capturing true positive instances, while the remarkable F1-Score reflects a balance between Precision and Recall. The overall high accuracy further solidifies SWNet&#x02019;s position as a robust and reliable choice for classification tasks, outperforming well-established networks in the field. In summary, SWNet sets a new benchmark for classification tasks, making it a compelling choice for applications requiring high precision, reliability, and interpretability.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Comparison of SWNet with modern networks for classification</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">Specificity</th><th align="left">F1-Score</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td align="left">EfficientNet [<xref ref-type="bibr" rid="CR42">42</xref>]</td><td align="left">89.05</td><td align="left">89.77</td><td align="left">88.03</td><td align="left">86.39</td><td align="left">91.88</td></tr><tr><td align="left">Mobilnet [<xref ref-type="bibr" rid="CR37">37</xref>]</td><td align="left">89.5</td><td align="left">90.22</td><td align="left">87.60</td><td align="left">92.23</td><td align="left">93.2</td></tr><tr><td align="left">Darknet [<xref ref-type="bibr" rid="CR43">43</xref>]</td><td align="left">91.66</td><td align="left">93.10</td><td align="left">90.18</td><td align="left">92.17</td><td align="left">94.44</td></tr><tr><td align="left">Proposed SWNet</td><td align="left">100</td><td align="left">99.90</td><td align="left">100</td><td align="left">99.95</td><td align="left">99.86</td></tr></tbody></table></table-wrap></p><p id="Par62">
<fig id="Fig13"><label>Fig. 13</label><caption><p>Predictions of test samples by SWNet enhanced with explainable artificial intelligence (XAI)</p></caption><graphic xlink:href="12911_2025_2889_Fig13_HTML" id="MO13"/></fig>
</p><p id="Par63">The provided Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> presents a comparative analysis of several neural network models based on various parameters. Each model is assessed according to its number of convolution layers, the quantity of convolution kernels employed, the presence of pooling layers, the utilization of batch normalization, the count of fully connected layers, and the recognition time. EfficientNet showcases a comprehensive architecture with its 65 convolution layers and a substantial number of convolution kernels (3,328,992). It incorporates 17 pooling layers, utilizes batch normalization, and boasts 49 fully connected layers, achieving a recognition time of 15 milliseconds. In contrast, MobileNet features fewer convolution layers (15) and a moderate number of convolution kernels (4,253,864), with only one pooling layer and no batch normalization. Despite its simpler architecture, MobileNet demonstrates a shorter recognition time of 10 milliseconds. Darknet, with 18 convolution layers and a significant number of convolution kernels (19,810,176), emphasizes batch normalization across its architecture. However, it exhibits a longer recognition time of 25 milliseconds. Finally, the proposed SWNET model introduces 33 convolution layers and a notable number of convolution kernels (9,368,192), along with batch normalization and 37 fully connected layers, resulting in a recognition time of 20 milliseconds. These specifications offer insights into theach model&#x02019;s computational complexities and inference speeds enabling informed decisions regarding their suitability for specific applications.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Models parameters where N1: No. convolution layers; N2: No. convolution kernels; N3: No. pooling layers; N4: No. Batch normalization; N5: No.fully connected layer: N6: Recognition Time; GAP: Global Average Pooling</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Network</th><th align="left">N1</th><th align="left">N2</th><th align="left">N3</th><th align="left">N4</th><th align="left">N5</th><th align="left">N6</th></tr></thead><tbody><tr><td align="left">EfficientNet [<xref ref-type="bibr" rid="CR42">42</xref>]</td><td align="left">65</td><td align="left">3328992</td><td align="left">17 (GAP)</td><td align="left">49</td><td align="left">1</td><td align="left">15 ms</td></tr><tr><td align="left">MobilNet [<xref ref-type="bibr" rid="CR37">37</xref>]</td><td align="left">15</td><td align="left">4253864</td><td align="left">1 (GAP)</td><td align="left">27</td><td align="left">0</td><td align="left">10 ms</td></tr><tr><td align="left">Darknet [<xref ref-type="bibr" rid="CR43">43</xref>]</td><td align="left">18</td><td align="left">19810176</td><td align="left">6 (Max pooling)</td><td align="left">18</td><td align="left">1</td><td align="left">25 ms</td></tr><tr><td align="left">Proposed SWNet</td><td align="left">33</td><td align="left">9368192</td><td align="left">1 (GAP)</td><td align="left">37</td><td align="left">3</td><td align="left">20 ms</td></tr></tbody></table></table-wrap></p><p id="Par64">On the other hand, we conducted several experiments to test the proposed model on different sets of databases to compare performance. The results are shown in Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Perforance comparsion of the used dataset in our experiment</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Acc.</th><th align="left">Sen.</th><th align="left">Spe.</th><th align="left">Pre.</th><th align="left">F1-Score</th><th align="left"># Images</th></tr></thead><tbody><tr><td align="left">Mnist-ham10000 [<xref ref-type="bibr" rid="CR57">57</xref>]</td><td align="left">81.93</td><td align="left">55.73</td><td align="left">85.71</td><td align="left">36.06</td><td align="left">43.79</td><td align="left">10015</td></tr><tr><td align="left">ISIC2019 [<xref ref-type="bibr" rid="CR58">58</xref>]</td><td align="left">84.76</td><td align="left">84.08</td><td align="left">85.39</td><td align="left">84.08</td><td align="left">84.08</td><td align="left">2099</td></tr><tr><td align="left">ISIC2019_2020 [<xref ref-type="bibr" rid="CR59">59</xref>]</td><td align="left">91.09</td><td align="left">92.60</td><td align="left">90.01</td><td align="left">86.97</td><td align="left">89.70</td><td align="left">11449</td></tr><tr><td align="left">Melanoma Skin [<xref ref-type="bibr" rid="CR60">60</xref>]</td><td align="left">99.86</td><td align="left">99.90</td><td align="left">100</td><td align="left">100</td><td align="left">99.95</td><td align="left">10605</td></tr></tbody></table></table-wrap></p><p id="Par65">Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref> presents the outcomes of our proposed method using the mnist-HAM10000 dataset, revealing an accuracy of 81.93%. Despite having the most significant number of training images among the datasets, this accuracy may not suffice for specialists to rely on, as indicated by the low F-score of 43.79%, signifying an imbalance in the data. The model exhibits bias towards the class with a higher volume of data. Conversely, with the ISIC2019 dataset containing a smaller training set of 2357 images, the accuracy improves to 84.76%. However, in experiments with the ISIC-2019&#x02013;2020 melanoma dataset, the model achieves an accuracy of 91.09% and an F-score of 89.70%, surpassing previous results. Despite this improvement, the model needs to meet the required accuracy for diagnosing diseases due to the prevalence of darkness in the background of trained photomicrographs. Nevertheless, the proposed SWNet model demonstrates superior performance in early skin cancer detection, successfully discerning complex differences between benign and malignant behaviors, and showcasing its effectiveness across multiple databases.</p><sec id="Sec12"><title>Skin cancer bias mitigation in dataset</title><p id="Par66">The Skin Cancer dataset poses a potential risk of bias in AI models, as seen in Fig. <xref rid="Fig14" ref-type="fig">14</xref>, explicitly affecting the SWNet model&#x02019;s interpretation of darkness as a circular image, a consequence of microscope artefacts. This bias has the potential to impede the model&#x02019;s ability to generalize effectively, creating challenges in adapting to the diverse and non-circular patterns characteristic of real-world skin abnormalities. Additionally, the dataset&#x02019;s limitations in adequately representing various skin types, ethnicities, and conditions further undermine the model&#x02019;s robustness across diverse populations, giving rise to concerns regarding potential misdiagnoses and oversimplification of the nuanced manifestations of skin cancer. To address these challenges, a diverse dataset is necessary to represent different skin types, ethnicities, and conditions. This inclusivity is essential for fostering a more comprehensive learning process. Augmenting the dataset through rotation and scaling becomes imperative to artificially introduce diversity, thereby reducing the model&#x02019;s sensitivity to specific features like circular patterns resulting from microscope artefacts. Another critical strategy involves incorporating adversarial training during the model development phase. This proactive approach exposes the model to potential biases, promoting adaptability and reducing susceptibility to artefacts. Regular model audits and continuous monitoring post-deployment are essential to promptly identify and rectify emerging biases, guaranteeing the model&#x02019;s reliability in real-world scenarios. Furthermore, integrating Explainable AI (XAI) techniques contributes to transparency, enabling healthcare professionals to understand the model&#x02019;s decision-making process. Collaboration with domain experts, particularly dermatologists and healthcare professionals, proves invaluable. Their expertise validates the model&#x02019;s outputs and enhances its clinical relevance and accuracy. Establishing feedback loops for real-world performance data and implementing iterative model improvement processes become crucial. This dynamic approach empowers developers to address evolving challenges and maintain the model&#x02019;s effectiveness. Lastly, a steadfast commitment to ethical considerations, focusing on fairness, equity, and patient well-being, guides AI&#x02019;s responsible development and deployment in healthcare. This commitment fosters trust among users and ensures positive outcomes in medical applications.<fig id="Fig14"><label>Fig. 14</label><caption><p>The initial column displays the ground truth images, the second column showcases the outcomes post-classification, and the third column exhibits the Grad-CAM visualizations</p></caption><graphic xlink:href="12911_2025_2889_Fig14_HTML" id="MO14"/></fig></p><p id="Par67">Integrating feature fusion from diverse datasets, including Mnist-ham10000, ISIC2019, ISIC2019_2020, and Melanoma Skin Cancer, presents a comprehensive approach to enhancing the performance of risk-biased skin cancer classification. Combining unique features extracted from these varied sources gives the model a more nuanced understanding of skin abnormalities, thereby improving its ability to discern risks associated with different skin conditions. The Mnist-ham10000 dataset contributes information about diverse skin types and conditions, while the ISIC2019 and ISIC2019_2020 datasets offer insights into a broad spectrum of skin lesions. Including the Melanoma Skin Cancer dataset further enriches the feature set with specific details pertaining to melanoma, a critical aspect in risk assessment. Through feature fusion, this amalgamation of datasets not only broadens the model&#x02019;s knowledge base but allows it to adapt to the intricacies of diverse skin characteristics. The summation of features from these datasets collectively fortifies the risk-biased skin cancer classification model, fostering improved accuracy and reliability in identifying potential health risks across a wide range of skin conditions and patient demographics.<fig id="Fig15"><label>Fig. 15</label><caption><p>Feature fusion: The first column presents the outcomes after classification, the second column highlights a biased model, and the third column demonstrates the results of the unbiased model</p></caption><graphic xlink:href="12911_2025_2889_Fig15_HTML" id="MO15"/></fig></p><p id="Par68">Feature fusion emerges as a potent strategy for addressing bias and promoting fairness within skin cancer datasets by incorporating diverse features, as seen in Fig. <xref rid="Fig15" ref-type="fig">15</xref>. Integrating features from various datasets, including Mnist-ham10000, ISIC2019, ISIC2019_2020, and Melanoma Skin Cancer, contributes to a more extensive and inclusive understanding of skin conditions. This method alleviates bias by exposing the model to a broader spectrum of skin types, lesions, and conditions, reducing the likelihood of favouring specific demographics. The potential bias from a limited dataset can be countered by incorporating features related to diverse skin characteristics, demographic information, and lesion types through feature fusion. This enables the model to identify patterns across various skin types, ethnicities, and conditions, ensuring a more comprehensive and equitable performance. As a result, the precision of skin cancer classification is improved, and the model&#x02019;s predictions remain impartial and unbiased across diverse patient populations.</p><p id="Par69">Furthermore, incorporating features from multiple datasets enables the model to adapt to the intricacies of real-world scenarios, effectively addressing concerns about fairness. For example, features from the Melanoma Skin Cancer dataset specifically focus on melanoma, providing vital information for accurate risk assessment in cases of this particular skin condition. The holistic perspective facilitated by feature fusion contributes to a more nuanced understanding of skin abnormalities, promoting fairness in the model&#x02019;s predictions. Therefore, feature fusion serves as a mechanism to reduce bias in skin cancer datasets by introducing a diverse array of features, fostering a more balanced and representative learning experience for the model. This enhances the model&#x02019;s overall performance and ensures its predictions are fair, unbiased, and applicable across a broad spectrum of skin conditions and patient demographics.<fig id="Fig16"><label>Fig. 16</label><caption><p>Challenges in hair-related bias</p></caption><graphic xlink:href="12911_2025_2889_Fig16_HTML" id="MO16"/></fig></p><p id="Par70">Figure <xref rid="Fig16" ref-type="fig">16</xref> shows that biases can arise in hair skin cancer datasets due to the challenges posed by hair. These biases are especially prevalent in individuals with darker skin tones or excessive hair. It is crucial to incorporate feature fusion and assimilate insights from various datasets to gain a more comprehensive understanding of the factors that influence skin health, including hair coverage.</p><p id="Par71">Incorporating features linked to hair coverage improves the model&#x02019;s ability to distinguish between skin abnormalities and hair patterns. This targeted inclusion addresses the unique challenges of hair skin cancer datasets, ensuring the model is better equipped to navigate the complexities introduced by the presence of hair. This enhances the accuracy of skin cancer detection and fosters fairness by minimizing biases associated with hair coverage.</p><p id="Par72">Furthermore, the diverse features introduced through feature fusion help alleviate biases tied to demographic factors and environmental conditions, fostering a more inclusive and equitable model. The model&#x02019;s ability to adapt to a broad range of features, including those associated with hair, contributes to a fair and reliable skin cancer detection framework that accommodates the diverse characteristics of individuals, regardless of hair coverage or skin type. Feature fusion is a powerful tool to counter biases and enhance fairness in hair and skin cancer datasets by enriching the model&#x02019;s knowledge with a comprehensive set of pertinent features.</p></sec></sec><sec id="Sec13"><title>Limitations and future directions</title><p id="Par73">The implementation of the SWNet model for skin cancer detection faces several limitations that need to be addressed: <list list-type="order"><list-item><p id="Par74">Data Storage and Management: Handling a large set of high-quality or large-sized images poses significant challenges in terms of loading, storing, and processing the data efficiently.</p></list-item><list-item><p id="Par75">Optimization Complexity: As data size increases, optimizing model parameters, such as the learning rate and batch size, becomes increasingly complex, requiring meticulous fine-tuning to achieve optimal performance.</p></list-item><list-item><p id="Par76">Dependency on Preprocessing Techniques: The performance of the SWNet model is highly dependent on the effectiveness of the preprocessing techniques used to extract features. Limitations in these techniques can adversely affect the model&#x02019;s overall performance.</p></list-item></list></p><p id="Par77">To overcome these challenges and further advance the capabilities of the SWNet model, several future research directions can be pursued: <list list-type="order"><list-item><p id="Par78">Utilization of Real-World Clinical Datasets: Future research should prioritize leveraging datasets from real-world clinical settings to better evaluate the model&#x02019;s performance and robustness. This would provide valuable insights into the model&#x02019;s applicability in practical scenarios.</p></list-item><list-item><p id="Par79">Exploration of Advanced Features and Techniques: Investigating additional handcrafted features or integrating other deep learning techniques can further enhance the model&#x02019;s performance. This includes experimenting with alternative architectures and feature extraction methods.</p></list-item><list-item><p id="Par80">Addressing Underrepresented Classes: Developing more advanced techniques to improve the model&#x02019;s performance on underrepresented classes is crucial for achieving balanced and accurate detection.</p></list-item><list-item><p id="Par81">Real-Time Clinical Deployment: Developing real-time skin cancer detection systems optimized for speed and efficiency, without compromising accuracy, would enable their deployment in clinical settings, offering practical benefits to healthcare providers and patients.</p></list-item><list-item><p id="Par82">Longitudinal Performance Evaluation: Conducting longitudinal studies to assess the model&#x02019;s performance over time and its ability to adapt to new data can provide insights into its long-term reliability and effectiveness in clinical practice.</p></list-item></list></p></sec><sec id="Sec14"><title>Conclusions</title><p id="Par83">We introduced SWNet, a novel convolutional neural network designed for the automated classification of skin cancer into benign and malignant categories. SWNet&#x02019;s architecture strategically enhances network width, offering significant advantages without escalating computational costs. Feature fusion was incorporated during model training on a public dataset, effectively addressing potential biases associated with skin conditions, particularly in individuals with darker skin tones or excessive hair.</p><p id="Par84">The extracted features were used to train the SoftMax classifier layer. To improve interpretability, we integrated explainable artificial intelligence (XAI) methodologies, specifically Grad-CAM, to identify salient features, generate interpretations of predictions, and assess the model&#x02019;s reliability. This approach provided valuable insights into the decision-making process, fostered a deeper understanding, and enhanced confidence in the model&#x02019;s outputs.</p><p id="Par85">Comparative analysis with state-of-the-art networks, including EfficientNet, MobileNet, and Darknet, pre-trained on the ImageNet dataset, demonstrated SWNet&#x02019;s superiority. The model achieved an accuracy of 99.86% and an F1 score of 99.95%, surpassing these benchmarks. Furthermore, SWNet&#x02019;s ability to classify normal and abnormal classes and its integration of feature fusion to mitigate biases reinforce its robustness and reliability in addressing diverse skin conditions.</p></sec></body><back><fn-group><fn><p><bold>Publisher's Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>The authors would like to thank the Queensland University of Technology for supporting the project.</p></ack><notes notes-type="author-contribution"><title>Authors' contributions</title><p>The authors contributed equally to this work. All authors reviewed the manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>The authors would like to acknowledge the support received through the following funding schemes of the Australian Government: Australian Research Council (ARC) Industrial Transformation Training Centre (ITTC) for Joint Biomechanics under Grant IC190100020.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>We have used public datasets as follows:</p><p>1-Mnist-ham10000 Dataset: <ext-link ext-link-type="uri" xlink:href="https://kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000">https://kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000</ext-link>.</p><p>2- ISIC2019 Dataset: <ext-link ext-link-type="uri" xlink:href="https://kaggle.com/datasets/nodoubttome/skin-cancer9-classesisic">https://kaggle.com/datasets/nodoubttome/skin-cancer9-classesisic</ext-link>.</p><p>3- ISIC 2020 Dataset: <ext-link ext-link-type="uri" xlink:href="https://kaggle.com/datasets/qikangdeng/isic-2019-and-2020-melanoma-dataset">https://kaggle.com/datasets/qikangdeng/isic-2019-and-2020-melanoma-dataset</ext-link>.</p><p>4- Melanoma Skin Cancer Dataset: <ext-link ext-link-type="uri" xlink:href="https://kaggle.com/datasets/hasnainjaved/melanoma-skin-cancer-dataset-of-10000-images.">https://kaggle.com/datasets/hasnainjaved/melanoma-skin-cancer-dataset-of-10000-images.</ext-link></p></notes><notes><title>Declarations</title><notes id="FPar1"><title>Ethics approval and consent to participate</title><p id="Par86">We have used public datasets as follows: </p><p id="Par87">&#x02013; Mnist-ham10000 Dataset: <ext-link ext-link-type="uri" xlink:href="https://kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000">https://kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000</ext-link>. </p><p id="Par88">&#x02013; ISIC2019 Dataset: <ext-link ext-link-type="uri" xlink:href="https://kaggle.com/datasets/nodoubttome/skin-cancer9-classesisic">https://kaggle.com/datasets/nodoubttome/skin-cancer9-classesisic</ext-link>. </p><p id="Par89">&#x02013; ISIC 2020 Dataset: <ext-link ext-link-type="uri" xlink:href="https://kaggle.com/datasets/qikangdeng/isic-2019-and-2020-melanoma-dataset">https://kaggle.com/datasets/qikangdeng/isic-2019-and-2020-melanoma-dataset</ext-link>. </p><p id="Par90">&#x02013; Melanoma Skin Cancer Dataset: https://kaggle.com/datasets/hasnainjaved/melanoma-skin-cancer-dataset-of-10000-images.</p></notes><notes id="FPar2"><title>Consent for publication</title><p id="Par91">Not applicable.</p></notes><notes id="FPar3" notes-type="COI-statement"><title>Competing interests</title><p id="Par92">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Nikolouzakis</surname><given-names>TK</given-names></name><name><surname>Falzone</surname><given-names>L</given-names></name><name><surname>Lasithiotakis</surname><given-names>K</given-names></name><name><surname>Kr&#x000fc;ger-Krasagakis</surname><given-names>S</given-names></name><name><surname>Kalogeraki</surname><given-names>A</given-names></name><name><surname>Sifaki</surname><given-names>M</given-names></name><name><surname>Spandidos</surname><given-names>DA</given-names></name><name><surname>Chrysos</surname><given-names>E</given-names></name><name><surname>Tsatsakis</surname><given-names>A</given-names></name><name><surname>Tsiaoussis</surname><given-names>J</given-names></name></person-group><article-title>Current and future trends in molecular biomarkers for diagnostic, prognostic, and predictive purposes in non-melanoma skin cancer</article-title><source>J Clin Med.</source><year>2020</year><volume>9</volume><issue>9</issue><fpage>2868</fpage><pub-id pub-id-type="doi">10.3390/jcm9092868</pub-id><pub-id pub-id-type="pmid">32899768</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Nikolouzakis TK, Falzone L, Lasithiotakis K, Kr&#x000fc;ger-Krasagakis S, Kalogeraki A, Sifaki M, Spandidos DA, Chrysos E, Tsatsakis A, Tsiaoussis J. Current and future trends in molecular biomarkers for diagnostic, prognostic, and predictive purposes in non-melanoma skin cancer. J Clin Med. 2020;9(9):2868. 10.3390/jcm9092868.<pub-id pub-id-type="pmid">32899768</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Thanh</surname><given-names>DNH</given-names></name><name><surname>Prasath</surname><given-names>VBS</given-names></name><name><surname>Hieu</surname><given-names>LM</given-names></name><name><surname>Hien</surname><given-names>NN</given-names></name></person-group><article-title>Melanoma skin cancer detection method based on adaptive principal curvature, colour normalisation and feature extraction with the ABCD rule</article-title><source>J Digit Imaging.</source><year>2020</year><volume>33</volume><issue>3</issue><fpage>574</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1007/s10278-019-00316-x</pub-id><pub-id pub-id-type="pmid">31848895</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Thanh DNH, Prasath VBS, Hieu LM, Hien NN. Melanoma skin cancer detection method based on adaptive principal curvature, colour normalisation and feature extraction with the ABCD rule. J Digit Imaging. 2020;33(3):574&#x02013;85. 10.1007/s10278-019-00316-x.<pub-id pub-id-type="pmid">31848895</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Lawson DH, Lee SJ, Tarhini AA, Margolin KA, Ernstoff MS, Kirkwood JM. E4697: Phase III cooperative group study of yeast-derived granulocyte macrophage colony-stimulating factor (GM-CSF) versus placebo as adjuvant treatment of patients with completely resected stage III-IV melanoma. J Clin Oncol. 2010;28(15_suppl):8504.</mixed-citation></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Shanthi</surname><given-names>D</given-names></name><name><surname>Maheshvari</surname><given-names>PN</given-names></name><name><surname>Sankarie</surname><given-names>SS</given-names></name></person-group><article-title>Survey on Detection of melanoma skin cancer using image processing and machine learning</article-title><source>Int J Res Anal Rev.</source><year>2020</year><volume>7</volume><issue>1</issue><fpage>237</fpage><lpage>248</lpage></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Shanthi D, Maheshvari PN, Sankarie SS. Survey on Detection of melanoma skin cancer using image processing and machine learning. Int J Res Anal Rev. 2020;7(1):237&#x02013;48.</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Naeem</surname><given-names>A</given-names></name><etal/></person-group><article-title>SNCNet: Skin Cancer Detection by Integrating Handcrafted and Deep Learning-Based Features Using Dermoscopy Images</article-title><source>Mathematics</source><year>2024</year><volume>12</volume><issue>7</issue><fpage>1030</fpage><pub-id pub-id-type="doi">10.3390/math12071030</pub-id></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Naeem A, et al. SNCNet: Skin Cancer Detection by Integrating Handcrafted and Deep Learning-Based Features Using Dermoscopy Images. Mathematics. 2024;12(7):1030.</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Naeem</surname><given-names>A</given-names></name><name><surname>Tayyaba</surname><given-names>A</given-names></name></person-group><article-title>DVFNet: A deep feature fusion-based model for the multiclassification of skin cancer utilizing dermoscopy images</article-title><source>PLoS ONE</source><year>2024</year><volume>19</volume><issue>3</issue><fpage>e0297667</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0297667</pub-id><pub-id pub-id-type="pmid">38507348</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Naeem A, Tayyaba A. DVFNet: A deep feature fusion-based model for the multiclassification of skin cancer utilizing dermoscopy images. PLoS ONE. 2024;19(3):e0297667.<pub-id pub-id-type="pmid">38507348</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Riaz</surname><given-names>S</given-names></name><etal/></person-group><article-title>Federated and Transfer Learning Methods for the Classification of Melanoma and Nonmelanoma Skin Cancers: A Prospective Study</article-title><source>Sensors</source><year>2023</year><volume>23</volume><issue>20</issue><fpage>8457</fpage><pub-id pub-id-type="doi">10.3390/s23208457</pub-id><pub-id pub-id-type="pmid">37896548</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Riaz S, et al. Federated and Transfer Learning Methods for the Classification of Melanoma and Nonmelanoma Skin Cancers: A Prospective Study. Sensors. 2023;23(20):8457.<pub-id pub-id-type="pmid">37896548</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Naeem</surname><given-names>A</given-names></name><etal/></person-group><article-title>SCDNet: a deep learning-based framework for the multiclassification of skin cancer using dermoscopy images</article-title><source>Sensors</source><year>2022</year><volume>22</volume><issue>15</issue><fpage>5652</fpage><pub-id pub-id-type="doi">10.3390/s22155652</pub-id><pub-id pub-id-type="pmid">35957209</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Naeem A, et al. SCDNet: a deep learning-based framework for the multiclassification of skin cancer using dermoscopy images. Sensors. 2022;22(15):5652.<pub-id pub-id-type="pmid">35957209</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Naeem</surname><given-names>A</given-names></name><name><surname>Tayyaba</surname><given-names>A</given-names></name></person-group><article-title>A Multiclassification Framework for Skin Cancer detection by the concatenation of Xception and ResNet101</article-title><source>J Comput Biomed Inform</source><year>2024</year><volume>6</volume><issue>02</issue><fpage>205</fpage><lpage>227</lpage></element-citation><mixed-citation id="mc-CR9" publication-type="journal">Naeem A, Tayyaba A. A Multiclassification Framework for Skin Cancer detection by the concatenation of Xception and ResNet101. J Comput Biomed Inform. 2024;6(02):205&#x02013;27.</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Naeem</surname><given-names>A</given-names></name><etal/></person-group><article-title>Predicting the Metastasis Ability of Prostate Cancer using Machine Learning Classifiers</article-title><source>J Comput Biomed Inform</source><year>2023</year><volume>4</volume><issue>02</issue><fpage>1</fpage><lpage>7</lpage></element-citation><mixed-citation id="mc-CR10" publication-type="journal">Naeem A, et al. Predicting the Metastasis Ability of Prostate Cancer using Machine Learning Classifiers. J Comput Biomed Inform. 2023;4(02):1&#x02013;7.</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Ayesha</surname><given-names>H</given-names></name><etal/></person-group><article-title>Multi-classification of Skin Cancer Using Multi-model Fusion Technique</article-title><source>J Comput iomed Inform</source><year>2023</year><volume>5</volume><issue>02</issue><fpage>195</fpage><lpage>219</lpage></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Ayesha H, et al. Multi-classification of Skin Cancer Using Multi-model Fusion Technique. J Comput iomed Inform. 2023;5(02):195&#x02013;219.</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Titano</surname><given-names>JJ</given-names></name><name><surname>Badgeley</surname><given-names>M</given-names></name><name><surname>Schefflein</surname><given-names>J</given-names></name><name><surname>Pain</surname><given-names>M</given-names></name><name><surname>Su</surname><given-names>A</given-names></name><name><surname>Cai</surname><given-names>M</given-names></name><name><surname>Swinburne</surname><given-names>N</given-names></name><etal/></person-group><article-title>Automated deep-neural-network surveillance of cranial images for acute neurologic events</article-title><source>Nat Med.</source><year>2018</year><volume>24</volume><issue>9</issue><fpage>1337</fpage><lpage>1341</lpage><pub-id pub-id-type="doi">10.1038/s41591-018-0147-y</pub-id><pub-id pub-id-type="pmid">30104767</pub-id>
</element-citation><mixed-citation id="mc-CR12" publication-type="journal">Titano JJ, Badgeley M, Schefflein J, Pain M, Su A, Cai M, Swinburne N, et al. Automated deep-neural-network surveillance of cranial images for acute neurologic events. Nat Med. 2018;24(9):1337&#x02013;41.<pub-id pub-id-type="pmid">30104767</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Tufail AB, Ma YK, Kaabar MK, Mart&#x000ed;nez F, Junejo AR, Ullah I, Khan R. Deep learning in cancer diagnosis and prognosis prediction: a minireview on challenges, recent trends, and future directions. Comput Math Methods Med. 2021(1):9025470.</mixed-citation></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Yanase</surname><given-names>J</given-names></name><name><surname>Triantaphyllou</surname><given-names>E</given-names></name></person-group><article-title>A systematic survey of computer-aided diagnosis in medicine: Past and present developments</article-title><source>Expert Syst Appl.</source><year>2019</year><volume>138</volume><fpage>112821</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2019.112821</pub-id></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Yanase J, Triantaphyllou E. A systematic survey of computer-aided diagnosis in medicine: Past and present developments. Expert Syst Appl. 2019;138:112821.</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Sufyan M, Shokat Z, Ashfaq UA. Artificial intelligence in cancer diagnosis and therapy: Current status and future perspective. Comput Biol Med. 2023;165:107356.</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Pham HN, Koay CY, Chakraborty S, Gupta S, Tan BL, Wu H, Vardhan A, Nguyen QH, Palaparthi NR, Nguyen BP, Chua MCH. Lesion segmentation and automated melanoma detection using deep convolutional neural networks and XGBoost. Proceedings of the 2019 International Conference on System Science and Engineering (ICSSE), 2019. 10.1109/ICSSE.2019.8823129.</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Demir A, Yilmaz F, Kose O. Early detection of skin cancer using deep learning architectures: resnet-101 and inception-v3. Proceedings of the 2019 Medical Technologies Congress (TIPTEKNO), 2019. 10.1109/TIPTEKNO47231.2019.8972045.</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Emara T, Afify HM, Ismail FH, Hassanien AE. A modified inception-v4 for imbalanced skin cancer classification dataset. 2019 14th International Conference on Computer Engineering and Systems (ICCES). IEEE. pp. 28-33.</mixed-citation></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Vasconcelos</surname><given-names>CN</given-names></name><name><surname>Vasconcelos</surname><given-names>BN</given-names></name></person-group><article-title>Experiments using deep learning for dermoscopy image analysis</article-title><source>Pattern Recogn Lett.</source><year>2020</year><volume>139</volume><fpage>95</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2017.11.005</pub-id></element-citation><mixed-citation id="mc-CR19" publication-type="journal">Vasconcelos CN, Vasconcelos BN. Experiments using deep learning for dermoscopy image analysis. Pattern Recogn Lett. 2020;139:95&#x02013;103. 10.1016/j.patrec.2017.11.005.</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Kassem</surname><given-names>MA</given-names></name><name><surname>Hosny</surname><given-names>KM</given-names></name><name><surname>Fouad</surname><given-names>MM</given-names></name></person-group><article-title>Skin lesions classification into eight classes for 2019 using deep convolutional neural network and transfer learning</article-title><source>IEEE Access.</source><year>2020</year><volume>8</volume><fpage>114822</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1109/access.2020.3003890</pub-id></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Kassem MA, Hosny KM, Fouad MM. Skin lesions classification into eight classes for 2019 using deep convolutional neural network and transfer learning. IEEE Access. 2020;8:114822&#x02013;32. 10.1109/access.2020.3003890.</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Hosny</surname><given-names>KM</given-names></name><name><surname>Kassem</surname><given-names>MA</given-names></name><name><surname>Fouad</surname><given-names>MM</given-names></name></person-group><article-title>Skin melanoma classification using ROI and data augmentation with deep convolutional neural networks</article-title><source>Multimed Tools Appl.</source><year>2020</year><volume>79</volume><fpage>24029</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1007/s11042-020-09067-2</pub-id></element-citation><mixed-citation id="mc-CR21" publication-type="journal">Hosny KM, Kassem MA, Fouad MM. Skin melanoma classification using ROI and data augmentation with deep convolutional neural networks. Multimed Tools Appl. 2020;79:24029&#x02013;55. 10.1007/s11042-020-09067-2.</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Diame ZE, Al-B MN, Salem MA, Roushdy M. Deep learning architecture lectures for aided melanoma skin disease recognition: a review. Proceedings of the 2021 International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC), 2021. 10.1109/MIUCC52538.2021.9447615.</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Ahmadi Mehr R, Ameri A. Skin Cancer Detection Based on Deep Learning. J Biomed Phys Eng. 2022;12(6):559-568. 10.31661/jbpe.v0i0.2207-1517.</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Li Z. A classification method for multi-class skin damage images combining quantum computing and Inception-ResNet-V1. Front Phys. 2022. 10.3389/fphy.2022.1046314.</mixed-citation></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>J</given-names></name><name><surname>Mridha</surname><given-names>K</given-names></name><name><surname>Uddin</surname><given-names>MM</given-names></name><name><surname>Khadka</surname><given-names>S</given-names></name><name><surname>Mridha</surname><given-names>MF</given-names></name></person-group><article-title>An Interpretable Skin Cancer Classification Using Optimized Convolutional Neural Network for a Smart Healthcare System</article-title><source>IEEE Access.</source><year>2023</year><volume>11</volume><fpage>41003</fpage><lpage>41018</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3269694</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Shin J, Mridha K, Uddin MM, Khadka S, Mridha MF. An Interpretable Skin Cancer Classification Using Optimized Convolutional Neural Network for a Smart Healthcare System. IEEE Access. 2023;11:41003&#x02013;18. 10.1109/ACCESS.2023.3269694.</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Thanka</surname><given-names>MR</given-names></name><etal/></person-group><article-title>A hybrid approach for melanoma classification using ensemble machine learning techniques with deep transfer learning</article-title><source>Comput Methods Programs Biomed Updat.</source><year>2023</year><volume>3</volume><fpage>100103</fpage><pub-id pub-id-type="doi">10.1016/j.cmpbup.2023.100103</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Thanka MR, et al. A hybrid approach for melanoma classification using ensemble machine learning techniques with deep transfer learning. Comput Methods Programs Biomed Updat. 2023;3:100103.</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Ta&#x0015f;ar, B. SkinCancerNet: Automated classification of skin lesion using deep transfer learning method. Traitement Signal. 2023;40(1):285-95. 10.18280/ts.400128.</mixed-citation></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Tahir</surname><given-names>M</given-names></name><name><surname>Naeem</surname><given-names>A</given-names></name><name><surname>Malik</surname><given-names>H</given-names></name><name><surname>Tanveer</surname><given-names>J</given-names></name><name><surname>Naqvi</surname><given-names>RA</given-names></name><name><surname>Lee</surname><given-names>S-W</given-names></name></person-group><article-title>DSCC\_Net: Multi-Classification Deep Learning Models for Diagnosing of Skin Cancer Using Dermoscopic Images</article-title><source>Cancers.</source><year>2023</year><volume>15</volume><fpage>2179</fpage><pub-id pub-id-type="doi">10.3390/cancers15072179</pub-id><pub-id pub-id-type="pmid">37046840</pub-id>
</element-citation><mixed-citation id="mc-CR28" publication-type="journal">Tahir M, Naeem A, Malik H, Tanveer J, Naqvi RA, Lee S-W. DSCC_Net: Multi-Classification Deep Learning Models for Diagnosing of Skin Cancer Using Dermoscopic Images. Cancers. 2023;15:2179. 10.3390/cancers15072179.<pub-id pub-id-type="pmid">37046840</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Qasim Gilani</surname><given-names>S</given-names></name><name><surname>Syed</surname><given-names>T</given-names></name><name><surname>Umair</surname><given-names>M</given-names></name><name><surname>Marques</surname><given-names>O</given-names></name></person-group><article-title>Skin Cancer Classification Using Deep Spiking Neural Network</article-title><source>J Digit Imaging.</source><year>2023</year><volume>36</volume><issue>3</issue><fpage>1137</fpage><lpage>1147</lpage><pub-id pub-id-type="doi">10.1007/s10278-023-00776-2</pub-id><pub-id pub-id-type="pmid">36690775</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Qasim Gilani S, Syed T, Umair M, Marques O. Skin Cancer Classification Using Deep Spiking Neural Network. J Digit Imaging. 2023;36(3):1137&#x02013;47. 10.1007/s10278-023-00776-2.<pub-id pub-id-type="pmid">36690775</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Waheed S, Saadi S, Shafry M, Rahim M, Suaib N, Najjar F, Mundher M, Salim A, Bahru J. Melanoma Skin Cancer Classification based on CNN Deep Learning Algorithms. Malays J Fundam Appl Sci. 2023;19:299-305. 10.11113/mjfas.v19n3.2900.</mixed-citation></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Dahdouh</surname><given-names>Y</given-names></name><name><surname>Anouar Boudhir</surname><given-names>A</given-names></name><name><surname>Ahmed</surname><given-names>MB</given-names></name></person-group><article-title>A New Approach using Deep Learning and Reinforcement Learning in HealthCare: Skin Cancer Classification</article-title><source>Int J Electr Comput Eng Syst.</source><year>2023</year><volume>14</volume><issue>5</issue><fpage>557</fpage><lpage>564</lpage></element-citation><mixed-citation id="mc-CR31" publication-type="journal">Dahdouh Y, Anouar Boudhir A, Ahmed MB. A New Approach using Deep Learning and Reinforcement Learning in HealthCare: Skin Cancer Classification. Int J Electr Comput Eng Syst. 2023;14(5):557&#x02013;64.</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Ahmed IMB, Maalej R, Kherallah M. MobileNet-Based Model for Histopathological Breast Cancer Image Classification. International Conference on Hybrid Intelligent Systems. Cham: Springer Nature Switzerland; 2022.</mixed-citation></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Alzubaidi</surname><given-names>L</given-names></name><name><surname>Bai</surname><given-names>J</given-names></name><name><surname>Al-Sabaawi</surname><given-names>A</given-names></name><name><surname>Santamar&#x000ed;a</surname><given-names>J</given-names></name><name><surname>Albahri</surname><given-names>AS</given-names></name><name><surname>Al-Dabbagh</surname><given-names>BSN</given-names></name><name><surname>Fadhel</surname><given-names>MA</given-names></name><etal/></person-group><article-title>A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications</article-title><source>J Big Data.</source><year>2023</year><volume>10</volume><issue>1</issue><fpage>46</fpage><pub-id pub-id-type="doi">10.1186/s40537-023-00727-2</pub-id></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Alzubaidi L, Bai J, Al-Sabaawi A, Santamar&#x000ed;a J, Albahri AS, Al-Dabbagh BSN, Fadhel MA, et al. A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications. J Big Data. 2023;10(1):46.</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Alzubaidi L, Khamael AL-D, Salhi A, Alammar Z, Fadhel MA, Albahri AS, Alamoodi AH, et al. Comprehensive review of deep learning in orthopaedics: Applications, challenges, trustworthiness, and fusion. Artif Intell Med 2024;155:102935.</mixed-citation></ref><ref id="CR35"><label>35.</label><citation-alternatives><element-citation id="ec-CR35" publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature.</source><year>2015</year><volume>521</volume><issue>7553</issue><fpage>436</fpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id>
</element-citation><mixed-citation id="mc-CR35" publication-type="journal">LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436.<pub-id pub-id-type="pmid">26017442</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Rabinovich A. Going deeper with convolutions. Proc IEEE Conf Comput Vis Pattern Recognit. 2015:1&#x02013;9.</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, Adam H. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1704.04861">arXiv:1704.04861</ext-link>.</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems. 2012:1097&#x02013;105.</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.1556">arXiv:1409.1556</ext-link>.</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. Proc IEEE Conf Comput Vis Pattern Recognit. 2016:770&#x02013;8.</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Huang G, Liu Z, Van Der Maaten L, Weinberger KQ. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2017. p. 4700&#x02013;8.</mixed-citation></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Berg</surname><given-names>AC</given-names></name></person-group><article-title>Imagenet large scale visual recognition challenge</article-title><source>Int J Comput Vis.</source><year>2015</year><volume>115</volume><issue>3</issue><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation><mixed-citation id="mc-CR42" publication-type="journal">Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, Berg AC. Imagenet large scale visual recognition challenge. Int J Comput Vis. 2015;115(3):211&#x02013;52.</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name><surname>Badgujar</surname><given-names>CM</given-names></name><name><surname>Armstrong</surname><given-names>PR</given-names></name><name><surname>Gerken</surname><given-names>AR</given-names></name><name><surname>Pordesimo</surname><given-names>LO</given-names></name><name><surname>Campbell</surname><given-names>JF</given-names></name></person-group><article-title>Identifying common stored product insects using automated deep learning methods</article-title><source>J Stored Prod Res.</source><year>2023</year><volume>103</volume><fpage>102166</fpage><pub-id pub-id-type="doi">10.1016/j.jspr.2023.102166</pub-id></element-citation><mixed-citation id="mc-CR43" publication-type="journal">Badgujar CM, Armstrong PR, Gerken AR, Pordesimo LO, Campbell JF. Identifying common stored product insects using automated deep learning methods. J Stored Prod Res. 2023;103:102166.</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Gibiansky A. Convolutional neural networks. <ext-link ext-link-type="uri" xlink:href="http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/">http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/</ext-link>. Accessed 26 Dec 2018.</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Nawaz W, Ahmed S, Tahir A, Khan HA. Classification of breast Cancer histology images using AlexNet. International Conference on Image Analysis and Recognition. Cham: Springer; 2018. pp. 869&#x02013;876.</mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Dahl GE, Sainath TN, Hinton GE. Improving deep neural networks for LVCSR using rectified linear units and dropout. In 2013 IEEE international conference on acoustics, speech and signal processing. IEEE; 2013. p. 8609&#x02013;13.</mixed-citation></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Taye</surname><given-names>MM</given-names></name></person-group><article-title>Theoretical understanding of convolutional neural network: concepts, architectures, applications, future directions</article-title><source>Computation.</source><year>2023</year><volume>11</volume><issue>3</issue><fpage>52</fpage><pub-id pub-id-type="doi">10.3390/computation11030052</pub-id></element-citation><mixed-citation id="mc-CR47" publication-type="journal">Taye MM. Theoretical understanding of convolutional neural network: concepts, architectures, applications, future directions. Computation. 2023;11(3):52.</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Xu Y, Wang Z, Wang Z, Guo YL, Fan R, Tian HY, Wang X. SimDCL: dropout-based simple graph contrastive learning for recommendation. Complex Intell Syst. 2023:1&#x02013;13.</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Alzubaidi L, Jebur SA, Jaber TA, Mohammed MA, Alwzwazy HA, Saihood A, Gammulle H, Santamaria J, Duan Y, Fookes C, Jurdak R. ATD Learning: A secure, smart, and decentralised learning method for big data environments. Inf Fusion. 2025:102953.</mixed-citation></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name><surname>Saihood</surname><given-names>AA</given-names></name><name><surname>Hasan</surname><given-names>MA</given-names></name><name><surname>Fadhel</surname><given-names>MA</given-names></name><name><surname>Alzubaid</surname><given-names>L</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Gu</surname><given-names>Y</given-names></name></person-group><article-title>Multiside graph neural network-based attention for local co-occurrence features fusion in lung nodule classification</article-title><source>Expert Syst Appl.</source><year>2024</year><volume>252</volume><fpage>124149</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2024.124149</pub-id></element-citation><mixed-citation id="mc-CR50" publication-type="journal">Saihood AA, Hasan MA, Fadhel MA, Alzubaid L, Gupta A, Gu Y. Multiside graph neural network-based attention for local co-occurrence features fusion in lung nodule classification. Expert Syst Appl. 2024;252:124149.</mixed-citation></citation-alternatives></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Alzubaidi L, Khamael AL-D, Obeed HA-H, Saihood A, Fadhel MA, Jebur SA, Chen Y, et al. MEFF-A model ensemble feature fusion approach for tackling adversarial attacks in medical imaging. Intell Syst Appl. 2024;22:200355.</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Poonia A, Sharma VK, Singh HK, Maheshwari S. Heuristically optimized weighted feature fusion with adaptive cascaded deep network: a novel breast cancer detection framework using mammogram images. Comput Methods Biomech Biomed Eng Imaging Vis. 2024;11(7):2278908.</mixed-citation></ref><ref id="CR53"><label>53.</label><citation-alternatives><element-citation id="ec-CR53" publication-type="journal"><person-group person-group-type="author"><name><surname>Albahri</surname><given-names>AS</given-names></name><name><surname>Duhaim</surname><given-names>AM</given-names></name><name><surname>Fadhel</surname><given-names>MA</given-names></name><name><surname>Alnoor</surname><given-names>A</given-names></name><name><surname>Baqer</surname><given-names>NS</given-names></name><name><surname>Alzubaidi</surname><given-names>L</given-names></name><name><surname>Albahri</surname><given-names>OS</given-names></name><etal/></person-group><article-title>A systematic review of trustworthy and explainable artificial intelligence in healthcare: Assessment of quality, bias risk, and data fusion</article-title><source>Inf Fusion.</source><year>2023</year><volume>96</volume><fpage>156</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2023.03.008</pub-id></element-citation><mixed-citation id="mc-CR53" publication-type="journal">Albahri AS, Duhaim AM, Fadhel MA, Alnoor A, Baqer NS, Alzubaidi L, Albahri OS, et al. A systematic review of trustworthy and explainable artificial intelligence in healthcare: Assessment of quality, bias risk, and data fusion. Inf Fusion. 2023;96:156&#x02013;91. 10.1016/j.inffus.2023.03.008.</mixed-citation></citation-alternatives></ref><ref id="CR54"><label>54.</label><citation-alternatives><element-citation id="ec-CR54" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Ogasawara</surname><given-names>K</given-names></name></person-group><article-title>Grad-CAM-based explainable artificial intelligence related to medical text processing</article-title><source>Bioengineering.</source><year>2023</year><volume>10</volume><issue>9</issue><fpage>1070</fpage><pub-id pub-id-type="doi">10.3390/bioengineering10091070</pub-id><pub-id pub-id-type="pmid">37760173</pub-id>
</element-citation><mixed-citation id="mc-CR54" publication-type="journal">Zhang H, Ogasawara K. Grad-CAM-based explainable artificial intelligence related to medical text processing. Bioengineering. 2023;10(9):1070.<pub-id pub-id-type="pmid">37760173</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR55"><label>55.</label><citation-alternatives><element-citation id="ec-CR55" publication-type="journal"><person-group person-group-type="author"><name><surname>Al-Hamadani</surname><given-names>MNA</given-names></name><name><surname>Fadhel</surname><given-names>MA</given-names></name><name><surname>Alzubaidi</surname><given-names>L</given-names></name><name><surname>Harangi</surname><given-names>B</given-names></name></person-group><article-title>Reinforcement Learning Algorithms and Applications in Healthcare and Robotics: A Comprehensive and Systematic Review</article-title><source>Sensors.</source><year>2024</year><volume>24</volume><issue>8</issue><fpage>2461</fpage><pub-id pub-id-type="doi">10.3390/s24082461</pub-id><pub-id pub-id-type="pmid">38676080</pub-id>
</element-citation><mixed-citation id="mc-CR55" publication-type="journal">Al-Hamadani MNA, Fadhel MA, Alzubaidi L, Harangi B. Reinforcement Learning Algorithms and Applications in Healthcare and Robotics: A Comprehensive and Systematic Review. Sensors. 2024;24(8):2461.<pub-id pub-id-type="pmid">38676080</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Fadhel MA, Alzubaidi L, Gu Y, Santamar&#x000ed;a J, Duan Y. Real-time diabetic foot ulcer classification based on deep learning &#x00026; parallel hardware computational tools. Multimedia Tools Appl. 2024:1&#x02013;26.</mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Mnist-ham10000 Dataset. https://kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000. Accessed 20 Jan 2024.</mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">ISIC2019 Dataset. https://kaggle.com/datasets/nodoubttome/skin-cancer9-classesisic. Accessed 20 Jan 2024.</mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">ISIC 2020 Dataset. https://kaggle.com/datasets/qikangdeng/isic-2019-and-2020-melanoma-dataset. Accessed 24 Jan 2024.</mixed-citation></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="other">Melanoma Skin Cancer Dataset. https://kaggle.com/datasets/hasnainjaved/melanoma-skin-cancer-dataset-of-10000-images. Accessed 28 Jan 2024.</mixed-citation></ref></ref-list></back></article>