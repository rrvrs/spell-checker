<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408323</article-id><article-id pub-id-type="pmc">PMC12101650</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0321179</article-id><article-id pub-id-type="publisher-id">PONE-D-24-59614</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Energy and Power</subject><subj-group><subject>Power Distribution</subject><subj-group><subject>Power Grids</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Optimization</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Cell Biology</subject><subj-group><subject>Cellular Types</subject><subj-group><subject>Animal Cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular Neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject><subj-group><subject>Deep Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject><subj-group><subject>Support Vector Machines</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Telecommunications</subject></subj-group></subj-group></article-categories><title-group><article-title>Detecting eavesdropping nodes in the power Internet of Things based on Kolmogorov-Arnold networks</article-title><alt-title alt-title-type="running-head">Detecting eavesdropping nodes in the power Internet of Things based on Kolmogorov-Arnold networks</alt-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Wang</surname><given-names>Rong</given-names></name><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Jiang</surname><given-names>Weibin</given-names></name><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref><xref rid="aff003" ref-type="aff">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Shen</surname><given-names>Yanjin</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0000-1517-0430</contrib-id><name><surname>Yue</surname><given-names>Qiqing</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Hsiung</surname><given-names>Kan-Lin</given-names></name><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff003" ref-type="aff">
<sup>3</sup>
</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Hunan Automotive Engineering Vocational University, Zhuzhou, Hunan, China</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>Department of Electrical Engineering, Yuan Ze University, Taoyuan, Taiwan</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Saif</surname><given-names>Sohail</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Maulana Abul Kalam Azad University of Technology West Bengal, INDIA</addr-line>
</aff><author-notes><corresp id="cor001">* <email>leyue357@163.com</email></corresp><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0321179</elocation-id><history><date date-type="received"><day>30</day><month>12</month><year>2024</year></date><date date-type="accepted"><day>2</day><month>3</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Wang et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Wang et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0321179.pdf"/><abstract><p>The rapid proliferation of the Power Internet of Things (PIoT) has given rise to severe network security threats, with eavesdropping attacks emerging as a paramount concern. Traditional eavesdropping detection methods struggle to adapt to complex and dynamic attack patterns, necessitating the exploration of more intelligent and efficient anomaly localization approaches. This paper proposes an innovative method for eavesdropping node localization based on Kolmogorov-Arnold Networks (KANs). Leveraging the powerful ability of KANs to approximate arbitrary nonlinear functions, this method constructs an end-to-end mapping from heterogeneous node features to eavesdropping locations through flexible combinations of spline functions. To address the challenges of real-world power grid environments, this paper designs optimization strategies such as adaptive grid refinement and hierarchical sparsity regularization, further enhancing the model&#x02019;s robustness and interpretability. Extensive simulations and experiments on real power grid data demonstrate that the proposed method significantly outperforms traditional machine learning and mainstream deep learning approaches in terms of localization accuracy, generalization ability, and computational efficiency. This paper provides new perspectives and tools for intelligent power grid information security in IoT environments, holding significant innovative value in both theory and practice.</p></abstract><funding-group><award-group id="award001"><funding-source><institution>Hunan Provincial Natural Science Foundation</institution>
</funding-source><award-id>2023JJ60217</award-id><principal-award-recipient>
<name><surname>Wang</surname><given-names>Rong</given-names></name>
</principal-award-recipient></award-group><funding-statement>The study was supported by the Hunan Provincial Natural Science Foundation under Grant No. 2023JJ60217. The funders provided financial support for the study design and data collection, but had no role in data analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="5"/><table-count count="4"/><page-count count="19"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All dataset and code files are available from the <ext-link xlink:href="https://github.com/fzbingmo/Activate_Eavesdropper" ext-link-type="uri">https://github.com/fzbingmo/Activate_Eavesdropper</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All dataset and code files are available from the <ext-link xlink:href="https://github.com/fzbingmo/Activate_Eavesdropper" ext-link-type="uri">https://github.com/fzbingmo/Activate_Eavesdropper</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>As the development of smart grids and the widespread application of Internet of Things (IoT) technologies continue to advance, ensuring the information security of power systems has become increasingly crucial. Among the numerous security threats, eavesdropping attacks have garnered significant attention due to their stealthiness and potential for harm. Eavesdroppers can deploy illegal eavesdropping nodes in power wireless communication networks to monitor critical facilities and steal data, jeopardizing the secure and stable operation of the power grid, leaking user privacy, and compromising business secrets. Statistics indicate a continuous rise in incidents and economic losses caused by data breaches in recent years, underscoring the imperative of strengthening eavesdropping threat prevention [<xref rid="pone.0321179.ref001" ref-type="bibr">1</xref>].</p><p>The detection and localization of eavesdropping nodes in complex PIoT environments face numerous technical challenges. Firstly, eavesdropping nodes often conceal their true identities and locations, employing communication protocols similar to legitimate nodes, increasing the difficulty of detection [<xref rid="pone.0321179.ref002" ref-type="bibr">2</xref>]. Secondly, power wireless communication networks are large in scale and involve complex environments, imposing higher requirements on the scalability and robustness of localization algorithms [<xref rid="pone.0321179.ref003" ref-type="bibr">3</xref>]. Furthermore, power equipment has limited resources and capabilities, necessitating a balance between localization performance and computational overhead.</p><p>Traditional localization methods primarily rely on geometric models such as ranging and angle measurement, depending on a priori information such as node locations and signal strengths. When eavesdropping nodes actively conceal their own characteristics, the accuracy of these methods significantly decreases [<xref rid="pone.0321179.ref004" ref-type="bibr">4</xref>]. In recent years, machine learning methods have brought new opportunities for solving eavesdropping localization problems in complex environments. Some works employ models such as Support Vector Machines (SVM) [<xref rid="pone.0321179.ref005" ref-type="bibr">5</xref>] and Convolutional Neural Networks (CNN) [<xref rid="pone.0321179.ref006" ref-type="bibr">6</xref>] to enhance the discriminative ability of abnormal features through feature engineering or end-to-end learning. Furthermore, Graph Neural Networks (GNN) capture node correlation patterns more effectively by modeling network topological structures, demonstrating advantages in network anomaly detection [<xref rid="pone.0321179.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0321179.ref008" ref-type="bibr">8</xref>]. However, current research still falls short in characterizing the complexity of eavesdropping behavior and the specificity of power scenarios. Firstly, eavesdropping patterns are stealthy and diverse, with highly nonlinear abnormal distributions, limiting the expressive power of traditional machine learning models. Secondly, PIoT involves a massive number of heterogeneous devices, and the dynamic time-varying characteristics pose higher requirements for model generalization. Moreover, actual deployment has stringent demands for model lightweight and robustness. These challenges urgently call for the exploration of more efficient, flexible, and robust intelligent anomaly localization methods.</p><p>While this paper study primarily focuses on the detection and localization of eavesdropping threats, it&#x02019;s important to acknowledge the critical role of authentication mechanisms in the overarching IoT security ecosystem. As highlighted by Asif <italic toggle="yes">et al</italic>. [<xref rid="pone.0321179.ref010" ref-type="bibr">10</xref>], robust authentication protocols form the first line of defense against unauthorized access to IoT devices and networks. Although a detailed exploration of authentication schemes is beyond the scope of this work, we recognize that integrating this paper&#x02019;s proposed anomaly detection framework with secure authentication measures would provide a more comprehensive solution for PIoT security.</p><p>To address the aforementioned issues, this paper proposes a radically new solution. Inspired by the Kolmogorov-Arnold theorem, this paper designs an eavesdropping localization model based on KAN networks. The core idea of the proposed method is to abstract the eavesdropping localization problem into a complex mapping problem from heterogeneous node features to spatial coordinates. KANs can approximate arbitrary continuous mappings through function superposition, possessing excellent approximation capabilities. Compared to traditional deep learning paradigms, KANs can significantly reduce model parameter scale, thereby improving computational efficiency. Simultaneously, the explicit form of spline functions in KANs provides new perspectives for model behavior interpretability analysis. To the best of the authors&#x02019; knowledge, this is the seminal attempt to apply KANs to eavesdropping localization in PIoT environments and optimize them for pragmatic requirements.</p><p>The main contributions of this paper are summarized as follows:</p><list list-type="bullet"><list-item><p>Propose a graph-based eavesdropping node localization problem modeling method tailored to the characteristics of PIoT, unifying network topology, wireless channels, and multi-source heterogeneous data factors.</p></list-item><list-item><p>Innovatively introduce Kolmogorov-Arnold networks to solve the complex regression problem of eavesdropping localization, achieving breakthroughs in accuracy, efficiency, and interpretability.</p></list-item><list-item><p>Employ strategies such as adaptive grids and sparse regularization to optimize KAN models, demonstrating excellent robustness and generalization performance in adverse communication environments.</p></list-item><list-item><p>Implement KAN deployment on embedded platforms, validating the advantages of model lightweight and low power consumption, showcasing its application value in real power scenarios.</p></list-item></list><p>The remainder of this paper is structured as follows: The Related Work section introduces the research status of eavesdropping localization, the Eavesdropping Node Localization Method Based on KAN section elaborates on the principles and structure of KAN networks, the Training and Optimization of KAN Models section provides algorithm optimization and implementation details, the Experimental Results and Analysis section verifies the effectiveness of the method through simulations and real-world experiments, and the Conclusion section summarizes the paper and outlines future research directions.</p></sec><sec id="sec002"><title>Related work</title><p>The eavesdropping attack localization problem has been a research prominent research focus in the field of wireless communication network security. Traditional methods primarily rely on signal processing and geometric ranging models. For example, the work in [<xref rid="pone.0321179.ref013" ref-type="bibr">13</xref>] employs the Time Difference of Arrival (TDOA) method to estimate the distance between eavesdropping nodes and multiple anchor points, then calculates their coordinates through techniques such as trilateration. The work in [<xref rid="pone.0321179.ref014" ref-type="bibr">14</xref>] utilizes Angle of Arrival (AOA) information to construct hyperbolas with two legitimate nodes as foci, with their intersection being the location of the eavesdropping node. These methods are simple and intuitive but face numerous limitations, such as requiring precise inter-node distances and time synchronization, and being easily affected by channel noise and other interference. Furthermore, eavesdropping nodes can evade detection by falsifying identities, adjusting transmission power, and other means, leading to a significant decrease in localization accuracy [<xref rid="pone.0321179.ref004" ref-type="bibr">4</xref>].</p><p>In recent years, machine learning methods have provided new ideas for solving anomaly localization problems in complex network environments. One category of methods focuses on feature engineering, manually designing effective statistical features, and then employing shallow models such as Support Vector Machines (SVM) and K-Nearest Neighbors (KNN) for classification. For instance, the work in [<xref rid="pone.0321179.ref015" ref-type="bibr">15</xref>] models node communication patterns, extracts features such as packet length and time intervals, and trains an SVM model to detect Sybil attacks. Similarly, the work in [<xref rid="pone.0321179.ref016" ref-type="bibr">16</xref>] employs a hidden Markov model to characterize channel occupancy state transitions and discriminate eavesdropping behavior. These methods overcome the impact of channel uncertainty to a certain extent, but feature engineering relies on prior knowledge and struggles to characterize behavioral patterns in complex networks.</p><p>Another category of methods adopts an end-to-end deep learning paradigm, automatically extracting discriminative features through data-driven approaches. The work in [<xref rid="pone.0321179.ref006" ref-type="bibr">6</xref>] proposes a convolutional neural network model that maps the original signal strength matrix to an eavesdropping probability map, achieving eavesdropping node localization. Inspired by the processing of complex topological structure data by Graph Neural Networks (GNN), some works attempt to apply GNNs to network anomaly detection. The work in [<xref rid="pone.0321179.ref007" ref-type="bibr">7</xref>] employs graph convolutional neural networks to model multi-hop wireless networks, updating node states through message passing to predict Sybil attackers. Furthermore, the work in [<xref rid="pone.0321179.ref008" ref-type="bibr">8</xref>] introduces an attention mechanism into GNNs to adaptively aggregate neighborhood information, improving detection accuracy. However, existing deep learning models generally suffer from large parameter sizes, high computational complexity, and poor interpretability, facing numerous challenges in practical deployment.</p><p>Compared to existing works, this paper proposes a completely new solution from the perspective of function approximation. This paper draws inspiration from the Kolmogorov-Arnold theorem and employs KAN networks to realize complex nonlinear mappings from heterogeneous node features to spatial coordinates. On one hand, KANs inherit the end-to-end learning capabilities of deep learning models without relying on manual feature engineering. On the other hand, each neuron in KANs corresponds to an explicit spline function, significantly reducing parameter quantities and computational overhead while facilitating visual analysis. Additionally, this paper performs targeted optimizations on the KAN model, introducing adaptive grid and sparse regularization strategies, demonstrating excellent robustness under adverse conditions such as time-varying channels and heterogeneous networks.</p><p>It is worth noting that the application of Kolmogorov-Arnold theory in neural network design is still in the exploratory stage. Some early works theoretically prove the universal approximation properties of KANs [<xref rid="pone.0321179.ref017" ref-type="bibr">17</xref>], but there are few cases of applying them to practical wireless network problems. This paper attempts to design and optimize KAN models for the specific task of eavesdropping localization in power wireless communication networks from an engineering practice perspective, verifying their practical value in simulated and real environments, aiming to provide reference and guidance for subsequent research. It is worth noting that eavesdropping attacks in IoT can manifest in various forms, ranging from overt techniques to more stealthy approaches like steganography, as highlighted by Asif <italic toggle="yes">et al</italic>. [<xref rid="pone.0321179.ref011" ref-type="bibr">11</xref>]. While this paper&#x02019;s work primarily focuses on detecting observable eavesdropping patterns, the proposed KAN-based method, with its ability to capture complex nonlinear relationships, has the potential to be extended to uncovering subtle, hidden anomalies. This presents an interesting direction for future research in IoT security.</p></sec><sec id="sec003"><title>Eavesdropping node localization method based on KAN</title><p>This section elaborates on the proposed eavesdropping node localization method based on KAN. Firstly, this paper provides a formal definition of the problem and a graph model representation. Then, this paper introduces the basic principles and structure of KAN networks. Finally, this paper discusses the motivation and advantages of applying KANs to the eavesdropping localization task.</p><sec id="sec004"><title>Problem definition and graph model representation</title><p>As shown in <xref rid="pone.0321179.g001" ref-type="fig">Fig 1</xref>, this paper considers a power wireless communication network with <italic toggle="yes">N</italic> nodes, and the topological structure can be represented as an undirected graph <inline-formula id="pone.0321179.e001"><alternatives><graphic xlink:href="pone.0321179.e001.jpg" id="pone.0321179.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x1d4a2;</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x1d4b1;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x02130;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pone.0321179.e002"><alternatives><graphic xlink:href="pone.0321179.e002.jpg" id="pone.0321179.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x1d4b1;</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the node set and <inline-formula id="pone.0321179.e003"><alternatives><graphic xlink:href="pone.0321179.e003.jpg" id="pone.0321179.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x02130;</mml:mi><mml:mo>&#x02286;</mml:mo><mml:mi>&#x1d4b1;</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>&#x1d4b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the edge set. The connection relationship between any two nodes <inline-formula id="pone.0321179.e004"><alternatives><graphic xlink:href="pone.0321179.e004.jpg" id="pone.0321179.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> can be represented by the element <italic toggle="yes">a</italic><sub><italic toggle="yes">ij</italic></sub> of the adjacency matrix <inline-formula id="pone.0321179.e005"><alternatives><graphic xlink:href="pone.0321179.e005.jpg" id="pone.0321179.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x1d400;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>: if there is a link between <inline-formula id="pone.0321179.e006"><alternatives><graphic xlink:href="pone.0321179.e006.jpg" id="pone.0321179.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321179.e007"><alternatives><graphic xlink:href="pone.0321179.e007.jpg" id="pone.0321179.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0321179.e008"><alternatives><graphic xlink:href="pone.0321179.e008.jpg" id="pone.0321179.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>; otherwise, <inline-formula id="pone.0321179.e009"><alternatives><graphic xlink:href="pone.0321179.e009.jpg" id="pone.0321179.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. Without loss of generality, this paper assumes that there are <italic toggle="yes">M</italic> eavesdropping nodes in the network, accessing the network through illegal links and forming communications with some legitimate nodes. The set of eavesdropping nodes can be represented as <inline-formula id="pone.0321179.e010"><alternatives><graphic xlink:href="pone.0321179.e010.jpg" id="pone.0321179.e010g" position="anchor"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x1d4b1;</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02286;</mml:mo><mml:mi>&#x1d4b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. The goal of this paper is to identify the eavesdropping nodes in <inline-formula id="pone.0321179.e011"><alternatives><graphic xlink:href="pone.0321179.e011.jpg" id="pone.0321179.e011g" position="anchor"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x1d4b1;</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> based on the observed network communication data and estimate their spatial coordinates <inline-formula id="pone.0321179.e012"><alternatives><graphic xlink:href="pone.0321179.e012.jpg" id="pone.0321179.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>.</p><fig position="float" id="pone.0321179.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0321179.g001</object-id><label>Fig 1</label><caption><title>Eavesdropping attack model for PIoT and its graphical model representation.</title></caption><graphic xlink:href="pone.0321179.g001" position="float"/></fig><p>In real PIoT environments, each node possesses multi-dimensional heterogeneous attributes, such as device type, communication protocol, and signal strength [<xref rid="pone.0321179.ref018" ref-type="bibr">18</xref>]. These attributes can be uniformly represented by the node feature matrix <inline-formula id="pone.0321179.e013"><alternatives><graphic xlink:href="pone.0321179.e013.jpg" id="pone.0321179.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x1d417;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, where <italic toggle="yes">F</italic> is the attribute dimension. Moreover, the state of the wireless network is closely related to the channel environment. This paper employs the channel matrix <inline-formula id="pone.0321179.e014"><alternatives><graphic xlink:href="pone.0321179.e014.jpg" id="pone.0321179.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x1d407;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> to characterize the transmission characteristics between nodes, such as path loss and fading coefficients. Combining the above factors, this paper defines the normalized graph signal <inline-formula id="pone.0321179.e015"><alternatives><graphic xlink:href="pone.0321179.e015.jpg" id="pone.0321179.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x1d412;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> as:</p><disp-formula id="pone.0321179.e016"><alternatives><graphic xlink:href="pone.0321179.e016.jpg" id="pone.0321179.e016g" position="anchor"/><mml:math id="M16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x1d412;</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x1d400;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x1d417;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x1d407;</mml:mi><mml:mi>;</mml:mi><mml:msub><mml:mi>&#x1d416;</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(1)</label></disp-formula><p>where <inline-formula id="pone.0321179.e017"><alternatives><graphic xlink:href="pone.0321179.e017.jpg" id="pone.0321179.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x000b7;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the normalization function, <inline-formula id="pone.0321179.e018"><alternatives><graphic xlink:href="pone.0321179.e018.jpg" id="pone.0321179.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x1d416;</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is the normalization parameter, and <italic toggle="yes">D</italic> is the feature dimension after normalization.</p><p>Based on the above graph model, the eavesdropping localization problem can be formalized as a mapping from the observed signal <bold>S</bold> to the eavesdropping node coordinates <inline-formula id="pone.0321179.e019"><alternatives><graphic xlink:href="pone.0321179.e019.jpg" id="pone.0321179.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. Traditional methods approximate this mapping by manually designing statistical features and using shallow models, but they face limitations in expressive power when dealing with eavesdropping behavior in dynamic and heterogeneous networks. The KAN can approximate arbitrary nonlinear mappings through flexible combinations of simple functions, providing new possibilities for improving the accuracy and generalization of eavesdropping localization.</p></sec><sec id="sec005"><title>Kolmogorov-Arnold network principles</title><p>The Kolmogorov-Arnold representation theorem states that any continuous function <inline-formula id="pone.0321179.e020"><alternatives><graphic xlink:href="pone.0321179.e020.jpg" id="pone.0321179.e020g" position="anchor"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x1d431;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> defined on an <italic toggle="yes">n</italic>-dimensional unit cube <inline-formula id="pone.0321179.e021"><alternatives><graphic xlink:href="pone.0321179.e021.jpg" id="pone.0321179.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> can be represented as a superposition of one-dimensional continuous functions [<xref rid="pone.0321179.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0321179.ref020" ref-type="bibr">20</xref>]:</p><disp-formula id="pone.0321179.e022"><alternatives><graphic xlink:href="pone.0321179.e022.jpg" id="pone.0321179.e022g" position="anchor"/><mml:math id="M22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x1d431;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(2)</label></disp-formula><p>where <inline-formula id="pone.0321179.e023"><alternatives><graphic xlink:href="pone.0321179.e023.jpg" id="pone.0321179.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321179.e024"><alternatives><graphic xlink:href="pone.0321179.e024.jpg" id="pone.0321179.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> are both one-dimensional continuous functions. The theorem indicates that any <italic toggle="yes">n</italic>-dimensional continuous function can be decomposed into a superposition of <inline-formula id="pone.0321179.e025"><alternatives><graphic xlink:href="pone.0321179.e025.jpg" id="pone.0321179.e025g" position="anchor"/><mml:math id="M25" display="inline" overflow="scroll"><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mspace width="0.167em"/><mml:mo>+</mml:mo><mml:mspace width="0.167em"/><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> one-dimensional functions. This inspires us to transform the high-dimensional complex mapping of eavesdropping localization into the learning of one-dimensional function combinations.</p><p>Accordingly, the KAN network structure is shown in <xref rid="pone.0321179.g002" ref-type="fig">Fig 2</xref> [<xref rid="pone.0321179.ref020" ref-type="bibr">20</xref>], consisting of an input layer, several hidden layers, and an output layer. The forward propagation process of the network is formulated as:</p><fig position="float" id="pone.0321179.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0321179.g002</object-id><label>Fig 2</label><caption><title>The model structure of KAN.</title></caption><graphic xlink:href="pone.0321179.g002" position="float"/></fig><disp-formula id="pone.0321179.e026"><alternatives><graphic xlink:href="pone.0321179.e026.jpg" id="pone.0321179.e026g" position="anchor"/><mml:math id="M26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>&#x1d421;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>&#x1d421;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace linebreak="newline"/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munderover></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:munderover></mml:mstyle><mml:msubsup><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(3)</label></disp-formula><p>where <inline-formula id="pone.0321179.e027"><alternatives><graphic xlink:href="pone.0321179.e027.jpg" id="pone.0321179.e027g" position="anchor"/><mml:math id="M27" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x1d421;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is the hidden state of the <italic toggle="yes">l</italic>-th layer, <italic toggle="yes">n</italic><sub><italic toggle="yes">l</italic></sub> is the width of that layer; <inline-formula id="pone.0321179.e028"><alternatives><graphic xlink:href="pone.0321179.e028.jpg" id="pone.0321179.e028g" position="anchor"/><mml:math id="M28" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x1d421;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mi>&#x1d412;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the input signal; <inline-formula id="pone.0321179.e029"><alternatives><graphic xlink:href="pone.0321179.e029.jpg" id="pone.0321179.e029g" position="anchor"/><mml:math id="M29" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is a one-dimensional function connecting the <italic toggle="yes">i</italic>-th neuron of the (<italic toggle="yes">l</italic>&#x02013;1)-th layer and the <italic toggle="yes">j</italic>-th neuron of the <italic toggle="yes">l</italic>-th layer. The output layer <inline-formula id="pone.0321179.e030"><alternatives><graphic xlink:href="pone.0321179.e030.jpg" id="pone.0321179.e030g" position="anchor"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x1d421;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> gives the predicted values of the eavesdropping node coordinates.</p><p>In summary, KANs provide an innovative idea for solving the high-dimensional anomaly mapping problem in PIoT. <xref rid="pone.0321179.g002" ref-type="fig">Fig 2</xref> vividly illustrates the network structure of KANs, which essentially approximates complex mappings through an adaptive cascade of spline functions. In contrast to the fixed activation functions employed by conventional neural networks, each neuron in a KAN corresponds to a flexibly adjustable spline function <inline-formula id="pone.0321179.e031"><alternatives><graphic xlink:href="pone.0321179.e031.jpg" id="pone.0321179.e031g" position="anchor"/><mml:math id="M31" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, parameterized as:</p><disp-formula id="pone.0321179.e032"><alternatives><graphic xlink:href="pone.0321179.e032.jpg" id="pone.0321179.e032g" position="anchor"/><mml:math id="M32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msubsup><mml:mi>&#x003c9;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>B</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(4)</label></disp-formula><p>where <italic toggle="yes">B</italic><sub><italic toggle="yes">k</italic></sub>(<italic toggle="yes">x</italic>) denotes the <italic toggle="yes">k</italic>-th order B-spline basis function and <inline-formula id="pone.0321179.e033"><alternatives><graphic xlink:href="pone.0321179.e033.jpg" id="pone.0321179.e033g" position="anchor"/><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003c9;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> represent the control points. By tuning the order and control points of the spline functions, various nonlinear mappings can be flexibly approximated. Through learning the spline control points <inline-formula id="pone.0321179.e034"><alternatives><graphic xlink:href="pone.0321179.e034.jpg" id="pone.0321179.e034g" position="anchor"/><mml:math id="M34" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003c9;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> of each neuron, KANs can capture rich local details and approximate arbitrarily complex functions. Moreover, due to the local support property of B-spline functions, backpropagation only needs to update a small number of control points related to the current input, dramatically reducing computational overhead.</p><p>The generalization capability of KANs in handling complex, heterogeneous PIoT environments shares parallels with the challenges addressed in federated learning security. As discussed by Asif <italic toggle="yes">et al</italic>. [<xref rid="pone.0321179.ref009" ref-type="bibr">9</xref>], advanced zero-shot learning frameworks in federated learning aim to enhance model generalization while preserving data privacy. This paper&#x02019;s KAN-based approach aligns with this objective, leveraging the approximation power of Kolmogorov-Arnold representation to adapt to unseen eavesdropping patterns while maintaining data confidentiality through the use of ZKPs. This positions our work within the broader context of secure and generalizable learning in distributed IoT systems.</p><p><xref rid="pone.0321179.g003" ref-type="fig">Fig 3</xref> compares the structural differences between KANs and traditional Multi-Layer Perceptrons (MLPs) from the perspective of parameter scale. Assuming a network with <italic toggle="yes">L</italic> layers and <italic toggle="yes">N</italic> neurons per layer, the number of weight parameters in an MLP is <italic toggle="yes">O</italic>(<italic toggle="yes">LN</italic><sup>2</sup>), while for a KAN it is merely <italic toggle="yes">O</italic>(<italic toggle="yes">LN</italic><sup>2</sup><italic toggle="yes">K</italic>), where <italic toggle="yes">K</italic> represents the average spline order and typically <inline-formula id="pone.0321179.e035"><alternatives><graphic xlink:href="pone.0321179.e035.jpg" id="pone.0321179.e035g" position="anchor"/><mml:math id="M35" display="inline" overflow="scroll"><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#x0226a;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. Consequently, KANs not only compress the model scale but also possess an explicit functional form, which is more conducive to interpretability analysis.</p><fig position="float" id="pone.0321179.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0321179.g003</object-id><label>Fig 3</label><caption><title>The model structure of MLP.</title></caption><graphic xlink:href="pone.0321179.g003" position="float"/></fig></sec><sec id="sec006"><title>Application of KANs in eavesdropping localization</title><p>Eavesdropping localization presents distinct challenges compared to general regression tasks:</p><list list-type="bullet"><list-item><p>The PIoT involves a massive number of heterogeneous devices with complex, dynamic communication patterns. The non-stationarity of the feature space imposes stringent requirements on model generalization ability.</p></list-item><list-item><p>Eavesdropping behavior occurs randomly and abruptly, necessitating models to rapidly adapt to dynamic changes and learn eavesdropping features from limited samples.</p></list-item><list-item><p>The actual power grid environment contains complex noise, such as channel fading and data loss, rendering model robustness and stability crucial.</p></list-item><list-item><p>Edge devices in PIoT have constrained computational resources, including processing power and memory. Models must balance performance and overhead during deployment.</p></list-item></list><p>These characteristics place elevated demands on model generalization, adaptability, robustness, and lightweight design. KANs exhibit unique advantages when addressing eavesdropping localization tasks:</p><list list-type="bullet"><list-item><p><bold>Generalization</bold>: Benefiting from the universal approximation property of the Kolmogorov-Arnold theorem, KANs can approximate high-dimensional complex mappings through flexible combinations of a small number of basis functions. Without assuming specific data distribution forms, KANs demonstrate stronger adaptability to heterogeneous networks.</p></list-item><list-item><p><bold>Adaptability</bold>: KANs can adaptively optimize network structure by dynamically adjusting the order and control points of spline functions. Incorporating attention mechanisms further enables KANs to adjust feature weights according to network state changes, swiftly responding to novel eavesdropping patterns.</p></list-item><list-item><p><bold>Robustness</bold>: Introducing multi-scale and sparse regularization strategies enhances the robustness of KANs against noise and missing values. Furthermore, the local smoothness of spline functions ensures output continuity under small perturbations, resulting in more stable generalization performance.</p></list-item><list-item><p><bold>Lightweight</bold>: KANs can significantly compress model parameter scale while approximating complex mappings, reducing storage and computational resource consumption. This facilitates efficient deployment on resource-constrained power edge devices.</p></list-item></list><p>In summary, KANs provide a promising new approach for tackling the eavesdropping localization problem in power wireless communication networks. The subsequent sections will elaborate on the training and optimization process of KANs to better adapt to the demands of practical applications.</p></sec></sec><sec id="sec007"><title>Training and optimization of KAN models</title><p>This section introduces the key techniques for applying KANs to the eavesdropping localization task, including loss function design, grid adaptive strategies, and sparse regularization methods. A complete training process is also provided.</p><sec id="sec008"><title>Loss function design</title><p>The primary training objective of KANs is to minimize the localization error, which is the discrepancy between the true coordinates <inline-formula id="pone.0321179.e036"><alternatives><graphic xlink:href="pone.0321179.e036.jpg" id="pone.0321179.e036g" position="anchor"/><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi>i</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and the predicted coordinates <inline-formula id="pone.0321179.e037"><alternatives><graphic xlink:href="pone.0321179.e037.jpg" id="pone.0321179.e037g" position="anchor"/><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> of eavesdropping nodes. Considering the stealthiness of eavesdropping behavior, training samples often contain only a small portion of known eavesdroppers. To fully utilize unlabeled data, this paper designs a semi-supervised composite loss function that integrates supervised error and graph regularization constraints:</p><disp-formula id="pone.0321179.e038"><alternatives><graphic xlink:href="pone.0321179.e038.jpg" id="pone.0321179.e038g" position="anchor"/><mml:math id="M38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x02112;</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>&#x0211b;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003a6;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(5)</label></disp-formula><p>where <inline-formula id="pone.0321179.e039"><alternatives><graphic xlink:href="pone.0321179.e039.jpg" id="pone.0321179.e039g" position="anchor"/><mml:math id="M39" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> represents the supervised loss, which employs mean squared error to measure the localization deviation of labeled nodes. <inline-formula id="pone.0321179.e040"><alternatives><graphic xlink:href="pone.0321179.e040.jpg" id="pone.0321179.e040g" position="anchor"/><mml:math id="M40" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> denotes the unsupervised graph regularization loss, which mines structural information from network topology through graph smoothing, encouraging connected nodes to have similar embeddings. <inline-formula id="pone.0321179.e041"><alternatives><graphic xlink:href="pone.0321179.e041.jpg" id="pone.0321179.e041g" position="anchor"/><mml:math id="M41" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x0211b;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003a6;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the parameter coefficient regularization term, imposing <italic toggle="yes">L</italic><sub>1</sub> regularization on spline function parameters <inline-formula id="pone.0321179.e042"><alternatives><graphic xlink:href="pone.0321179.e042.jpg" id="pone.0321179.e042g" position="anchor"/><mml:math id="M42" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> to induce sparsity and enhance generalization ability. <inline-formula id="pone.0321179.e043"><alternatives><graphic xlink:href="pone.0321179.e043.jpg" id="pone.0321179.e043g" position="anchor"/><mml:math id="M43" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321179.e044"><alternatives><graphic xlink:href="pone.0321179.e044.jpg" id="pone.0321179.e044g" position="anchor"/><mml:math id="M44" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> are balancing factors.</p><p>The supervised loss adopts the Mean Squared Error (MSE) to measure the localization deviation of labeled eavesdropping nodes:</p><disp-formula id="pone.0321179.e045"><alternatives><graphic xlink:href="pone.0321179.e045.jpg" id="pone.0321179.e045g" position="anchor"/><mml:math id="M45" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>&#x1d4b1;</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>&#x1d4b1;</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(6)</label></disp-formula><p>For unlabeled nodes, this paper designs a graph regularization term as the unsupervised loss:</p><disp-formula id="pone.0321179.e046"><alternatives><graphic xlink:href="pone.0321179.e046.jpg" id="pone.0321179.e046g" position="anchor"/><mml:math id="M46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>&#x1d4b1;</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>&#x1d4b1;</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>&#x1d4a9;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>&#x1d4a9;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:msub><mml:mover><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(7)</label></disp-formula><p>where <inline-formula id="pone.0321179.e047"><alternatives><graphic xlink:href="pone.0321179.e047.jpg" id="pone.0321179.e047g" position="anchor"/><mml:math id="M47" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x1d4a9;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is the neighbor set of node <inline-formula id="pone.0321179.e048"><alternatives><graphic xlink:href="pone.0321179.e048.jpg" id="pone.0321179.e048g" position="anchor"/><mml:math id="M48" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. This loss encourages connected nodes to have similar embedding coordinates, enabling network structure information to guide eavesdropping localization in a self-supervised manner. Furthermore, this paper imposes sparse regularization on network functions to improve model generalization and interpretability:</p><disp-formula id="pone.0321179.e049"><alternatives><graphic xlink:href="pone.0321179.e049.jpg" id="pone.0321179.e049g" position="anchor"/><mml:math id="M49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x0211b;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003a6;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munderover></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:munderover></mml:mstyle><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>&#x003c9;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(8)</label></disp-formula><p>where <inline-formula id="pone.0321179.e050"><alternatives><graphic xlink:href="pone.0321179.e050.jpg" id="pone.0321179.e050g" position="anchor"/><mml:math id="M50" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">|</mml:mo><mml:mi>&#x000b7;</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> denotes the <italic toggle="yes">L</italic><sub>1</sub> norm. Sparse regularization helps automatically select the key functions and control points that contribute the most to eavesdropping localization.</p></sec><sec id="sec009"><title>Grid adaptive strategy</title><p>The approximation capability of KANs primarily depends on the complexity of spline functions. In the early stages of training, lower-order, sparse spline functions can be used to accelerate convergence. As training progresses, appropriately increasing the spline order and control point density can further improve accuracy. To adaptively optimize spline functions, this paper employs a dynamic grid refinement strategy:</p><list list-type="bullet"><list-item><p><bold>1. Initialization</bold>: Uniformly place <italic toggle="yes">K</italic><sub>0</sub> control points on each coordinate axis to form spline functions of order <italic toggle="yes">k</italic><sub>0</sub>.</p></list-item><list-item><p><bold>2. Refinement Criterion</bold>: Trigger grid refinement when the validation loss decreases by less than threshold <inline-formula id="pone.0321179.e051"><alternatives><graphic xlink:href="pone.0321179.e051.jpg" id="pone.0321179.e051g" position="anchor"/><mml:math id="M51" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> for <italic toggle="yes">T</italic> consecutive epochs.</p></list-item><list-item><p><bold>3. Refinement Operation</bold>: Bisect each interval and add new control points. Simultaneously increase the order <italic toggle="yes">k</italic> by one.</p></list-item><list-item><p><bold>4. Reparameterization</bold>: Fit the new and old functions using least squares to inherit the original function shape.</p></list-item><list-item><p><bold>5. Termination Condition</bold>: Repeat the above process until reaching the maximum order <italic toggle="yes">k</italic><sub><italic toggle="yes">max</italic></sub> or validation performance no longer improves.</p></list-item></list><p>It is worth noting that due to the local support property of B-spline functions, grid refinement only affects a limited interval, resulting in low computational overhead.</p><p>As mentioned earlier, this paper introduces <italic toggle="yes">L</italic><sub>1</sub> regularization in the loss function to encourage sparsity. However, since there is strong coupling between spline function parameters, the sparsification effect may be suboptimal. To further induce structural sparsity, this paper proposes a hierarchical sparse regularization method. Firstly, group <italic toggle="yes">L</italic><sub>2,1</sub> norm [<xref rid="pone.0321179.ref021" ref-type="bibr">21</xref>] is imposed at the network layer level:</p><disp-formula id="pone.0321179.e052"><alternatives><graphic xlink:href="pone.0321179.e052.jpg" id="pone.0321179.e052g" position="anchor"/><mml:math id="M52" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x0211b;</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msqrt><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>&#x003c9;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(9)</label></disp-formula><p>This equation encourages more hidden layer outputs to be zero, pruning redundant neurons. Secondly, group LASSO [<xref rid="pone.0321179.ref022" ref-type="bibr">22</xref>] is imposed within each spline function:</p><disp-formula id="pone.0321179.e053"><alternatives><graphic xlink:href="pone.0321179.e053.jpg" id="pone.0321179.e053g" position="anchor"/><mml:math id="M53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x0211b;</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x1d4a2;</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:msqrt><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:msqrt><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>&#x003c9;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(10)</label></disp-formula><p>where <italic toggle="yes">g</italic> is a subset of spline function control points, inducing block sparsity in the local function and eliminating redundant control points. Finally, it is combined with the <italic toggle="yes">L</italic><sub>1</sub> term in a weighted manner:</p><disp-formula id="pone.0321179.e054"><alternatives><graphic xlink:href="pone.0321179.e054.jpg" id="pone.0321179.e054g" position="anchor"/><mml:math id="M54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x0211b;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003a6;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>&#x0211b;</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>&#x0211b;</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>&#x003c9;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(11)</label></disp-formula><p>By jointly inducing sparsity locally and globally, a more concise and efficient network structure can be obtained. Integrating the above modules, the complete training process of the eavesdropping localization KAN model is shown in Algorithm 1.</p><p>
<bold>Algorithm 1 KAN training algorithm for eavesdropping localization.</bold>
</p><p specific-use="line">
<monospace>
<bold>Input:</bold> Graph signal matrix <bold>S</bold>, partial eavesdropping labels <inline-formula id="pone.0321179.e055"><alternatives><graphic xlink:href="pone.0321179.e055.jpg" id="pone.0321179.e055g" position="anchor"/><mml:math id="M55" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>&#x1d4b1;</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula></monospace>
</p><p specific-use="line">
<monospace>
<bold>Output:</bold> Trained KAN parameters <inline-formula id="pone.0321179.e056"><alternatives><graphic xlink:href="pone.0321179.e056.jpg" id="pone.0321179.e056g" position="anchor"/><mml:math id="M56" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:msup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula></monospace>
</p><p specific-use="line">
<monospace> 1: Randomly initialize <inline-formula id="pone.0321179.e057"><alternatives><graphic xlink:href="pone.0321179.e057.jpg" id="pone.0321179.e057g" position="anchor"/><mml:math id="M57" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:msup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, set initial grid <inline-formula id="pone.0321179.e058"><alternatives><graphic xlink:href="pone.0321179.e058.jpg" id="pone.0321179.e058g" position="anchor"/><mml:math id="M58" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></monospace>
</p><p specific-use="line">
<monospace> 2: <bold>while</bold> not converged <bold>do</bold></monospace>
</p><p specific-use="line">
<monospace> 3: &#x02003; Randomly sample minibatch <inline-formula id="pone.0321179.e059"><alternatives><graphic xlink:href="pone.0321179.e059.jpg" id="pone.0321179.e059g" position="anchor"/><mml:math id="M59" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x0212c;</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:msubsup><mml:mrow><mml:msub><mml:mi>&#x1d412;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x1d432;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula></monospace>
</p><p specific-use="line">
<monospace> 4: &#x02003; Compute predicted outputs <inline-formula id="pone.0321179.e060"><alternatives><graphic xlink:href="pone.0321179.e060.jpg" id="pone.0321179.e060g" position="anchor"/><mml:math id="M60" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi>&#x1d432;</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">K</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x1d412;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula></monospace>
</p><p specific-use="line">
<monospace> 5: &#x02003; Compute loss <inline-formula id="pone.0321179.e061"><alternatives><graphic xlink:href="pone.0321179.e061.jpg" id="pone.0321179.e061g" position="anchor"/><mml:math id="M61" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x02112;</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>&#x0211b;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003a6;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></monospace>
</p><p specific-use="line">
<monospace> 6: &#x02003; Calculate gradients <inline-formula id="pone.0321179.e062"><alternatives><graphic xlink:href="pone.0321179.e062.jpg" id="pone.0321179.e062g" position="anchor"/><mml:math id="M62" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>&#x02207;</mml:mo><mml:mrow><mml:mi>&#x003a6;</mml:mi></mml:mrow></mml:msub><mml:mi>&#x02112;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> through backpropagation</monospace>
</p><p specific-use="line">
<monospace> 7: &#x02003; Update parameters using gradient descent <inline-formula id="pone.0321179.e063"><alternatives><graphic xlink:href="pone.0321179.e063.jpg" id="pone.0321179.e063g" position="anchor"/><mml:math id="M63" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003a6;</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mi>&#x003a6;</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:msub><mml:mo>&#x02207;</mml:mo><mml:mrow><mml:mi>&#x003a6;</mml:mi></mml:mrow></mml:msub><mml:mi>&#x02112;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula></monospace>
</p><p specific-use="line">
<monospace> 8: &#x02003; <bold>if</bold> grid refinement criterion is met <bold>then</bold></monospace>
</p><p specific-use="line">
<monospace> 9: &#x02003;&#x02003; Update spline grid <inline-formula id="pone.0321179.e064"><alternatives><graphic xlink:href="pone.0321179.e064.jpg" id="pone.0321179.e064g" position="anchor"/><mml:math id="M64" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>&#x02190;</mml:mo><mml:mrow><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="1em"/><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></monospace>
</p><p specific-use="line">
<monospace> 10: &#x02003; <bold>end if</bold></monospace>
</p><p specific-use="line">
<monospace> 11: &#x02003; <bold>if</bold> early stopping criterion is met <bold>then</bold></monospace>
</p><p specific-use="line">
<monospace> 12: &#x02003;&#x02003; Break</monospace>
</p><p specific-use="line">
<monospace> 13: &#x02003; <bold>end if</bold></monospace>
</p><p specific-use="line">
<monospace> 14: <bold>end while</bold></monospace>
</p><p specific-use="line">
<monospace> 15: <bold>return</bold>
<inline-formula id="pone.0321179.e065"><alternatives><graphic xlink:href="pone.0321179.e065.jpg" id="pone.0321179.e065g" position="anchor"/><mml:math id="M65" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:msup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula></monospace>
</p><p>At each training step, a minibatch is sampled from the overall data, and the outputs and loss function values are computed through forward propagation. The model is then updated via backpropagation. Simultaneously, the spline grid is adaptively adjusted, and iteration is controlled using an early stopping mechanism. Upon completion of training, the KAN model for the eavesdropping localization task is obtained.</p></sec><sec id="sec010"><title>Model deployment and acceleration</title><p>To further enhance the deployment efficiency of KAN models on resource-constrained platforms, this paper adopts the following optimization measures.</p><p><bold>Quantization-aware training</bold>: Quantization error terms are introduced to simulate the quantization loss during inference [<xref rid="pone.0321179.ref023" ref-type="bibr">23</xref>], allowing the training process to adapt to the accuracy loss caused by quantization, thereby reducing performance degradation:</p><disp-formula id="pone.0321179.e066"><alternatives><graphic xlink:href="pone.0321179.e066.jpg" id="pone.0321179.e066g" position="anchor"/><mml:math id="M66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mspace width="0.167em"/><mml:mrow><mml:mo>&#x02212;</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mi>Q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(12)</label></disp-formula><p><bold>Sparsification acceleration</bold>: During the inference stage, the smaller control points in KANs are directly pruned to zero, resulting in sparse spline functions. Fully utilizing sparsity can significantly reduce computational and storage overhead.</p><p><bold>Fixed-point implementation</bold>: Model weights and intermediate activations are represented using fixed-point numbers, replacing floating-point operations with integer and fixed-point operations. Under the premise of controllable accuracy loss, resource consumption is greatly compressed.</p><p><bold>Model compression</bold>: Knowledge distillation [<xref rid="pone.0321179.ref024" ref-type="bibr">24</xref>] and other methods are employed to compress the complex KAN model trained into a smaller model, further reducing the number of parameters and computations.</p><p>Through the above methods, KAN models can achieve highly streamlined and efficient hardware deployment. In the experiments, this paper will evaluate the speed and energy consumption performance of the optimized models on embedded devices.</p><p>It&#x02019;s worth noting that while this paper proposed KAN-based method relies on centralized processing of IoT data, it has the potential to be extended to edge computing scenarios. As suggested by Salam <italic toggle="yes">et al</italic>. [<xref rid="pone.0321179.ref012" ref-type="bibr">12</xref>], anomaly detection models can be deployed on PIoT edge devices to validate the authenticity of detected anomalies without revealing sensitive data or model details. Future work could explore adapting our KAN approach to enable secure and privacy-preserving edge-based eavesdropping detection, further enhancing the practicality and scalability of the solution.</p></sec></sec><sec id="sec011"><title>Experimental results and analysis</title><p>To comprehensively evaluate the performance of the proposed KAN method, this paper conducts extensive experiments on multiple simulated and real PIoT datasets and compares it with classic machine learning and deep learning methods. Additionally, the effectiveness of each module is verified through ablation studies, and the efficiency of the deployed model on resource-constrained platforms is tested.</p><sec id="sec012"><title>Experimental setup</title><p><bold>Datasets</bold>: This paper employs a simulated dataset and a real-world PIoT dataset for evaluation. The simulated data is generated through the wireless communication network simulation platform in [<xref rid="pone.0321179.ref018" ref-type="bibr">18</xref>], containing 10 groups of data with different network scales (100 to 1000 nodes) and eavesdropping ratios (1% to 10%). Each group of data is divided into a training set (60%), validation set (20%), and test set (20%). The real-world dataset originates from the micro-PMU communication records of smart power equipment in a community over a continuous week, containing 100 nodes, of which 3 were implanted with eavesdropping programs. This dataset is derived from the same smart power equipment monitoring system as described in our previous work [<xref rid="pone.0321179.ref018" ref-type="bibr">18</xref>], ensuring consistency and comparability of the experimental setup. To further validate the representativeness and generalizability of the findings, we extended the original dataset to cover a longer time span of one month and included data from a larger set of 500 nodes, out of which 5 were identified as eavesdroppers through manual verification.</p><p>The extended dataset was processed and analyzed following the same procedures outlined in this paper. The proposed KAN-based method achieved an F1-score of 0.938 and a localization error of 0.887m on this larger dataset, demonstrating consistent performance with the results reported on the initial one-week dataset. This confirms that our approach can effectively scale to larger, more diverse real-world scenarios and provides robust eavesdropping detection over extended periods.</p><p>It&#x02019;s worth noting that the use of the same experimental platform and dataset source as in [<xref rid="pone.0321179.ref018" ref-type="bibr">18</xref>] allows for a fair and direct comparison of the proposed method with the baseline techniques discussed in that work. This further highlights the significant improvements achieved by our KAN-based approach in terms of detection accuracy and localization precision.</p><p><bold>Comparison methods</bold>: This paper selects traditional machine learning models such as Decision Tree (DT), K-Nearest Neighbor (KNN), and Support Vector Machine (SVM), as well as deep learning models such as MLP, Convolutional Neural Network (CNN), and Graph Convolutional Network (GCN) [<xref rid="pone.0321179.ref025" ref-type="bibr">25</xref>] as baselines.</p><p><bold>Evaluation metrics</bold>: Accuracy, Precision, Recall, and F1-score are adopted to assess the performance of eavesdropping node classification:</p><disp-formula id="pone.0321179.e067"><alternatives><graphic xlink:href="pone.0321179.e067.jpg" id="pone.0321179.e067g" position="anchor"/><mml:math id="M67" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mspace linebreak="newline"/><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mspace linebreak="newline"/><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mspace linebreak="newline"/><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(13)</label></disp-formula><p>where TP, TN, FP, and FN represent the number of true positive, true negative, false positive, and false negative samples, respectively. As shown in <xref rid="pone.0321179.e067" ref-type="disp-formula">Eq 13</xref>, accuracy measures the proportion of correctly classified samples out of the total samples, while precision and recall focus on the subsets of samples predicted as positive and truly positive, respectively. F1-score, the harmonic mean of precision and recall, is a commonly used metric for comprehensively evaluating classifier performance. Additionally, the average localization error (<italic toggle="yes">Loc</italic><sub><italic toggle="yes">Err</italic></sub>) is employed to measure the estimation accuracy of eavesdropping node coordinates:</p><disp-formula id="pone.0321179.e068"><alternatives><graphic xlink:href="pone.0321179.e068.jpg" id="pone.0321179.e068g" position="anchor"/><mml:math id="M68" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>O</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>&#x1d4b1;</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>&#x1d4b1;</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.167em"/><mml:mrow><mml:mo>&#x02212;</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(14)</label></disp-formula><p><bold>Hyperparameter selection</bold>: Through grid search on the validation set, the optimal structure of KANs is determined to be 3 layers with 40 neurons per layer and cubic spline functions. The initial learning rate is set to 0.001, decaying by 50% at each grid refinement. The regularization term coefficients are <inline-formula id="pone.0321179.e069"><alternatives><graphic xlink:href="pone.0321179.e069.jpg" id="pone.0321179.e069g" position="anchor"/><mml:math id="M69" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mn>0.1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0321179.e070"><alternatives><graphic xlink:href="pone.0321179.e070.jpg" id="pone.0321179.e070g" position="anchor"/><mml:math id="M70" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mn>0.01</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0321179.e071"><alternatives><graphic xlink:href="pone.0321179.e071.jpg" id="pone.0321179.e071g" position="anchor"/><mml:math id="M71" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mn>0.001</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pone.0321179.e072"><alternatives><graphic xlink:href="pone.0321179.e072.jpg" id="pone.0321179.e072g" position="anchor"/><mml:math id="M72" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mn>0.0001</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. All models are trained using the Adam optimizer with a batch size of 64. Training is stopped when the validation set performance does not improve for 5 consecutive rounds.</p><p><bold>Hardware environment</bold>: Offline training is conducted on a workstation equipped with an NVIDIA RTX 3090 GPU. Online inference is deployed on an NVIDIA Jetson TX2 embedded development board with 256 CUDA cores and 8GB memory. Models are compiled into optimized binary code through TVM [<xref rid="pone.0321179.ref026" ref-type="bibr">26</xref>]. All experiments are repeated 3 times, and the average values are taken to eliminate randomness.</p></sec><sec id="sec013"><title>Localization performance evaluation</title><p><xref rid="pone.0321179.t001" ref-type="table">Tables 1</xref> and <xref rid="pone.0321179.t002" ref-type="table">2</xref> shows the localization performance of different methods on the two datasets. As evidenced by the simulated data, KANs attain an F1-score of 0.945 for eavesdropping node classification and diminish the localization error to 0.632 m, markedly surpassing other baseline methods. This demonstrates that KANs can effectively learn eavesdropping behavior features in complex environments. In contrast, shallow models like decision trees are limited by their expressive power and struggle to characterize nonlinear anomaly patterns. Although MLPs employ deep structures, they lack modeling of network topology and perform inferior to GCNs and KANs. It is worth mentioning that GCNs introduce graph convolution to fuse node correlation information; however, their aggregation approach remains linear, and their ability to capture anomalies is less flexible compared to the nonlinear spline functions in KANs.</p><table-wrap position="float" id="pone.0321179.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0321179.t001</object-id><label>Table 1</label><caption><title>Localization performance of different methods on simulated datasets.</title></caption><alternatives><graphic xlink:href="pone.0321179.t001" id="pone.0321179.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="2" colspan="1">Method</th><th align="left" colspan="5" rowspan="1">Simulated Data</th></tr><tr><th align="left" rowspan="1" colspan="1">Acc</th><th align="left" rowspan="1" colspan="1">Prec</th><th align="left" rowspan="1" colspan="1">Rec</th><th align="left" rowspan="1" colspan="1">F1</th><th align="left" rowspan="1" colspan="1">Loc Err</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">DT</td><td align="left" rowspan="1" colspan="1">0.628</td><td align="left" rowspan="1" colspan="1">0.712</td><td align="left" rowspan="1" colspan="1">0.445</td><td align="left" rowspan="1" colspan="1">0.531</td><td align="left" rowspan="1" colspan="1">5.632</td></tr><tr><td align="left" rowspan="1" colspan="1">KNN</td><td align="left" rowspan="1" colspan="1">0.765</td><td align="left" rowspan="1" colspan="1">0.708</td><td align="left" rowspan="1" colspan="1">0.619</td><td align="left" rowspan="1" colspan="1">0.654</td><td align="left" rowspan="1" colspan="1">4.428</td></tr><tr><td align="left" rowspan="1" colspan="1">SVM</td><td align="left" rowspan="1" colspan="1">0.823</td><td align="left" rowspan="1" colspan="1">0.795</td><td align="left" rowspan="1" colspan="1">0.682</td><td align="left" rowspan="1" colspan="1">0.726</td><td align="left" rowspan="1" colspan="1">4.105</td></tr><tr><td align="left" rowspan="1" colspan="1">MLP</td><td align="left" rowspan="1" colspan="1">0.859</td><td align="left" rowspan="1" colspan="1">0.831</td><td align="left" rowspan="1" colspan="1">0.784</td><td align="left" rowspan="1" colspan="1">0.806</td><td align="left" rowspan="1" colspan="1">3.580</td></tr><tr><td align="left" rowspan="1" colspan="1">CNN</td><td align="left" rowspan="1" colspan="1">0.875</td><td align="left" rowspan="1" colspan="1">0.846</td><td align="left" rowspan="1" colspan="1">0.805</td><td align="left" rowspan="1" colspan="1">0.824</td><td align="left" rowspan="1" colspan="1">3.215</td></tr><tr><td align="left" rowspan="1" colspan="1">GCN</td><td align="left" rowspan="1" colspan="1">0.905</td><td align="left" rowspan="1" colspan="1">0.892</td><td align="left" rowspan="1" colspan="1">0.863</td><td align="left" rowspan="1" colspan="1">0.877</td><td align="left" rowspan="1" colspan="1">1.804</td></tr><tr><td align="left" rowspan="1" colspan="1">GAT</td><td align="left" rowspan="1" colspan="1">0.912</td><td align="left" rowspan="1" colspan="1">0.903</td><td align="left" rowspan="1" colspan="1">0.875</td><td align="left" rowspan="1" colspan="1">0.885</td><td align="left" rowspan="1" colspan="1">1.573</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer</td><td align="left" rowspan="1" colspan="1">0.923</td><td align="left" rowspan="1" colspan="1">0.916</td><td align="left" rowspan="1" colspan="1">0.889</td><td align="left" rowspan="1" colspan="1">0.902</td><td align="left" rowspan="1" colspan="1">1.206</td></tr><tr><td align="left" rowspan="1" colspan="1">KAN</td><td align="left" rowspan="1" colspan="1">0.944</td><td align="left" rowspan="1" colspan="1">0.937</td><td align="left" rowspan="1" colspan="1">0.954</td><td align="left" rowspan="1" colspan="1">0.945</td><td align="left" rowspan="1" colspan="1">0.632</td></tr></tbody></table></alternatives></table-wrap><table-wrap position="float" id="pone.0321179.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0321179.t002</object-id><label>Table 2</label><caption><title>Localization performance of different methods on real-world datasets.</title></caption><alternatives><graphic xlink:href="pone.0321179.t002" id="pone.0321179.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="2" colspan="1">Method</th><th align="left" colspan="5" rowspan="1">Real-world Data</th></tr><tr><th align="left" rowspan="1" colspan="1">Acc</th><th align="left" rowspan="1" colspan="1">Prec</th><th align="left" rowspan="1" colspan="1">Rec</th><th align="left" rowspan="1" colspan="1">F1</th><th align="left" rowspan="1" colspan="1">Loc Err</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">DT</td><td align="left" rowspan="1" colspan="1">0.503</td><td align="left" rowspan="1" colspan="1">0.624</td><td align="left" rowspan="1" colspan="1">0.327</td><td align="left" rowspan="1" colspan="1">0.412</td><td align="left" rowspan="1" colspan="1">9.315</td></tr><tr><td align="left" rowspan="1" colspan="1">KNN</td><td align="left" rowspan="1" colspan="1">0.751</td><td align="left" rowspan="1" colspan="1">0.682</td><td align="left" rowspan="1" colspan="1">0.593</td><td align="left" rowspan="1" colspan="1">0.624</td><td align="left" rowspan="1" colspan="1">6.502</td></tr><tr><td align="left" rowspan="1" colspan="1">SVM</td><td align="left" rowspan="1" colspan="1">0.794</td><td align="left" rowspan="1" colspan="1">0.763</td><td align="left" rowspan="1" colspan="1">0.655</td><td align="left" rowspan="1" colspan="1">0.697</td><td align="left" rowspan="1" colspan="1">5.844</td></tr><tr><td align="left" rowspan="1" colspan="1">MLP</td><td align="left" rowspan="1" colspan="1">0.832</td><td align="left" rowspan="1" colspan="1">0.795</td><td align="left" rowspan="1" colspan="1">0.748</td><td align="left" rowspan="1" colspan="1">0.766</td><td align="left" rowspan="1" colspan="1">4.916</td></tr><tr><td align="left" rowspan="1" colspan="1">CNN</td><td align="left" rowspan="1" colspan="1">0.853</td><td align="left" rowspan="1" colspan="1">0.821</td><td align="left" rowspan="1" colspan="1">0.779</td><td align="left" rowspan="1" colspan="1">0.796</td><td align="left" rowspan="1" colspan="1">4.337</td></tr><tr><td align="left" rowspan="1" colspan="1">GCN</td><td align="left" rowspan="1" colspan="1">0.884</td><td align="left" rowspan="1" colspan="1">0.867</td><td align="left" rowspan="1" colspan="1">0.835</td><td align="left" rowspan="1" colspan="1">0.849</td><td align="left" rowspan="1" colspan="1">2.673</td></tr><tr><td align="left" rowspan="1" colspan="1">GAT</td><td align="left" rowspan="1" colspan="1">0.892</td><td align="left" rowspan="1" colspan="1">0.878</td><td align="left" rowspan="1" colspan="1">0.847</td><td align="left" rowspan="1" colspan="1">0.858</td><td align="left" rowspan="1" colspan="1">2.325</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer</td><td align="left" rowspan="1" colspan="1">0.905</td><td align="left" rowspan="1" colspan="1">0.893</td><td align="left" rowspan="1" colspan="1">0.864</td><td align="left" rowspan="1" colspan="1">0.878</td><td align="left" rowspan="1" colspan="1">1.952</td></tr><tr><td align="left" rowspan="1" colspan="1">KAN</td><td align="left" rowspan="1" colspan="1">0.926</td><td align="left" rowspan="1" colspan="1">0.912</td><td align="left" rowspan="1" colspan="1">0.885</td><td align="left" rowspan="1" colspan="1">0.897</td><td align="left" rowspan="1" colspan="1">1.411</td></tr></tbody></table></alternatives></table-wrap><p>In real PIoT scenarios, due to higher data complexity and noise levels, the performance of all models decreases, but the relative advantages remain consistent. Among them, KANs achieve an F1-score of 0.897 and a localization error of 1.804m, still substantially leading other methods. This fully demonstrates the effectiveness and robustness of KANs in actual deployment environments. For a more intuitive comparison, <xref rid="pone.0321179.g004" ref-type="fig">Fig 4</xref> illustrates the error curves of various models on the test set as the number of iterations changes. It can be observed that KANs converge the fastest, with a rapid initial decrease and stable performance in later stages, ultimately converging to the lowest localization error. This benefits from the small parameter size of KANs and the initialization of spline functions consistent with the network topological structure, alleviating optimization difficulties.</p><fig position="float" id="pone.0321179.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0321179.g004</object-id><label>Fig 4</label><caption><title>The error curves of different methods.</title></caption><graphic xlink:href="pone.0321179.g004" position="float"/></fig><p>To further validate the superiority of our approach, we compared the performance of KAN with advanced deep learning models including Graph Attention Networks (GAT) and Transformers. On the simulated dataset, GAT achieved an F1-score of 0.885 and a localization error of 1.573m, while Transformer reached an F1-score of 0.902 and a localization error of 1.206m. Similarly, on the real-world dataset, GAT obtained an F1-score of 0.858 and a localization error of 2.325m, while Transformer attained an F1-score of 0.878 and a localization error of 1.952m. Despite their strong expressive power, these models still underperformed KAN, which demonstrates the effectiveness of our function approximation-based approach for eavesdropping localization.</p><p>Furthermore, we analyzed the convergence behavior of these models during training, as depicted in <xref rid="pone.0321179.g004" ref-type="fig">Fig 4</xref>. KAN exhibits the fastest convergence speed and reaches the lowest localization error, benefiting from its compact parameter space and efficient optimization procedure. In contrast, GAT and Transformer converge slower than KAN but still outperform other baselines, indicating their capability to learn complex node representations. These results highlight the advantages of our proposed method in terms of both final performance and training efficiency.</p></sec><sec id="sec014"><title>Ablation study</title><p>Ablation experiments aim to validate the contributions of each module in the KAN framework. <xref rid="pone.0321179.t003" ref-type="table">Table 3</xref> presents the ablation results under different configurations.</p><table-wrap position="float" id="pone.0321179.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0321179.t003</object-id><label>Table 3</label><caption><title>Ablation study results.</title></caption><alternatives><graphic xlink:href="pone.0321179.t003" id="pone.0321179.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">Acc</th><th align="left" rowspan="1" colspan="1">Prec</th><th align="left" rowspan="1" colspan="1">Rec</th><th align="left" rowspan="1" colspan="1">F1</th><th align="left" rowspan="1" colspan="1">Loc Err</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">KAN-full (Ours)</td><td align="left" rowspan="1" colspan="1">0.944</td><td align="left" rowspan="1" colspan="1">0.937</td><td align="left" rowspan="1" colspan="1">0.954</td><td align="left" rowspan="1" colspan="1">0.945</td><td align="left" rowspan="1" colspan="1">0.632</td></tr><tr><td align="left" rowspan="1" colspan="1">w/o spline</td><td align="left" rowspan="1" colspan="1">0.852</td><td align="left" rowspan="1" colspan="1">0.817</td><td align="left" rowspan="1" colspan="1">0.783</td><td align="left" rowspan="1" colspan="1">0.794</td><td align="left" rowspan="1" colspan="1">3.319</td></tr><tr><td align="left" rowspan="1" colspan="1">w/o unsup loss</td><td align="left" rowspan="1" colspan="1">0.917</td><td align="left" rowspan="1" colspan="1">0.904</td><td align="left" rowspan="1" colspan="1">0.893</td><td align="left" rowspan="1" colspan="1">0.898</td><td align="left" rowspan="1" colspan="1">0.875</td></tr><tr><td align="left" rowspan="1" colspan="1">w/o sparse reg</td><td align="left" rowspan="1" colspan="1">0.932</td><td align="left" rowspan="1" colspan="1">0.921</td><td align="left" rowspan="1" colspan="1">0.938</td><td align="left" rowspan="1" colspan="1">0.928</td><td align="left" rowspan="1" colspan="1">0.744</td></tr><tr><td align="left" rowspan="1" colspan="1">w/o adaptive grid</td><td align="left" rowspan="1" colspan="1">0.927</td><td align="left" rowspan="1" colspan="1">0.912</td><td align="left" rowspan="1" colspan="1">0.923</td><td align="left" rowspan="1" colspan="1">0.917</td><td align="left" rowspan="1" colspan="1">0.905</td></tr><tr><td align="left" rowspan="1" colspan="1">w/o layerwise sparse</td><td align="left" rowspan="1" colspan="1">0.939</td><td align="left" rowspan="1" colspan="1">0.930</td><td align="left" rowspan="1" colspan="1">0.945</td><td align="left" rowspan="1" colspan="1">0.937</td><td align="left" rowspan="1" colspan="1">0.672</td></tr><tr><td align="left" rowspan="1" colspan="1">w/o splinewise sparse</td><td align="left" rowspan="1" colspan="1">0.935</td><td align="left" rowspan="1" colspan="1">0.926</td><td align="left" rowspan="1" colspan="1">0.941</td><td align="left" rowspan="1" colspan="1">0.934</td><td align="left" rowspan="1" colspan="1">0.697</td></tr></tbody></table></alternatives></table-wrap><p>Firstly, removing spline functions and directly training linear function clusters (w/o spline) leads to a severe performance degradation. This indicates that the nonlinear function approximation capability of KANs is crucial for capturing complex eavesdropping behavior patterns. Secondly, removing the unsupervised graph regularization loss (w/o unsup loss) in semi-supervised training considerably impacts both classification and localization performance. This demonstrates that the topological structure information contained in unlabeled data can help generalization and avoid overfitting. Furthermore, not employing sparse regularization (w/o sparse reg) or adaptive grid refinement (w/o adaptive grid) also results in a certain degree of performance decline. Network sparsification compresses redundant parameters and improves generalization ability, while adaptive grid adjustment facilitates the refinement of local details in later training stages. It is worth noting that further removing partial terms in the hierarchical sparse regularization, such as layer-wise sparsity (w/o layerwise sparse) and intra-spline sparsity (w/o splinewise sparse), leads to a relatively mild performance decrease. This suggests that combining global and local induction is more conducive to obtaining a concise and efficient network structure.</p></sec><sec id="sec015"><title>Resource-constrained environment evaluation</title><p>This section examines the deployment efficiency of KAN models on resource-constrained platforms. <xref rid="pone.0321179.g005" ref-type="fig">Fig 5</xref> compares the speed-accuracy trade-off of different methods on the Jetson TX2 embedded device. It can be observed that shallow models like decision trees are computationally fast but perform poorly. The inference of MLPs and CNNs is slower, but their localization errors are significantly higher than KANs. GAT and Transformer strike a balance between accuracy and speed, but their resource consumption is still suboptimal compared to KANs. GCNs also demonstrate a good speed-accuracy balance but are outperformed by KANs in both aspects.</p><fig position="float" id="pone.0321179.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0321179.g005</object-id><label>Fig 5</label><caption><title>Speed-accuracy trade-off of different methods on embedded devices.</title></caption><graphic xlink:href="pone.0321179.g005" position="float"/></fig><p>Benefiting from the efficient design of spline functions and network pruning techniques, KANs can maintain high localization accuracy while achieving faster inference compared to GCNs, GAT, and Transformer. Specifically, a KAN with 3 layers and 40 neurons per layer, which has approximately 130 parameters per layer, has a model size of only 3.2MB while achieving an F1-score as high as 0.93.</p><p>Furthermore, <xref rid="pone.0321179.t004" ref-type="table">Table 4</xref> compares the resource overhead of KAN models after different optimizations, by applying quantization (KAN-int8), sparsification (KAN-int8-sparse), and knowledge distillation (KAN-int8-sparse-KD), the inference speed of KANs can be significantly improved with minimal impact on accuracy. Quantization-aware training replaces 32-bit floating-point numbers with 8-bit fixed-point numbers during inference, significantly reducing memory consumption. After sparsification, non-zero parameters are further compressed to 15% of the original, and inference time and energy consumption are greatly reduced. The quantized and sparsified KAN model is 3.4 times faster than MLPs and 5.1 times faster than CNNs, while the knowledge distilled variant achieves an even more impressive 14.6 times speedup over MLPs and 21.9 times over CNNs. These results demonstrate the effectiveness of model compression techniques in enabling efficient deployment of KANs on resource-constrained IoT devices.</p><table-wrap position="float" id="pone.0321179.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0321179.t004</object-id><label>Table 4</label><caption><title>Resource overhead of KAN models after different optimizations.</title></caption><alternatives><graphic xlink:href="pone.0321179.t004" id="pone.0321179.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Optimization</th><th align="left" rowspan="1" colspan="1">Model Size</th><th align="left" rowspan="1" colspan="1">Latency</th><th align="left" rowspan="1" colspan="1">Energy</th><th align="left" rowspan="1" colspan="1">F1</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">KAN-float32</td><td align="left" rowspan="1" colspan="1">3.2 MB</td><td align="left" rowspan="1" colspan="1">8.6 ms</td><td align="left" rowspan="1" colspan="1">126 mJ</td><td align="left" rowspan="1" colspan="1">0.934</td></tr><tr><td align="left" rowspan="1" colspan="1">KAN-int8</td><td align="left" rowspan="1" colspan="1">0.8 MB</td><td align="left" rowspan="1" colspan="1">4.5 ms</td><td align="left" rowspan="1" colspan="1">78 mJ</td><td align="left" rowspan="1" colspan="1">0.926</td></tr><tr><td align="left" rowspan="1" colspan="1">KAN-int8-sparse</td><td align="left" rowspan="1" colspan="1">0.3 MB</td><td align="left" rowspan="1" colspan="1">1.9 ms</td><td align="left" rowspan="1" colspan="1">34 mJ</td><td align="left" rowspan="1" colspan="1">0.925</td></tr><tr><td align="left" rowspan="1" colspan="1">KAN-int8-sparse-KD</td><td align="left" rowspan="1" colspan="1">0.1 MB</td><td align="left" rowspan="1" colspan="1">0.7 ms</td><td align="left" rowspan="1" colspan="1">15 mJ</td><td align="left" rowspan="1" colspan="1">0.920</td></tr></tbody></table></alternatives></table-wrap><p>In summary, the proposed KAN method achieves significant performance advantages in the eavesdropping node localization task, exhibiting excellent generalization ability, robustness, and interpretability. Through customized optimization, KANs can be efficiently deployed on resource-constrained embedded platforms, providing a lightweight solution for real-time eavesdropping attack prevention in actual PIoT environments.</p></sec></sec><sec sec-type="conclusions" id="sec016"><title>Conclusion</title><p>Addressing the increasingly severe eavesdropping security threats in PIoT, this paper proposes an innovative intelligent localization method based on Kolmogorov-Arnold networks. Leveraging the superior ability of KANs to approximate complex nonlinear functions, this method achieves end-to-end mapping from heterogeneous node data to eavesdropping locations through flexible combinations of spline functions. On this basis, deep optimization strategies such as graph regularization, adaptive grid refinement, and hierarchical sparsity induction are incorporated, enhancing model performance while considering the stringent requirements of practical scenarios for generalization, robustness, and interpretability. In large-scale simulation experiments and tests on real power grid data, the proposed method demonstrates significant performance advantages over traditional machine learning and mainstream deep learning approaches. Furthermore, this paper fully considers the resource constraints of industrial deployment environments and realizes lightweight implementation for real-time online detection on embedded platforms, breaking through the bottleneck of difficult local deployment of intelligent analysis models.</p><p>In conclusion, this paper achieves landmark progress in theoretical innovation, methodological design, and engineering implementation, and the proposed KAN paradigm is expected to become a new paradigm for intelligent power grid information security in IoT environments. Prospectively, the authors aspire to expand the breadth and depth of the propounded framework&#x02019;s applications in the power industry and further refine online incremental learning and adaptive architecture search mechanisms to contend with the rapid evolution of network security challenges. It is believed that through collaborative innovation in industry, academia, and research, the AI-driven security protection system based on KANs can provide solid technical support and intellectual assistance for safeguarding the future of the energy internet.</p></sec></body><back><ref-list><title>References</title><ref id="pone.0321179.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Suhaimy</surname><given-names>N</given-names></name>, <name><surname>Radzi</surname><given-names>NAM</given-names></name>, <name><surname>Ahmad</surname><given-names>WSHMW</given-names></name>, <name><surname>Azmi</surname><given-names>KHM</given-names></name>, <name><surname>Hannan</surname><given-names>MA</given-names></name>. <article-title>Current and future communication solutions for smart grids: a review</article-title>. <source>IEEE Access</source>. <year>2022</year>;<volume>10</volume>:<fpage>43639</fpage>&#x02013;<lpage>68</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/access.2022.3168740</pub-id></mixed-citation></ref><ref id="pone.0321179.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Gunduz</surname><given-names>MZ</given-names></name>, <name><surname>Das</surname><given-names>R</given-names></name>. <article-title>Cyber-security on smart grid: threats and potential solutions</article-title>. <source>Comput Netw</source>. <year>2020</year>;<volume>169</volume>:<fpage>107094</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.comnet.2019.107094</pub-id></mixed-citation></ref><ref id="pone.0321179.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Zhuang</surname><given-names>P</given-names></name>, <name><surname>Liang</surname><given-names>H</given-names></name>. <article-title>False data injection attacks against state-of-charge estimation of battery energy storage systems in smart distribution networks</article-title>. <source>IEEE Trans Smart Grid</source>. <year>2021</year>;<volume>12</volume>(<issue>3</issue>):<fpage>2566</fpage>&#x02013;<lpage>77</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tsg.2020.3042926</pub-id></mixed-citation></ref><ref id="pone.0321179.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Reda</surname><given-names>HT</given-names></name>, <name><surname>Anwar</surname><given-names>A</given-names></name>, <name><surname>Mahmood</surname><given-names>A</given-names></name>. <article-title>Comprehensive survey and taxonomies of false data injection attacks in smart grids: attack models, targets, and impacts</article-title>. <source>Renew Sustain Energy Rev</source>. <year>2022</year>;<volume>163</volume>:<fpage>112423</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.rser.2022.112423</pub-id></mixed-citation></ref><ref id="pone.0321179.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Wei</surname><given-names>X</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Dong</surname><given-names>Z</given-names></name>, <name><surname>Shahidehpour</surname><given-names>M</given-names></name>. <article-title>Detection of false data injection attacks in smart grid: a secure federated deep learning approach</article-title>. <source>IEEE Trans Smart Grid</source>. <year>2022</year>;<volume>13</volume>(<issue>6</issue>):<fpage>4862</fpage>&#x02013;<lpage>72</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tsg.2022.3204796</pub-id></mixed-citation></ref><ref id="pone.0321179.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Reda</surname><given-names>HT</given-names></name>, <name><surname>Anwar</surname><given-names>A</given-names></name>, <name><surname>Mahmood</surname><given-names>AN</given-names></name>, <name><surname>Tari</surname><given-names>Z</given-names></name>. <article-title>A taxonomy of cyber defence strategies against false data attacks in smart grids</article-title>. <source>ACM Comput Surv</source>. <year>2023</year>;<volume>55</volume>(14s):<fpage>1</fpage>&#x02013;<lpage>37</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/3592797</pub-id></mixed-citation></ref><ref id="pone.0321179.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Bi</surname><given-names>S</given-names></name>, <name><surname>Zhang</surname><given-names>Y-JA</given-names></name>. <article-title>Locational detection of the false data injection attack in a smart grid: a multilabel classification approach</article-title>. <source>IEEE Internet Things J</source>. <year>2020</year>;<volume>7</volume>(<issue>9</issue>):<fpage>8218</fpage>&#x02013;<lpage>27</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/jiot.2020.2983911</pub-id></mixed-citation></ref><ref id="pone.0321179.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Lin</surname><given-names>W-T</given-names></name>, <name><surname>Chen</surname><given-names>G</given-names></name>, <name><surname>Huang</surname><given-names>Y</given-names></name>. <article-title>Incentive edge-based federated learning for false data injection attack detection on power grid state estimation: a novel mechanism design approach</article-title>. <source>Appl Energy</source>. <year>2022</year>;<volume>314</volume>:<fpage>118828</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.apenergy.2022.118828</pub-id></mixed-citation></ref><ref id="pone.0321179.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Asif</surname><given-names>M</given-names></name>, <name><surname>Naz</surname><given-names>S</given-names></name>, <name><surname>Ali</surname><given-names>F</given-names></name>, <name><surname>Salam</surname><given-names>A</given-names></name>, <name><surname>Amin</surname><given-names>F</given-names></name>, <name><surname>Ullah</surname><given-names>F</given-names></name>, <etal>et al</etal>. <article-title>Advanced zero-shot learning (azsl) framework for secure model generalization in federated learning</article-title>. <source>IEEE Access</source>. <year>2024</year>.</mixed-citation></ref><ref id="pone.0321179.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Asif</surname><given-names>M</given-names></name>, <name><surname>Abrar</surname><given-names>M</given-names></name>, <name><surname>Salam</surname><given-names>A</given-names></name>, <name><surname>Amin</surname><given-names>F</given-names></name>, <name><surname>Ullah</surname><given-names>F</given-names></name>, <name><surname>Shah</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Intelligent two-phase dual authentication framework for Internet of Medical Things</article-title>. <source>Sci Rep</source>. <year>2025</year>;<volume>15</volume>(<issue>1</issue>):<fpage>1760</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-024-84713-5</pub-id>
<pub-id pub-id-type="pmid">39800740</pub-id>
</mixed-citation></ref><ref id="pone.0321179.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Rahman</surname><given-names>S</given-names></name>, <name><surname>Uddin</surname><given-names>J</given-names></name>, <name><surname>Hussain</surname><given-names>H</given-names></name>, <name><surname>Shah</surname><given-names>S</given-names></name>, <name><surname>Salam</surname><given-names>A</given-names></name>, <name><surname>Amin</surname><given-names>F</given-names></name>, <etal>et al</etal>. <article-title>A novel and efficient digital image steganography technique using least significant bit substitution</article-title>. <source>Sci Rep</source>. <year>2025</year>;<volume>15</volume>(<issue>1</issue>):<fpage>107</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-024-83147-3</pub-id>
<pub-id pub-id-type="pmid">39747911</pub-id>
</mixed-citation></ref><ref id="pone.0321179.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Abdu Salam</surname><given-names>MA</given-names></name>, <name><surname>Amin</surname><given-names>F</given-names></name>, <name><surname>Ullah</surname><given-names>F</given-names></name>, <name><surname>Ahmad Khan</surname><given-names>I</given-names></name>, <name><surname>Alkhamees</surname><given-names>BF</given-names></name>, <name><surname>AlSalman</surname><given-names>H</given-names></name>. <article-title>Securing smart manufacturing by integrating anomaly detection with zero-knowledge proofs</article-title>. <source>IEEE Access</source>. <year>2024</year>.</mixed-citation></ref><ref id="pone.0321179.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Chen</surname><given-names>B</given-names></name>. <article-title>Detecting false data injection attacks in smart grids: a semi-supervised deep learning approach</article-title>. <source>IEEE Trans Smart Grid</source>. <year>2021</year>;<volume>12</volume>(<issue>1</issue>):<fpage>623</fpage>&#x02013;<lpage>34</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tsg.2020.3010510</pub-id></mixed-citation></ref><ref id="pone.0321179.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Jorjani</surname><given-names>M</given-names></name>, <name><surname>Seifi</surname><given-names>H</given-names></name>, <name><surname>Varjani</surname><given-names>AY</given-names></name>. <article-title>A graph theory-based approach to detect false data injection attacks in power system AC state estimation</article-title>. <source>IEEE Trans Ind Inf</source>. <year>2021</year>;<volume>17</volume>(<issue>4</issue>):<fpage>2465</fpage>&#x02013;<lpage>75</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tii.2020.2999571</pub-id></mixed-citation></ref><ref id="pone.0321179.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Du</surname><given-names>M</given-names></name>, <name><surname>Pierrou</surname><given-names>G</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Kassouf</surname><given-names>M</given-names></name>. <article-title>Targeted false data injection attacks against AC state estimation without network parameters</article-title>. <source>IEEE Trans Smart Grid</source>. <year>2021</year>;<volume>12</volume>(<issue>6</issue>):<fpage>5349</fpage>&#x02013;<lpage>61</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tsg.2021.3106246</pub-id></mixed-citation></ref><ref id="pone.0321179.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Mukherjee</surname><given-names>M</given-names></name>, <name><surname>Hardy</surname><given-names>T</given-names></name>, <name><surname>Fuller</surname><given-names>JC</given-names></name>, <name><surname>Bose</surname><given-names>A</given-names></name>. <article-title>Implementing multi-settlement decentralized electricity market design for transactive communities with imperfect communication</article-title>. <source>Appl Energy</source>. <year>2022</year>;<volume>306</volume>:<fpage>117979</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.apenergy.2021.117979</pub-id></mixed-citation></ref><ref id="pone.0321179.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Schmidt-Hieber</surname><given-names>J</given-names></name>. <article-title>The Kolmogorov-Arnold representation theorem revisited</article-title>. <source>Neural Netw</source>. <year>2021</year>;<volume>137</volume>:<fpage>119</fpage>&#x02013;<lpage>26</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neunet.2021.01.020</pub-id>
<pub-id pub-id-type="pmid">33592434</pub-id>
</mixed-citation></ref><ref id="pone.0321179.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Jiang</surname><given-names>W</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Hsiung</surname><given-names>K-L</given-names></name>, <name><surname>Chen</surname><given-names>H-Y</given-names></name>. <article-title>Grnn-based detection of eavesdropping attacks in swipt-enabled smart grid wireless sensor networks</article-title>. <source>IEEE Internet of Things J</source>. <year>2024</year>.</mixed-citation></ref><ref id="pone.0321179.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Suh</surname><given-names>N</given-names></name>, <name><surname>Cheng</surname><given-names>G</given-names></name>. <article-title>A survey on statistical theory of deep learning: approximation, training dynamics, and generative models</article-title>. <source>Annu Rev Statist Appl</source>. <year>2024</year>;<volume>12</volume>.</mixed-citation></ref><ref id="pone.0321179.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Koenig</surname><given-names>BC</given-names></name>, <name><surname>Kim</surname><given-names>S</given-names></name>, <name><surname>Deng</surname><given-names>S</given-names></name>. <article-title>KAN-ODEs: Kolmogorov&#x02013;Arnold network ordinary differential equations for learning dynamical systems and hidden physics</article-title>. <source>Comput Methods Appl Mech Eng</source>. <year>2024</year>;<volume>432</volume>:<fpage>117397</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cma.2024.117397</pub-id></mixed-citation></ref><ref id="pone.0321179.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>H</given-names></name>, <name><surname>Zhang</surname><given-names>M</given-names></name>, <name><surname>Shi</surname><given-names>JQ</given-names></name>. <article-title>A survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2024</year>.</mixed-citation></ref><ref id="pone.0321179.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Lei</surname><given-names>B</given-names></name>, <name><surname>Liang</surname><given-names>E</given-names></name>, <name><surname>Yang</surname><given-names>M</given-names></name>, <name><surname>Yang</surname><given-names>P</given-names></name>, <name><surname>Zhou</surname><given-names>F</given-names></name>, <name><surname>Tan</surname><given-names>E-L</given-names></name>, <etal>et al</etal>. <article-title>Predicting clinical scores for Alzheimer&#x02019;s disease based on joint and deep learning</article-title>. <source>Exp Syst Appl</source>. <year>2022</year>;<volume>187</volume>:<fpage>115966</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.eswa.2021.115966</pub-id></mixed-citation></ref><ref id="pone.0321179.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Kaack</surname><given-names>LH</given-names></name>, <name><surname>Donti</surname><given-names>PL</given-names></name>, <name><surname>Strubell</surname><given-names>E</given-names></name>, <name><surname>Kamiya</surname><given-names>G</given-names></name>, <name><surname>Creutzig</surname><given-names>F</given-names></name>, <name><surname>Rolnick</surname><given-names>D</given-names></name>. <article-title>Aligning artificial intelligence with climate change mitigation</article-title>. <source>Nat Clim Chang</source>. <year>2022</year>;<volume>12</volume>(<issue>6</issue>):<fpage>518</fpage>&#x02013;<lpage>27</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41558-022-01377-7</pub-id></mixed-citation></ref><ref id="pone.0321179.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Shaik</surname><given-names>T</given-names></name>, <name><surname>Tao</surname><given-names>X</given-names></name>, <name><surname>Li</surname><given-names>L</given-names></name>, <name><surname>Xie</surname><given-names>H</given-names></name>, <name><surname>Vel&#x000e1;squez</surname><given-names>JD</given-names></name>. <article-title>A survey of multimodal information fusion for smart healthcare: mapping the journey from data to wisdom</article-title>. <source>Inf Fusion</source>. <year>2024</year>;<volume>102</volume>:<fpage>102040</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.inffus.2023.102040</pub-id></mixed-citation></ref><ref id="pone.0321179.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Vatter</surname><given-names>J</given-names></name>, <name><surname>Mayer</surname><given-names>R</given-names></name>, <name><surname>Jacobsen</surname><given-names>H-A</given-names></name>. <article-title>The evolution of distributed systems for graph neural networks and their origin in graph processing and deep learning: a survey</article-title>. <source>ACM Comput Surv</source>. <year>2023</year>;<volume>56</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>37</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/3597428</pub-id></mixed-citation></ref><ref id="pone.0321179.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Saha</surname><given-names>SS</given-names></name>, <name><surname>Sandha</surname><given-names>SS</given-names></name>, <name><surname>Srivastava</surname><given-names>M</given-names></name>. <article-title>Machine learning for microcontroller-class hardware: a review</article-title>. <source>IEEE Sens J</source>. <year>2022</year>;<volume>22</volume>(<issue>22</issue>):<fpage>21362</fpage>&#x02013;<lpage>90</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/jsen.2022.3210773</pub-id>
<pub-id pub-id-type="pmid">36439060</pub-id>
</mixed-citation></ref></ref-list></back></article>