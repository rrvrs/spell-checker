<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="data-paper" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Data</journal-id><journal-id journal-id-type="iso-abbrev">Sci Data</journal-id><journal-title-group><journal-title>Scientific Data</journal-title></journal-title-group><issn pub-type="epub">2052-4463</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40410207</article-id><article-id pub-id-type="pmc">PMC12102372</article-id>
<article-id pub-id-type="publisher-id">5174</article-id><article-id pub-id-type="doi">10.1038/s41597-025-05174-7</article-id><article-categories><subj-group subj-group-type="heading"><subject>Data Descriptor</subject></subj-group></article-categories><title-group><article-title>A large-scale MEG and EEG dataset for object recognition in naturalistic scenes</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Guohao</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Ming</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Zhen</surname><given-names>Shuyi</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Tang</surname><given-names>Shaohua</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Zheng</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6748-6434</contrib-id><name><surname>Zhen</surname><given-names>Zonglei</given-names></name><address><email>zhenzonglei@bnu.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/022k4wk35</institution-id><institution-id institution-id-type="GRID">grid.20513.35</institution-id><institution-id institution-id-type="ISNI">0000 0004 1789 9964</institution-id><institution>Beijing Key Laboratory of Applied Experimental Psychology, Faculty of Psychology, </institution><institution>Beijing Normal University, </institution></institution-wrap>Beijing, 100875 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/022k4wk35</institution-id><institution-id institution-id-type="GRID">grid.20513.35</institution-id><institution-id institution-id-type="ISNI">0000 0004 1789 9964</institution-id><institution>State Key Laboratory of Cognitive Neuroscience and Learning &#x00026; IDG/McGovern Institute for Brain Research, </institution><institution>Beijing Normal University, </institution></institution-wrap>Beijing, 100875 China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/022k4wk35</institution-id><institution-id institution-id-type="GRID">grid.20513.35</institution-id><institution-id institution-id-type="ISNI">0000 0004 1789 9964</institution-id><institution>Department of Systems Science, Faculty of Arts and Sciences, </institution><institution>Beijing Normal University, </institution></institution-wrap>Zhuhai, 519087 China </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/022k4wk35</institution-id><institution-id institution-id-type="GRID">grid.20513.35</institution-id><institution-id institution-id-type="ISNI">0000 0004 1789 9964</institution-id><institution>Department of Psychology, Faculty of Arts and Sciences, </institution><institution>Beijing Normal University, </institution></institution-wrap>Zhuhai, 519087 China </aff></contrib-group><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>12</volume><elocation-id>857</elocation-id><history><date date-type="received"><day>21</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>9</day><month>5</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Neuroimaging with large-scale naturalistic stimuli is increasingly employed to elucidate neural mechanisms of object recognition in natural scenes. However, most existing large-scale neuroimaging datasets with naturalistic stimuli primarily rely on functional magnetic resonance imaging (fMRI), which provides high spatial resolution but is limited in capturing the temporal dynamics. To address this limitation, we extended our Natural Object Dataset-fMRI (NOD-fMRI) by collecting both magnetoencephalography (MEG) and electroencephalography (EEG) data from the same participants while viewing the same naturalistic stimuli. As a result, NOD contains fMRI, MEG, and EEG responses to 57,000 naturalistic images from 30 participants. This enables the examination of brain activity elicited by naturalistic stimuli with both high spatial resolution (via fMRI) and high temporal resolution (via MEG and EEG). Furthermore, the multimodal nature of NOD allows researchers to combine datasets from different modalities to achieve a more comprehensive view of object processing. We believe that the NOD dataset will serve as a valuable resource for advancing our understanding of the cognitive and neural mechanisms underlying object recognition.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Perception</kwd><kwd>Object vision</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China (National Science Foundation of China)</institution></institution-wrap></funding-source><award-id>62433015</award-id><award-id>31771251</award-id><principal-award-recipient><name><surname>Zhen</surname><given-names>Zonglei</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>Funder: STI 2030-Major Projects of the Ministry of Science and Technology of China Grant Reference Number: 2021ZD0200407</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background &#x00026; Summary</title><p id="Par2">An important goal of visual neuroscience is to understand the neural mechanisms by which the human brain rapidly and effortlessly recognizes various objects in natural scenes encountered in daily life. To this end, the naturalistic paradigm is becoming increasingly popular in visual neuroscience, which utilizes complex, dynamic, and diverse naturalistic stimuli<sup><xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR6">6</xref></sup>. Emerging evidence shows naturalistic stimuli, which encapsulate a wide variety of visual features and cognitive contexts, can induce highly reproducible brain responses and thus help researchers accurately characterize the spatiotemporal activation patterns of the brain across a wide range of spatiotemporal scales<sup><xref ref-type="bibr" rid="CR7">7</xref>&#x02013;<xref ref-type="bibr" rid="CR10">10</xref></sup>. In recent years, a series of high-quality, large-scale brain imaging datasets based on naturalistic visual stimuli have been released to propel the field of human visual neuroscience, including datasets that use discrete natural images or clips as stimuli, such as Natural Scenes Dataset (NSD)<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, Natural Object Dataset (NOD)<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, Human Action Dataset (HAD)<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, Bold Moments Dataset (BMD)<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, and THINGS<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>, as well as datasets that use continuous movies as stimuli, such as The Grand Budapest Hotel<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, Forrest Gump<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. These datasets provide valuable resources for exploring the brain organization and coding principles of the visual cortex in processing naturalistic stimuli. Most of these datasets are functional magnetic resonance imaging (fMRI) data, and only a few datasets collect magnetoencephalography (MEG) or electroencephalography (EEG) data using naturalistic visual stimuli.</p><p id="Par3">Functional MRI excels at capturing spatial activity patterns because of its high spatial resolution. Yet, the low temporal resolution limits its ability to accurately characterize the neural spatiotemporal dynamics of object recognition in naturalistic scenes. In contrast to fMRI, MEG and EEG record neural magnetic and electrical activity with millisecond-level temporal resolution, making them ideal for examining the rapid temporal dynamics of neural activity<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>. Moreover, MEG and EEG show complementary sensitivity profiles and field patterns<sup><xref ref-type="bibr" rid="CR20">20</xref>&#x02013;<xref ref-type="bibr" rid="CR22">22</xref></sup>. MEG is more sensitive to tangential sources located in the sulci, while EEG is more sensitive to radial sources located in the gyri. However, to our knowledge, only two publicly available datasets collect both fMRI and M/EEG data under identical large-scale naturalistic stimuli: the StudyForest datasets, which include fMRI<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> and MEG<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> data with the Forrest Gump movie as stimuli. THINGS<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>, as the first dataset to include fMRI, MEG, and EEG modalities in response to natural images, has served as an excellent reference template for investigating natural object representations in human brain with different techniques. However, given the complex and rich information contained in natural images, more data are still needed to accurately characterize the spatiotemporal patterns of brain activity in response to the natural stimuli.</p><p id="Par4">To address this gap, we extended the previously collected NOD-fMRI dataset by acquiring MEG and EEG data from the same participants and stimuli, making NOD a multimodal dataset integrating both high temporal resolution (M/EEG) and high spatial resolution (fMRI). Inheriting from NOD-fMRI, the NOD&#x02019;s MEG and EEG aims to accurately capture brain activation patterns and to characterize both within-individual differences elicited by various naturalistic stimuli and inter-individual variability from similar stimuli. Firstly, to accurately capture the differences in neural activation patterns in response to diverse stimuli within individuals, we initially collected MEG and EEG data from 9 participants viewing 4,000 images from ImageNet. Secondly, to better capture inter-individual differences, we collected EEG and MEG data from an additional 21 participants who viewed at least 1,000 images from ImageNet. This expansion resulted in a comprehensive dataset (N&#x02009;=&#x02009;30) that is particularly effective at characterizing variability across individuals than other publicly available naturalistic neuroimaging datasets, which typically include fewer than 10 participants.</p><p id="Par5">Compared to the landmark multimodal naturalistic neuroimaging dataset THINGS, the NOD exhibits several distinct features. Firstly, the NOD dataset includes EEG, MEG, and fMRI data collected from the same participants, whereas the EEG and fMRI/MEG data in the THINGS dataset are derived from separate participant groups. Secondly, the NOD dataset comprises a larger number of participants than the THINGS dataset. We anticipate that including MEG and EEG data for the NOD dataset will facilitate precise characterization of the human brain&#x02019;s neural activity patterns involved in processing objects within naturalistic scenes and advance our understanding of the neural mechanisms underlying human object recognition.</p></sec><sec id="Sec2"><title>Methods</title><sec id="Sec3"><title>Participants</title><p id="Par6">The participants in NOD-MEG and NOD-EEG experiments were recruited from the same cohort that had previously participated in the NOD-fMRI experiment, thereby ensuring that the fMRI, EEG, and MEG data were obtained from the same individuals. A total of 30 healthy participants (18 females, mean age&#x02009;&#x000b1;&#x02009;standard deviation [SD], 21.23&#x02009;&#x000b1;&#x02009;1.98 years) took part in the NOD-MEG experiment. Among them, 19 participants (8 females, mean age&#x02009;&#x000b1;&#x02009;SD, 21.26&#x02009;&#x000b1;&#x02009;2.00 years) were involved in the NOD-EEG experiment. All participants had normal or corrected-to-normal vision, reported no history of psychiatric or neurological disorders, and provided written informed consent for both their participation and the sharing of their anonymized data. The Institutional Review Board of Beijing Normal University approved the study (approval number: ICBIR_A_0111_001_02).</p></sec><sec id="Sec4"><title>Experiment design</title><sec id="Sec5"><title>Stimuli</title><p id="Par7">Both the NOD-MEG and NOD-EEG experiments used the same stimuli as the NOD-fMRI data. Specifically, a three-stage procedure was utilized to select 60,000 candidate stimuli from ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012)<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, which contains more than one million images annotated of the 1000 object categories. Initially, 60 images were randomly sampled for each category, with criteria requiring a square aspect ratio (&#x02248;1) and high resolution (&#x0003e;100,000 pixels). These images were then visually inspected to identify blurred or mislabelled instances. Finally, the detected improper images were replaced with those meeting the initial criteria, ensuring the integrity and quality of the dataset. Then, the 60,000 images were used as the stimuli superset from which images were selected for the MEG and EEG experiments.</p></sec><sec id="Sec6"><title>MEG experiment</title><p id="Par8">Each trial of NOD-MEG lasted approximately 1500&#x02009;ms, with the stimulus presented for 800&#x02009;ms, followed by a variable fixation period of 700&#x02009;&#x000b1;&#x02009;200&#x02009;ms. The stimuli were loaded using Psychophysics Toolbox Version 3 (PTB-3) in MATLAB 2016 (MathWorks, Natick, Massachusetts, USA), generated by a standard PCI graphics card (GeForce GT620; NVIDIA, Santa Clara, California, USA), and then projected onto a screen with a resolution of 1024&#x02009;&#x000d7;&#x02009;768 pixels via a DLP projector (NP63+; NEC, Tokyo, Japan). Participants viewed the visual stimuli on the projection screen via a mirror reflection. Each stimulus was 600&#x02009;&#x000d7;&#x02009;600 pixels in size (visual angles&#x02009;=&#x02009;16&#x000b0;; viewing distance&#x02009;=&#x02009;700&#x02009;mm, Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1b</xref>). Each run of the experiment consisted of 200 trials, lasting approximately 390&#x02009;seconds. In the experiment, participants were asked to press a button to indicate whether the object presented in the image was animate. Among the 30 participants, 9 completed 4 MEG sessions, each viewing a total of 4000 unique images (four images/category). These four sessions comprised ses-ImageNet01 (2 runs), ses-ImageNet02 (2 runs), ses-ImageNet03 (8 runs), and ses-ImageNet04 (8 runs). The remaining 21 participants completed only 1 MEG session, namely ses-ImageNet01 (5 runs), and each viewed 1000 unique images (one image/category). Therefore, we collected a total of 25&#x02009;hours of MEG data on 30 participants with 57000 images as stimuli (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1a</xref>). Additionally, we also collected MRI structural images for each participant to facilitate source localization of MEG signals.<fig id="Fig1"><label>Fig. 1</label><caption><p>Design of the Natural Object Dataset (NOD) experiments. (<bold>a</bold>) The stimuli used in the NOD-M/EEG experiment are derived from the ImageNet database, which comprises 1,000 object categories, with 57 images per category. In total, 25&#x02009;hours MEG data (273 channels) were acquired from 30 participants and 24&#x02009;hours EEG data (64 channels) were acquired from 19 participants. (<bold>b</bold>) In the experiments, an image was presented for 0.8&#x02009;seconds, followed by a fixation cross for 0.7&#x02009;seconds, with a jitter of&#x02009;&#x000b1;&#x02009;0.2&#x02009;seconds.</p></caption><graphic xlink:href="41597_2025_5174_Fig1_HTML" id="d33e333"/></fig></p></sec><sec id="Sec7"><title>EEG experiment</title><p id="Par9">Each trial of NOD-EEG lasted approximately 1500&#x02009;ms, with the stimulus presented for 800&#x02009;ms, followed by a variable fixation period of 700&#x02009;&#x000b1;&#x02009;200&#x02009;ms. The stimuli were loaded using Psychophysics Toolbox Version 3 (PTB-3) in MATLAB 2016 (MathWorks, Natick, Massachusetts, USA), generated by a standard PCI graphics card (GeForce GT620; NVIDIA, Santa Clara, California, USA), and then displayed on a screen with a resolution of 1440&#x02009;&#x000d7;&#x02009;960 pixels. Each stimulus was 600&#x02009;&#x000d7;&#x02009;600 pixels in size (visual angles&#x02009;=&#x02009;16&#x000b0;; viewing distance&#x02009;=&#x02009;700&#x02009;mm, Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1b</xref>). Each run of the experiment consisted of 125 trials, lasting approximately 190&#x02009;seconds. Consistent with the MEG experiment, participants were required to press a button to indicate whether the presented object was animate. Among the 19 participants, 9 completed 4 EEG sessions (ses-ImageNet01-04), each consisting of eight runs and 4,000 unique images (four images/category) viewed per participant. The remaining 10 participants completed 2 EEG sessions (ses-ImageNet01-02), also with eight runs per session and 2,000 unique images (two images/category) viewed per participant. As a result, we collected a total of 24&#x02009;hours of EEG data from 19 participants with 56000 images as stimuli (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1a</xref>).</p></sec></sec><sec id="Sec8"><title>Data acquisition</title><sec id="Sec9"><title>MEG recordings</title><p id="Par10">MEG data were recorded using a 275-channel whole-head axial gradiometer DSQ3500 MEG system (CTF, Canada) at the Institute of Biophysics, Chinese Academy of Sciences, Beijing, China. Three channels (MLF55, MRT23, and MRT16) were not used due to equipment malfunctions. The MEG signal sampling rate was 1200&#x02009;Hz, without the use of online filters during acquisition. To accurately synchronize stimuli with neural signals, we used real-time markers to record the sampling points of stimulus presentations and participant responses, with all marker information stored in the UPPT001 stimulus channel of the MEG data. At the start of each session, three head position indicator coils were attached to the participant&#x02019;s nasion (NAS), left preauricular point (LPA), and right preauricular point (RPA) to measure their head position in the MEG helmet. A custom chin rest was used during data collection to minimize potential head movement of the participants.</p></sec><sec id="Sec10"><title>EEG recordings</title><p id="Par11">EEG data were recorded using a 64-channel EEG system with an international 10&#x02013;20 system (NeuroScan, USA) at the Faculty of Psychology, Beijing Normal University, Beijing, China. The EEG signal was recorded using Curry Neuroimage 8 with sampling rate as 500&#x02009;Hz, amplified by a SynAmps 2 amplifier (Compumedics NeuroScan). The entire EEG acquisition process was conducted in an electromagnetically shielded room, and participants were instructed to minimize movement during data acquisition.</p></sec><sec id="Sec11"><title>Structural MRI</title><p id="Par12">Structural T1w images were acquired for anatomical reference with a three-dimensional magnetization-prepared rapid acquisition gradient echo sequence (1 slab; 208 sagittal slices; FOV, 256&#x02009;&#x000d7;&#x02009;256&#x02009;mm; slice thickness, 1&#x02009;mm; isotropic voxel size, 1&#x02009;&#x000d7;&#x02009;1&#x02009;&#x000d7;&#x02009;1&#x02009;mm; TR, 2530&#x02009;ms; TE, 2.27&#x02009;ms; TI, 1100&#x02009;ms; and flip angle, 7&#x000b0;) using a Siemens MAGNETOM Prisma 3&#x02009;T MRI scanner with a 64-channel phased-array head coil at the Imaging Center for Brain Research, Beijing Normal University, Beijing, China .<fig id="Fig2"><label>Fig. 2</label><caption><p>Overview of data processing pipeline and the shared data. The raw M/EEG data were primarily processed using the MNE software suite. The shared data includes the raw, preprocessed, and epoched data.</p></caption><graphic xlink:href="41597_2025_5174_Fig2_HTML" id="d33e364"/></fig></p></sec><sec id="Sec12"><title>Data preprocessing</title><p id="Par13">Both MEG and EEG data were preprocessed offline by combining different tools including MNE-BIDS<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>, MNE-Python<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>, MEGnet<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, MNE-ICALabel<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, meegkit<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> and pyprep<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. The data processing workflow generally included the following six steps, after which the raw data, preprocessed time series (preprocessed data), and trial-segmented (epoched) data for each participant were shared (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>).<list list-type="order"><list-item><p id="Par14">BIDS conversion. The raw MEG and EEG data were first converted to Brain-Imaging-Data-Structure (BIDS) format. During this process, we manually checked the data integrity and wrote participant information, experiment time, and other details into the BIDS structure.</p></list-item><list-item><p id="Par15">Downsample. M/EEG data were downsampled to 250&#x02009;Hz to reduce the computational burden of downstream analysis.</p></list-item><list-item><p id="Par16">Bad sensor fixing. In M/EEG, sensors can sometimes be too noisy or quiet, making it difficult to record brain signals. We used established automatic bad channel detection algorithms (Maxwell<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> for MEG, RANSAC<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> for EEG) to detect bad channels in each run of M/EEG. No bad channels were found in the MEG data, while 1.34 bad channels (SD&#x02009;=&#x02009;2.32) on average were detected per EEG data run. The bad channels were then corrected using interpolation algorithms (spherical splines&#x000a0;interpolation<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> for EEG, field map interpolation<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> for MEG).</p></list-item><list-item><p id="Par17">Noise reduction. Due to the high sensitivity of M/EEG recordings, weak brain signals are often mixed with environmental noise. The Zapline algorithm<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> was used to reduce power line noise (50&#x02009;Hz) in both MEG and EEG (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3a</xref>). Besides, the CTF&#x02019;s third-order gradient compensation algorithm was applied to reduce noise in the MEG data, and the average of all EEG channels was used to re-referencing the EEG data.<fig id="Fig3"><label>Fig. 3</label><caption><p>The effects of line noise reduction and ICA artifacts removal from a sample participant (i.e., sub01). (<bold>a</bold>) The Power Spectral Density (PSD) of the raw MEG and EEG data shows line noise at 50&#x02009;Hz and its harmonics (upper panel). The line noise was effectively removed using zapline denoising (lower panel). (<bold>b</bold>) The left panel shows the spatial maps and time series of typical artifact components extracted from M/EEG data via ICA decomposition. The right panel presents the data from two frontal lobe sensors in the M/EEG recordings: the upper section shows the raw signals before artifact rejection, while the lower section presents the cleaned signals following artifact rejection.</p></caption><graphic xlink:href="41597_2025_5174_Fig3_HTML" id="d33e460"/></fig></p></list-item><list-item><p id="Par18">ICA artifact rejection. Each run of MEG and EEG data was first filtered within the 1&#x02013;100&#x02009;Hz range to reduce the impact of low-frequency drifts and high-frequency noise on the ICA decomposition. The data was then decomposed into different independent components using ICA algorithm, and the artifacts components were identified by combining the automatic artifact selection algorithms (MEGnet<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> for MEG, MNE-ICALabel<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> for EEG) and manual inspection. Finally, these artifact components were removed on the 0.1&#x02013;100&#x02009;Hz time series through regression (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3b</xref>).</p></list-item><list-item><p id="Par19">Epoching. We divided the time window of each trial from &#x02212;100ms to 800&#x02009;ms relative to stimulus onset to fully capture the neural response induced by the stimulus. Baseline correction was performed using the time window from 100&#x02009;ms before onset of the images. Then, all trials for each participant were concatenated to create the participant&#x02019;s epoch data.</p></list-item></list></p></sec></sec></sec><sec id="Sec13"><title>Data Records</title><p id="Par20">Both MEG and EEG data were organized into the BIDS format via mne-bids and uploaded to the OpenNeuro public repository<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. The accession numbers are ds005810<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> and ds005811<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> for MEG and EEG data, respectively. In brief, the raw MEG and EEG data from each participant are stored in the &#x0201c;sub-&#x0003c;subID&#x0003e;&#x0201d; directory, while preprocessed data and epoch data are stored in the &#x0201c;derivatives/preprocessed/raw&#x0201d; and &#x0201c;derivatives/preprocessed/epochs&#x0201d; directories, respectively (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4a,b</xref>).<fig id="Fig4"><label>Fig. 4</label><caption><p>The directory and file structure of the Natural Object Dataset&#x02019;s M/EEG data. (<bold>a,</bold><bold>b</bold>) Overview of the directory and file structure for the MEG and EEG data, respectively. (<bold>c,</bold><bold>d</bold>) A detailed view of the file structure from a single sample subject (i.e., sub01) for the MEG and EEG data, respectively.</p></caption><graphic xlink:href="41597_2025_5174_Fig4_HTML" id="d33e518"/></fig></p><sec id="Sec14"><title>Stimulus images</title><p id="Par21">The stimuli images used for MEG and EEG are identical and are stored in the &#x0201c;stimuli/ImageNet&#x0201d; directory. Images within the folder are named in the format &#x0201c;&#x0003c;synsetID&#x0003e;_&#x0003c;imageID&#x0003e;.JPEG&#x0201d;, where synsetID is the ImageNet category information, and imageID is the unique number for the image within that category. The image metadata including category information is available in the table files under &#x0201c;stimuli/metadata&#x0201d;.</p></sec><sec id="Sec15"><title>Raw data</title><p id="Par22">Raw MEG and EEG data are stored in BIDS format in the NOD-MEG and NOD-EEG directories, respectively (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4c,d</xref>). Each participant&#x02019;s directory contains multiple session folders, designated as &#x0201c;ses-&#x0003c;sesID&#x0003e;&#x0201d;, which in turn include &#x0201c;meg&#x0201d; or &#x0201c;eeg&#x0201d; data folders. The &#x0201c;ses-MRI/anat&#x0201d; directory contains the participant&#x02019;s structural MRI images. The comprehensive trial information for each participant is documented in the file &#x0201c; derivatives/detailed_events/sub-&#x0003c;subID&#x0003e;_events.csv&#x0201d; in which each row corresponds to a trial, and each column contains metadata for that trial, including the session and run ID, category information of the stimuli, and participant response.</p></sec><sec id="Sec16"><title>Preprocessed data</title><p id="Par23">The full-time series data of preprocessed MEG and EEG are archived in the &#x0201c;derivatives/raw&#x0201d; directory, named &#x0201c;sub-&#x0003c;subID&#x0003e;_ses-&#x0003c;sesID&#x0003e;_task-ImageNet_run-&#x0003c;runID&#x0003e;_&#x0003c;meg/eeg&#x0003e;_clean.fif&#x0201d;. The epoch data derived from preprocessed MEG and EEG are stored within the &#x0201c;derivatives/epochs&#x0201d; directory. In this directory, all data for each participant are concatenated into a single file, named as &#x0201c;sub-&#x0003c;subID&#x0003e;_epo.fif&#x0201d;. The trial information within each participant&#x02019;s epochs data can be accessed via the metadata of the epochs data, which are aligned with the content of the participant&#x02019;s &#x0201c;sub-&#x0003c;subID&#x0003e;_events.csv&#x0201d; file.</p></sec></sec><sec id="Sec17"><title>Technical Validation</title><sec id="Sec18"><title>Recognition accuracy</title><p id="Par24">Participants&#x02019; understanding and active engagement in the experimental task are crucial for the success of the neuroscience experiment. For this, the recognition accuracy for each participant in each session was calculated to evaluate their task engagement. The average recognition accuracy for all participants was 83.69%, with no recognition accuracy lower than 70% in all sessions (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5a</xref>). This suggests that participants were effectively engaged in the task of determining the animacy of the presented images.<fig id="Fig5"><label>Fig. 5</label><caption><p>Basic quality metrics for M/EEG datasets. (<bold>a</bold>) The mean recognition accuracy for each subject in each session of the MEG and EEG experiments. (<bold>b</bold>) Quantification of MEG head motion. The within-session motion is measured as the Euclidean distance between head coil positions across runs, while the between-session motion is evaluated as the Euclidean distance between head coil positions across different sessions.</p></caption><graphic xlink:href="41597_2025_5174_Fig5_HTML" id="d33e556"/></fig></p></sec><sec id="Sec19"><title>Head motion</title><p id="Par25">Changes of head position in MEG recordings can significantly impact the data quality of MEG. As during MEG data collection, the coordinates of three reference points on the human brain (i.e., NAS, LPA, and RPA) are recorded, we can quantify intra-session head motion (between runs) for each participant by calculating the distance between initial run positions, and evaluate inter-session head motion by calculating the average positional change across sessions. As shown in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5b</xref>, all participants exhibited consistently low head motion (median&#x02009;=&#x02009;0.14&#x02009;mm) within sessions. Although head motion between sessions was slightly higher, it remained at a very low level overall (median&#x02009;=&#x02009;1.01&#x02009;mm), indicating participants show good control of head motion within the dataset. Since EEG data collection does not record the head position, the head motion in EEG recordings was not assessed.</p></sec><sec id="Sec20"><title>Temporal dynamics of face representation</title><p id="Par26">A large body of research has demonstrated that MEG or EEG signals originating from the occipitotemporal lobe can effectively characterize the temporal dynamics of object category representation<sup><xref ref-type="bibr" rid="CR38">38</xref>&#x02013;<xref ref-type="bibr" rid="CR47">47</xref></sup>. Here, we attempt to replicate findings from previous studies on the temporal dynamics of face representation using M/EEG signals from the occipitotemporal lobe (MEG 108 sensors, EEG 16 sensors). Specifically, we constructed a linear support vector machine (SVM) with the spatiotemporal patterns from occipitotemporal sensors at each time point as the input to perform face versus object decoding. To this end, we first used InsightFace<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> to label the stimuli image set into face or object categories. Moreover, to address the imbalance in sample sizes between face and object, we randomly selected samples from the object category to match sample size between object and face. A 10-fold cross-validation procedure was used to estimate classification accuracy. As shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>, both MEG and EEG data reliably decoded the presence of face stimuli in the images and exhibited similar temporal dynamics. However, MEG demonstrated better performance than EEG in decoding face versus object after the onset of decoding timing (~250&#x02009;ms). This indicates that both MEG and EEG data reliably contain information about object category representation but show different sensitivities.<fig id="Fig6"><label>Fig. 6</label><caption><p>Decoding analysis of face vs. object based on the spatial pattern at each time point measured by MEG and EEG shows temporal dynamics of face representation. The line represents the mean decoding accuracy across all subjects, and the dots below indicate time points where the decoding accuracy is significantly larger than chance level (&#x0003e;50%).</p></caption><graphic xlink:href="41597_2025_5174_Fig6_HTML" id="d33e588"/></fig></p></sec><sec id="Sec21"><title>M/EEG-fMRI fusion analysis</title><p id="Par27">Leveraging the fusion analysis of the MEG, EEG, and fMRI collected with the same stimuli on the same group of participants, the NOD data provides avenues to explore the spatiotemporal dynamics of object representation. To prove this possibility, we perform a similarity-based fusion analysis of fMRI and M/EEG<sup><xref ref-type="bibr" rid="CR49">49</xref>,<xref ref-type="bibr" rid="CR50">50</xref></sup> to examine the temporal dynamics of the ventral temporal cortex (VTC)<sup><xref ref-type="bibr" rid="CR51">51</xref>&#x02013;<xref ref-type="bibr" rid="CR53">53</xref></sup>. For the fMRI data, the region of interest (ROI) for the VTC was defined according to the Human Connectome Project Multi-Modal Parcellation (HCP-MMP)<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>, including PIT, V8, VVC, FFC, PH, TE2p, PHA1-PHA3, VMV1-VMV3. The fMRI representational dissimilarity matrix (fMRI-RDM) for the 1000 ImageNet categories was constructed by calculating the correlation distance between the group-level VTC activation patterns (beta maps) obtained by averaging the activation patterns from each category across runs and participants. For the M/EEG data, the occipitotemporal sensors approximately corresponding to the VTC were selected as the ROI (MEG 108 sensors, EEG 16 sensors). The M/EEG-RDMs for the 1000 categories were constructed at each time point by calculating the correlation distance between the group-level category specific spatial patterns which were generated by averaging the activation patterns for each category across all runs and all participants.</p><p id="Par28">The Spearman correlation coefficients between the RDMs from fMRI, MEG, and EEG were computed at each time point to reveal the temporal dynamics of VTC processing naturalistic stimuli and a non-parametric bootstrap method was used to estimate the significance of the Spearman correlation coefficient. Several findings were revealed (Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>). First, the category representations measured by both MEG and EEG exhibited strong similarity to the category representations of fMRI, indicating that the two modalities shared some common representation information with the fMRI. Second, EEG demonstrated greater representational similarity to fMRI in the early stage (&#x0003c;300&#x02009;ms), while MEG showed a larger representational similarity to fMRI in the late stage (&#x0003e;400&#x02009;ms). Finally, the similarity in category representation between MEG and EEG is relatively lower than the similarity between them and fMRI, indicating substantial differences in the origins of MEG and EEG signals or low signal to noise ratio of them. These results suggest that the fusion of fMRI and M/EEG could help us depict the representational dynamics of VTC in object recognition.<fig id="Fig7"><label>Fig. 7</label><caption><p>The M/EEG-fMRI fusion analysis reveals the temporal dynamics of the ventral temporal cortex (VTC) in processing naturalistic visual stimuli. The representational dissimilarity matrix (RDM) for the 1000 ImageNet categories was computed with the neural spatial patterns measured by the fMRI, MEG, and EEG, respectively (the left and lower color bars of the RDMs indicate the corresponding ImageNet&#x02019;s superclass). The Spearman correlation coefficients between M/EEG RDMs and the fMRI RDM were then computed at each time point to integrate the spatiotemporal information from them. The shaded region around the line indicates the 95% confidence interval.</p></caption><graphic xlink:href="41597_2025_5174_Fig7_HTML" id="d33e622"/></fig></p></sec></sec><sec id="Sec22"><title>Usage Notes</title><p id="Par29">The present dataset expands upon the NOD-fMRI data by capturing neural activity with MEG and EEG from the same cohort of participants as they viewed the same set of naturalistic image stimuli. The complementary spatial and temporal information from the fMRI, MEG, and EEG data allows for a comprehensive elucidation of brain activity patterns in the processing of naturalistic image stimuli. The multimodal dataset also provides a platform for validating and improving fusion algorithms, helping researchers develop new analytical tools and models in cognitive neuroscience.</p><p id="Par30">Several key points should be noted when utilizing the dataset. Firstly, both participants and stimuli are aligned via unique identifiers (IDs) across the three modalities. That is, the same participant and stimuli ID represent the same participant and stimuli in all of three datasets. Secondly, to help user access the information of each trial, the stimuli information has been embedded directly into the metadata of the mne.Epochs object. Besides, both the preprocessed and epoch data of the M/EEG dataset are stored in the.fif file format, with all stimuli information for each participant stored in the &#x0201c;derivatives/detailed_events&#x0201d; file. Thirdly, the task in NOD asked participants to actively indicate whether each presented image was animate or inanimate, which differs from the tasks used in the THINGS (detecting a catch image) and NSD (detecting a new image). Since the task context has potential to affect the neural representations of objects<sup><xref ref-type="bibr" rid="CR55">55</xref>,<xref ref-type="bibr" rid="CR56">56</xref></sup>, it is important to consider such differences when comparing object representations across datasets. Finally, since a participant was involved in experiments for all three modalities with the same stimuli, the later-collected EEG/MEG data may exhibit familiarity and repetition effects relative to the earlier-collected fMRI data<sup><xref ref-type="bibr" rid="CR57">57</xref>,<xref ref-type="bibr" rid="CR58">58</xref></sup>. Researchers should be aware of this when trying to integrate these datasets into an analysis.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This research was supported by Beijing Natural Science Foundation (L247010), National Natural Science Foundation of China (62433015, 31771251) and STI 2030-Major Projects of the Ministry of Science and Technology of China (2021ZD0200407). The funders had no role in the study design; data collection, analysis, and interpretation; writing of the manuscript; or the decision to submit the article for publication.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Z.Z. conceived of the idea and design of the study. G.Z., M.Z., S.Z. and S.T., performed the study. G.Z. and Z.Z. wrote the manuscript. Z.Z. and Z.L. supervised the research.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>The code of the experimental design is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/BNUCNL/NaturalVisionProject">https://github.com/BNUCNL/NaturalVisionProject</ext-link>; the code for preprocessing and technical validation is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/colehank/NOD-MEEG">https://github.com/colehank/NOD-MEEG</ext-link>.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par31">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Felsen</surname><given-names>G</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><article-title>A natural approach to studying vision</article-title><source>Nat Neurosci</source><year>2005</year><volume>8</volume><fpage>1643</fpage><lpage>1646</lpage><pub-id pub-id-type="doi">10.1038/nn1608</pub-id><pub-id pub-id-type="pmid">16306891</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Felsen, G. &#x00026; Dan, Y. A natural approach to studying vision. <italic>Nat Neurosci</italic><bold>8</bold>, 1643&#x02013;1646 (2005).<pub-id pub-id-type="pmid">16306891</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>J-H</given-names></name><name><surname>Brang</surname><given-names>D</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name></person-group><article-title>Naturalistic stimuli: A paradigm for multiscale functional characterization of the human brain</article-title><source>Current Opinion in Biomedical Engineering</source><year>2021</year><volume>19</volume><fpage>100298</fpage><pub-id pub-id-type="doi">10.1016/j.cobme.2021.100298</pub-id><pub-id pub-id-type="pmid">34423178</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Zhang, Y., Kim, J.-H., Brang, D. &#x00026; Liu, Z. Naturalistic stimuli: A paradigm for multiscale functional characterization of the human brain. <italic>Current Opinion in Biomedical Engineering</italic><bold>19</bold>, 100298 (2021).<pub-id pub-id-type="pmid">34423178</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Sonkusare</surname><given-names>S</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name></person-group><article-title>Naturalistic Stimuli in Neuroscience: Critically Acclaimed</article-title><source>Trends in Cognitive Sciences</source><year>2019</year><volume>23</volume><fpage>699</fpage><lpage>714</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.05.004</pub-id><pub-id pub-id-type="pmid">31257145</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Sonkusare, S., Breakspear, M. &#x00026; Guo, C. Naturalistic Stimuli in Neuroscience: Critically Acclaimed. <italic>Trends in Cognitive Sciences</italic><bold>23</bold>, 699&#x02013;714 (2019).<pub-id pub-id-type="pmid">31257145</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Poline</surname><given-names>J-B</given-names></name></person-group><article-title>Nature abhors a paywall: How open science can realize the potential of naturalistic stimuli</article-title><source>Neuroimage</source><year>2020</year><volume>216</volume><fpage>116330</fpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116330</pub-id><pub-id pub-id-type="pmid">31704292</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">DuPre, E., Hanke, M. &#x00026; Poline, J.-B. Nature abhors a paywall: How open science can realize the potential of naturalistic stimuli. <italic>Neuroimage</italic><bold>216</bold>, 116330 (2020).<pub-id pub-id-type="pmid">31704292</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Gifford, A. T. <italic>et al</italic>. The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes. Preprint at 10.48550/arXiv.2301.03198 (2023).</mixed-citation></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>J&#x000e4;&#x000e4;skel&#x000e4;inen</surname><given-names>IP</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name><name><surname>Glerean</surname><given-names>E</given-names></name><name><surname>Ahveninen</surname><given-names>J</given-names></name></person-group><article-title>Movies and narratives as naturalistic stimuli in neuroimaging</article-title><source>NeuroImage</source><year>2021</year><volume>224</volume><fpage>117445</fpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117445</pub-id><pub-id pub-id-type="pmid">33059053</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">J&#x000e4;&#x000e4;skel&#x000e4;inen, I. P., Sams, M., Glerean, E. &#x00026; Ahveninen, J. Movies and narratives as naturalistic stimuli in neuroimaging. <italic>NeuroImage</italic><bold>224</bold>, 117445 (2021).<pub-id pub-id-type="pmid">33059053</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Turini</surname><given-names>J</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>A neural mechanism for contextualizing fragmented inputs during naturalistic vision</article-title><source>eLife</source><year>2019</year><volume>8</volume><fpage>e48182</fpage><pub-id pub-id-type="doi">10.7554/eLife.48182</pub-id><pub-id pub-id-type="pmid">31596234</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Kaiser, D., Turini, J. &#x00026; Cichy, R. M. A neural mechanism for contextualizing fragmented inputs during naturalistic vision. <italic>eLife</italic><bold>8</bold>, e48182 (2019).<pub-id pub-id-type="pmid">31596234</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>Y</given-names></name><name><surname>Mohsenzadeh</surname><given-names>Y</given-names></name></person-group><article-title>Neural processing of naturalistic audiovisual events in space and time</article-title><source>Commun Biol</source><year>2025</year><volume>8</volume><fpage>110</fpage><pub-id pub-id-type="doi">10.1038/s42003-024-07434-5</pub-id><pub-id pub-id-type="pmid">39843939</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Hu, Y. &#x00026; Mohsenzadeh, Y. Neural processing of naturalistic audiovisual events in space and time. <italic>Commun Biol</italic><bold>8</bold>, 110 (2025).<pub-id pub-id-type="pmid">39843939</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Kringelbach</surname><given-names>ML</given-names></name><name><surname>Perl</surname><given-names>YS</given-names></name><name><surname>Tagliazucchi</surname><given-names>E</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name></person-group><article-title>Toward naturalistic neuroscience: Mechanisms underlying the flattening of brain hierarchy in movie-watching compared to rest and task</article-title><source>Sci. Adv.</source><year>2023</year><volume>9</volume><fpage>eade6049</fpage><pub-id pub-id-type="doi">10.1126/sciadv.ade6049</pub-id><pub-id pub-id-type="pmid">36638163</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">Kringelbach, M. L., Perl, Y. S., Tagliazucchi, E. &#x00026; Deco, G. Toward naturalistic neuroscience: Mechanisms underlying the flattening of brain hierarchy in movie-watching compared to rest and task. <italic>Sci. Adv.</italic><bold>9</bold>, eade6049 (2023).<pub-id pub-id-type="pmid">36638163</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Retter</surname><given-names>TL</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Uncovering the neural magnitude and spatio-temporal dynamics of natural image categorization in a fast visual stream</article-title><source>Neuropsychologia</source><year>2016</year><volume>91</volume><fpage>9</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2016.07.028</pub-id><pub-id pub-id-type="pmid">27461075</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Retter, T. L. &#x00026; Rossion, B. Uncovering the neural magnitude and spatio-temporal dynamics of natural image categorization in a fast visual stream. <italic>Neuropsychologia</italic><bold>91</bold>, 9&#x02013;28 (2016).<pub-id pub-id-type="pmid">27461075</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>EJ</given-names></name><etal/></person-group><article-title>A massive 7&#x02009;T fMRI dataset to bridge cognitive neuroscience and artificial intelligence</article-title><source>Nat Neurosci</source><year>2022</year><volume>25</volume><fpage>116</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00962-x</pub-id><pub-id pub-id-type="pmid">34916659</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Allen, E. J. <italic>et al</italic>. A massive 7&#x02009;T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. <italic>Nat Neurosci</italic><bold>25</bold>, 116&#x02013;126 (2022).<pub-id pub-id-type="pmid">34916659</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>Z</given-names></name><etal/></person-group><article-title>A large-scale fMRI dataset for the visual processing of naturalistic scenes</article-title><source>Sci Data</source><year>2023</year><volume>10</volume><fpage>559</fpage><pub-id pub-id-type="doi">10.1038/s41597-023-02471-x</pub-id><pub-id pub-id-type="pmid">37612327</pub-id>
</element-citation><mixed-citation id="mc-CR12" publication-type="journal">Gong, Z. <italic>et al</italic>. A large-scale fMRI dataset for the visual processing of naturalistic scenes. <italic>Sci Data</italic><bold>10</bold>, 559 (2023).<pub-id pub-id-type="pmid">37612327</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>M</given-names></name><etal/></person-group><article-title>A large-scale fMRI dataset for human action recognition</article-title><source>Sci Data</source><year>2023</year><volume>10</volume><fpage>415</fpage><pub-id pub-id-type="doi">10.1038/s41597-023-02325-6</pub-id><pub-id pub-id-type="pmid">37369643</pub-id>
</element-citation><mixed-citation id="mc-CR13" publication-type="journal">Zhou, M. <italic>et al</italic>. A large-scale fMRI dataset for human action recognition. <italic>Sci Data</italic><bold>10</bold>, 415 (2023).<pub-id pub-id-type="pmid">37369643</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Lahner</surname><given-names>B</given-names></name><etal/></person-group><article-title>Modeling short visual events through the BOLD moments video fMRI dataset and metadata</article-title><source>Nat Commun</source><year>2024</year><volume>15</volume><fpage>6241</fpage><pub-id pub-id-type="doi">10.1038/s41467-024-50310-3</pub-id><pub-id pub-id-type="pmid">39048577</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Lahner, B. <italic>et al</italic>. Modeling short visual events through the BOLD moments video fMRI dataset and metadata. <italic>Nat Commun</italic><bold>15</bold>, 6241 (2024).<pub-id pub-id-type="pmid">39048577</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><etal/></person-group><article-title>THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior</article-title><source>eLife</source><year>2023</year><volume>12</volume><fpage>e82580</fpage><pub-id pub-id-type="doi">10.7554/eLife.82580</pub-id><pub-id pub-id-type="pmid">36847339</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Hebart, M. N. <italic>et al</italic>. THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior. <italic>eLife</italic><bold>12</bold>, e82580 (2023).<pub-id pub-id-type="pmid">36847339</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Visconti di Oleggio Castello</surname><given-names>M</given-names></name><name><surname>Chauhan</surname><given-names>V</given-names></name><name><surname>Jiahui</surname><given-names>G</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name></person-group><article-title>An fMRI dataset in response to &#x0201c;The Grand Budapest Hotel&#x0201d;, a socially-rich, naturalistic movie</article-title><source>Sci Data</source><year>2020</year><volume>7</volume><fpage>383</fpage><pub-id pub-id-type="doi">10.1038/s41597-020-00735-4</pub-id><pub-id pub-id-type="pmid">33177526</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Visconti di Oleggio Castello, M., Chauhan, V., Jiahui, G. &#x00026; Gobbini, M. I. An fMRI dataset in response to &#x0201c;The Grand Budapest Hotel&#x0201d;, a socially-rich, naturalistic movie. <italic>Sci Data</italic><bold>7</bold>, 383 (2020).<pub-id pub-id-type="pmid">33177526</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Dai</surname><given-names>Y</given-names></name><name><surname>Xie</surname><given-names>H</given-names></name><name><surname>Zhen</surname><given-names>Z</given-names></name></person-group><article-title>A studyforrest extension, MEG recordings while watching the audio-visual movie &#x0201c;Forrest Gump&#x0201d;</article-title><source>Sci Data</source><year>2022</year><volume>9</volume><fpage>206</fpage><pub-id pub-id-type="doi">10.1038/s41597-022-01299-1</pub-id><pub-id pub-id-type="pmid">35562378</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">Liu, X., Dai, Y., Xie, H. &#x00026; Zhen, Z. A studyforrest extension, MEG recordings while watching the audio-visual movie &#x0201c;Forrest Gump&#x0201d;. <italic>Sci Data</italic><bold>9</bold>, 206 (2022).<pub-id pub-id-type="pmid">35562378</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Hari</surname><given-names>R</given-names></name><name><surname>Lev&#x000e4;nen</surname><given-names>S</given-names></name><name><surname>Raij</surname><given-names>T</given-names></name></person-group><article-title>Timing of human cortical functions during cognition: role of MEG</article-title><source>Trends in Cognitive Sciences</source><year>2000</year><volume>4</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01549-7</pub-id><pub-id pub-id-type="pmid">11115759</pub-id>
</element-citation><mixed-citation id="mc-CR18" publication-type="journal">Hari, R., Lev&#x000e4;nen, S. &#x00026; Raij, T. Timing of human cortical functions during cognition: role of MEG. <italic>Trends in Cognitive Sciences</italic><bold>4</bold>, 455&#x02013;462 (2000).<pub-id pub-id-type="pmid">11115759</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes da Silva</surname><given-names>F</given-names></name></person-group><article-title>EEG and MEG: Relevance to Neuroscience</article-title><source>Neuron</source><year>2013</year><volume>80</volume><fpage>1112</fpage><lpage>1128</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.017</pub-id><pub-id pub-id-type="pmid">24314724</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Lopes da Silva, F. EEG and MEG: Relevance to Neuroscience. <italic>Neuron</italic><bold>80</bold>, 1112&#x02013;1128 (2013).<pub-id pub-id-type="pmid">24314724</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Malmivuo</surname><given-names>J</given-names></name></person-group><article-title>Comparison of the Properties of EEG and MEG in Detecting the Electric Activity of the Brain</article-title><source>Brain Topogr</source><year>2012</year><volume>25</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1007/s10548-011-0202-1</pub-id><pub-id pub-id-type="pmid">21912974</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">Malmivuo, J. Comparison of the Properties of EEG and MEG in Detecting the Electric Activity of the Brain. <italic>Brain Topogr</italic><bold>25</bold>, 1&#x02013;19 (2012).<pub-id pub-id-type="pmid">21912974</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Ahlfors</surname><given-names>SP</given-names></name><name><surname>Han</surname><given-names>J</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name><name><surname>H&#x000e4;m&#x000e4;l&#x000e4;inen</surname><given-names>MS</given-names></name></person-group><article-title>Sensitivity of MEG and EEG to Source Orientation</article-title><source>Brain Topogr</source><year>2010</year><volume>23</volume><fpage>227</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1007/s10548-010-0154-x</pub-id><pub-id pub-id-type="pmid">20640882</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Ahlfors, S. P., Han, J., Belliveau, J. W. &#x00026; H&#x000e4;m&#x000e4;l&#x000e4;inen, M. S. Sensitivity of MEG and EEG to Source Orientation. <italic>Brain Topogr</italic><bold>23</bold>, 227&#x02013;232 (2010).<pub-id pub-id-type="pmid">20640882</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><collab>Multivariate pattern analysis of MEG and EEG</collab></person-group><article-title>A comparison of representational structure in time and space</article-title><source>NeuroImage</source><year>2017</year><volume>158</volume><fpage>441</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.07.023</pub-id><pub-id pub-id-type="pmid">28716718</pub-id>
</element-citation><mixed-citation id="mc-CR22" publication-type="journal">Multivariate pattern analysis of MEG and EEG. A comparison of representational structure in time and space. <italic>NeuroImage</italic><bold>158</bold>, 441&#x02013;454 (2017).<pub-id pub-id-type="pmid">28716718</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>H&#x000e4;usler</surname><given-names>CO</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name></person-group><article-title>A studyforrest extension, an annotation of spoken language in the German dubbed movie &#x0201c;Forrest Gump&#x0201d; and its audio-description</article-title><source>F1000Res</source><year>2021</year><volume>10</volume><fpage>54</fpage><pub-id pub-id-type="doi">10.12688/f1000research.27621.1</pub-id><pub-id pub-id-type="pmid">33732435</pub-id>
</element-citation><mixed-citation id="mc-CR23" publication-type="journal">H&#x000e4;usler, C. O. &#x00026; Hanke, M. A studyforrest extension, an annotation of spoken language in the German dubbed movie &#x0201c;Forrest Gump&#x0201d; and its audio-description. <italic>F1000Res</italic><bold>10</bold>, 54 (2021).<pub-id pub-id-type="pmid">33732435</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><etal/></person-group><article-title>ImageNet Large Scale Visual Recognition Challenge</article-title><source>Int J Comput Vis</source><year>2015</year><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Russakovsky, O. <italic>et al</italic>. ImageNet Large Scale Visual Recognition Challenge. <italic>Int J Comput Vis</italic><bold>115</bold>, 211&#x02013;252 (2015).</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Niso</surname><given-names>G</given-names></name><etal/></person-group><article-title>MEG-BIDS, the brain imaging data structure extended to magnetoencephalography</article-title><source>Sci Data</source><year>2018</year><volume>5</volume><fpage>180110</fpage><pub-id pub-id-type="doi">10.1038/sdata.2018.110</pub-id><pub-id pub-id-type="pmid">29917016</pub-id>
</element-citation><mixed-citation id="mc-CR25" publication-type="journal">Niso, G. <italic>et al</italic>. MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. <italic>Sci Data</italic><bold>5</bold>, 180110 (2018).<pub-id pub-id-type="pmid">29917016</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Appelhoff</surname><given-names>S</given-names></name><etal/></person-group><article-title>MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis</article-title><source>Journal of Open Source Software</source><year>2019</year><volume>4</volume><fpage>1896</fpage><pub-id pub-id-type="doi">10.21105/joss.01896</pub-id><pub-id pub-id-type="pmid">35990374</pub-id>
</element-citation><mixed-citation id="mc-CR26" publication-type="journal">Appelhoff, S. <italic>et al</italic>. MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. <italic>Journal of Open Source Software</italic><bold>4</bold>, 1896 (2019).<pub-id pub-id-type="pmid">35990374</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Gramfort, A. MEG and EEG data analysis with MNE-Python. <italic>Frontiers in Neuroscience</italic>.</mixed-citation></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="data"><name><surname>Larson</surname><given-names>E</given-names></name><etal/><year>2024</year><data-title>MNE-Python</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.13340330</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="data">Larson, E. <italic>et al</italic>. MNE-Python. <italic>Zenodo</italic>10.5281/zenodo.13340330 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Treacher</surname><given-names>AH</given-names></name><etal/></person-group><article-title>MEGnet: Automatic ICA-based artifact removal for MEG using spatiotemporal convolutional neural networks</article-title><source>NeuroImage</source><year>2021</year><volume>241</volume><fpage>118402</fpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118402</pub-id><pub-id pub-id-type="pmid">34274419</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Treacher, A. H. <italic>et al</italic>. MEGnet: Automatic ICA-based artifact removal for MEG using spatiotemporal convolutional neural networks. <italic>NeuroImage</italic><bold>241</bold>, 118402 (2021).<pub-id pub-id-type="pmid">34274419</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>A</given-names></name><name><surname>Feitelberg</surname><given-names>J</given-names></name><name><surname>Saini</surname><given-names>AP</given-names></name><name><surname>H&#x000f6;chenberger</surname><given-names>R</given-names></name><name><surname>Scheltienne</surname><given-names>M</given-names></name></person-group><article-title>MNE-ICALabel: Automatically annotating ICA components with ICLabel in Python</article-title><source>Journal of Open Source Software</source><year>2022</year><volume>7</volume><fpage>4484</fpage><pub-id pub-id-type="doi">10.21105/joss.04484</pub-id></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Li, A., Feitelberg, J., Saini, A. P., H&#x000f6;chenberger, R. &#x00026; Scheltienne, M. MNE-ICALabel: Automatically annotating ICA components with ICLabel in Python. <italic>Journal of Open Source Software</italic><bold>7</bold>, 4484 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="data"><name><surname>Barascud</surname><given-names>N</given-names></name><year>2023</year><data-title>nbara/python-meegkit: v0.1.5</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.10210992</pub-id></element-citation><mixed-citation id="mc-CR31" publication-type="data">Barascud, N. nbara/python-meegkit: v0.1.5. <italic>Zenodo</italic>10.5281/zenodo.10210992 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="data"><name><surname>Appelhoff</surname><given-names>S</given-names></name><etal/><year>2023</year><data-title>PyPREP: A Python implementation of the preprocessing pipeline (PREP) for EEG data</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.10047462</pub-id></element-citation><mixed-citation id="mc-CR32" publication-type="data">Appelhoff, S. <italic>et al</italic>. PyPREP: A Python implementation of the preprocessing pipeline (PREP) for EEG data. <italic>Zenodo</italic>10.5281/zenodo.10047462 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Perrin</surname><given-names>F</given-names></name><name><surname>Pernier</surname><given-names>J</given-names></name><name><surname>Bertrand</surname><given-names>O</given-names></name><name><surname>Echallier</surname><given-names>JF</given-names></name></person-group><article-title>Spherical splines for scalp potential and current density mapping</article-title><source>Electroencephalography and Clinical Neurophysiology</source><year>1989</year><volume>72</volume><fpage>184</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(89)90180-6</pub-id><pub-id pub-id-type="pmid">2464490</pub-id>
</element-citation><mixed-citation id="mc-CR33" publication-type="journal">Perrin, F., Pernier, J., Bertrand, O. &#x00026; Echallier, J. F. Spherical splines for scalp potential and current density mapping. <italic>Electroencephalography and Clinical Neurophysiology</italic><bold>72</bold>, 184&#x02013;187 (1989).<pub-id pub-id-type="pmid">2464490</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>de Cheveign&#x000e9;</surname><given-names>A</given-names></name></person-group><article-title>ZapLine: A simple and effective method to remove power line artifacts</article-title><source>NeuroImage</source><year>2020</year><volume>207</volume><fpage>116356</fpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116356</pub-id><pub-id pub-id-type="pmid">31786167</pub-id>
</element-citation><mixed-citation id="mc-CR34" publication-type="journal">de Cheveign&#x000e9;, A. ZapLine: A simple and effective method to remove power line artifacts. <italic>NeuroImage</italic><bold>207</bold>, 116356 (2020).<pub-id pub-id-type="pmid">31786167</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Markiewicz, C. J. <italic>et al</italic>. The OpenNeuro resource for sharing of neuroscience data. <italic>eLife</italic><bold>10</bold>, (2021).</mixed-citation></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="data"><name><surname>Zhang</surname><given-names>G</given-names></name><etal/><year>2025</year><data-title>NOD-MEG</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds005810.v1.0.5</pub-id></element-citation><mixed-citation id="mc-CR36" publication-type="data">Zhang, G. <italic>et al</italic>. NOD-MEG. <italic>OpenNeuro</italic>10.18112/openneuro.ds005810.v1.0.5 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="data"><name><surname>Zhang</surname><given-names>G</given-names></name><etal/><year>2025</year><data-title>NOD-EEG</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds005811.v1.0.8</pub-id></element-citation><mixed-citation id="mc-CR37" publication-type="data">Zhang, G. <italic>et al</italic>. NOD-EEG. <italic>OpenNeuro</italic>10.18112/openneuro.ds005811.v1.0.8 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Barragan-Jason</surname><given-names>G</given-names></name><name><surname>Cauchoix</surname><given-names>M</given-names></name><name><surname>Barbeau</surname><given-names>EJ</given-names></name></person-group><article-title>The neural speed of familiar face recognition</article-title><source>Neuropsychologia</source><year>2015</year><volume>75</volume><fpage>390</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.06.017</pub-id><pub-id pub-id-type="pmid">26100560</pub-id>
</element-citation><mixed-citation id="mc-CR38" publication-type="journal">Barragan-Jason, G., Cauchoix, M. &#x00026; Barbeau, E. J. The neural speed of familiar face recognition. <italic>Neuropsychologia</italic><bold>75</bold>, 390&#x02013;401 (2015).<pub-id pub-id-type="pmid">26100560</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name><surname>Cauchoix</surname><given-names>M</given-names></name><name><surname>Barragan-Jason</surname><given-names>G</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name><name><surname>Barbeau</surname><given-names>EJ</given-names></name></person-group><article-title>The Neural Dynamics of Face Detection in the Wild Revealed by MVPA</article-title><source>J. Neurosci.</source><year>2014</year><volume>34</volume><fpage>846</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3030-13.2014</pub-id><pub-id pub-id-type="pmid">24431443</pub-id>
</element-citation><mixed-citation id="mc-CR39" publication-type="journal">Cauchoix, M., Barragan-Jason, G., Serre, T. &#x00026; Barbeau, E. J. The Neural Dynamics of Face Detection in the Wild Revealed by MVPA. <italic>J. Neurosci.</italic><bold>34</bold>, 846&#x02013;854 (2014).<pub-id pub-id-type="pmid">24431443</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Linkenkaer-Hansen</surname><given-names>K</given-names></name><etal/></person-group><article-title>Face-selective processing in human extrastriate cortex around 120&#x02009;ms after stimulus onset revealed by magneto- and electroencephalography</article-title><source>Neuroscience Letters</source><year>1998</year><volume>253</volume><fpage>147</fpage><lpage>150</lpage><pub-id pub-id-type="doi">10.1016/S0304-3940(98)00586-2</pub-id><pub-id pub-id-type="pmid">9792232</pub-id>
</element-citation><mixed-citation id="mc-CR40" publication-type="journal">Linkenkaer-Hansen, K. <italic>et al</italic>. Face-selective processing in human extrastriate cortex around 120&#x02009;ms after stimulus onset revealed by magneto- and electroencephalography. <italic>Neuroscience Letters</italic><bold>253</bold>, 147&#x02013;150 (1998).<pub-id pub-id-type="pmid">9792232</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Liu, J., Harris, A. &#x00026; Kanwisher, N. Stages of processing in face perception: an MEG study. <italic>nature neuroscience</italic><bold>5</bold>, (2002).</mixed-citation></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>Caliskan</surname><given-names>A</given-names></name><name><surname>Yuksel</surname><given-names>ME</given-names></name><name><surname>Badem</surname><given-names>H</given-names></name><name><surname>Basturk</surname><given-names>A</given-names></name></person-group><article-title>A Deep Neural Network Classifier for Decoding Human Brain Activity Based on Magnetoencephalography</article-title><source>Elektronika ir Elektrotechnika</source><year>2017</year><volume>23</volume><fpage>63</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.5755/j01.eie.23.2.18002</pub-id></element-citation><mixed-citation id="mc-CR42" publication-type="journal">Caliskan, A., Yuksel, M. E., Badem, H. &#x00026; Basturk, A. A Deep Neural Network Classifier for Decoding Human Brain Activity Based on Magnetoencephalography. <italic>Elektronika ir Elektrotechnika</italic><bold>23</bold>, 63&#x02013;67 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name><surname>Hamam&#x000e9;</surname><given-names>CM</given-names></name><etal/></person-group><article-title>Functional selectivity in the human occipitotemporal cortex during natural vision: Evidence from combined intracranial EEG and eye-tracking</article-title><source>NeuroImage</source><year>2014</year><volume>95</volume><fpage>276</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.03.025</pub-id><pub-id pub-id-type="pmid">24650595</pub-id>
</element-citation><mixed-citation id="mc-CR43" publication-type="journal">Hamam&#x000e9;, C. M. <italic>et al</italic>. Functional selectivity in the human occipitotemporal cortex during natural vision: Evidence from combined intracranial EEG and eye-tracking. <italic>NeuroImage</italic><bold>95</bold>, 276&#x02013;286 (2014).<pub-id pub-id-type="pmid">24650595</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name><surname>Van De Nieuwenhuijzen</surname><given-names>ME</given-names></name><etal/></person-group><article-title>MEG-based decoding of the spatiotemporal dynamics of visual category perception</article-title><source>NeuroImage</source><year>2013</year><volume>83</volume><fpage>1063</fpage><lpage>1073</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.07.075</pub-id><pub-id pub-id-type="pmid">23927900</pub-id>
</element-citation><mixed-citation id="mc-CR44" publication-type="journal">Van De Nieuwenhuijzen, M. E. <italic>et al</italic>. MEG-based decoding of the spatiotemporal dynamics of visual category perception. <italic>NeuroImage</italic><bold>83</bold>, 1063&#x02013;1073 (2013).<pub-id pub-id-type="pmid">23927900</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Contini</surname><given-names>EW</given-names></name><name><surname>Wardle</surname><given-names>SG</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><article-title>Decoding the time-course of object recognition in the human brain: From visual features to categorical decisions</article-title><source>Neuropsychologia</source><year>2017</year><volume>105</volume><fpage>165</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.02.013</pub-id><pub-id pub-id-type="pmid">28215698</pub-id>
</element-citation><mixed-citation id="mc-CR45" publication-type="journal">Contini, E. W., Wardle, S. G. &#x00026; Carlson, T. A. Decoding the time-course of object recognition in the human brain: From visual features to categorical decisions. <italic>Neuropsychologia</italic><bold>105</bold>, 165&#x02013;176 (2017).<pub-id pub-id-type="pmid">28215698</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name></person-group><article-title>Multivariate pattern analysis of MEG and EEG: A comparison of representational structure in time and space</article-title><source>NeuroImage</source><year>2017</year><volume>158</volume><fpage>441</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.07.023</pub-id><pub-id pub-id-type="pmid">28716718</pub-id>
</element-citation><mixed-citation id="mc-CR46" publication-type="journal">Cichy, R. M. &#x00026; Pantazis, D. Multivariate pattern analysis of MEG and EEG: A comparison of representational structure in time and space. <italic>NeuroImage</italic><bold>158</bold>, 441&#x02013;454 (2017).<pub-id pub-id-type="pmid">28716718</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>T</given-names></name><name><surname>Tovar</surname><given-names>DA</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Representational dynamics of object vision: The first 1000&#x02009;ms</article-title><source>Journal of Vision</source><year>2013</year><volume>13</volume><fpage>1</fpage><lpage>1</lpage><pub-id pub-id-type="doi">10.1167/13.10.1</pub-id><pub-id pub-id-type="pmid">23908380</pub-id>
</element-citation><mixed-citation id="mc-CR47" publication-type="journal">Carlson, T., Tovar, D. A., Alink, A. &#x00026; Kriegeskorte, N. Representational dynamics of object vision: The first 1000&#x02009;ms. <italic>Journal of Vision</italic><bold>13</bold>, 1&#x02013;1 (2013).<pub-id pub-id-type="pmid">23908380</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Ren, X. <italic>et al</italic>. Facial geometric detail recovery via implicit representation. in <italic>2023 IEEE 17th international conference on automatic face and gesture recognition (FG)</italic> 1&#x02013;8 10.1109/FG57933.2023.10042505 (2023).</mixed-citation></ref><ref id="CR49"><label>49.</label><citation-alternatives><element-citation id="ec-CR49" publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Similarity-Based Fusion of MEG and fMRI Reveals Spatio-Temporal Dynamics in Human Cortex During Visual Object Recognition</article-title><source>Cereb. Cortex</source><year>2016</year><volume>26</volume><fpage>3563</fpage><lpage>3579</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw135</pub-id><pub-id pub-id-type="pmid">27235099</pub-id>
</element-citation><mixed-citation id="mc-CR49" publication-type="journal">Cichy, R. M., Pantazis, D. &#x00026; Oliva, A. Similarity-Based Fusion of MEG and fMRI Reveals Spatio-Temporal Dynamics in Human Cortex During Visual Object Recognition. <italic>Cereb. Cortex</italic><bold>26</bold>, 3563&#x02013;3579 (2016).<pub-id pub-id-type="pmid">27235099</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>A M/EEG-fMRI Fusion Primer: Resolving Human Brain Responses in Space and Time</article-title><source>Neuron</source><year>2020</year><volume>107</volume><fpage>772</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.001</pub-id><pub-id pub-id-type="pmid">32721379</pub-id>
</element-citation><mixed-citation id="mc-CR50" publication-type="journal">Cichy, R. M. &#x00026; Oliva, A. A M/EEG-fMRI Fusion Primer: Resolving Human Brain Responses in Space and Time. <italic>Neuron</italic><bold>107</bold>, 772&#x02013;781 (2020).<pub-id pub-id-type="pmid">32721379</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname><given-names>I</given-names></name></person-group><article-title>What constrains the organization of the ventral temporal cortex?</article-title><source>Trends in Cognitive Sciences</source><year>2000</year><volume>4</volume><fpage>1</fpage><lpage>2</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(99)01416-3</pub-id><pub-id pub-id-type="pmid">10637614</pub-id>
</element-citation><mixed-citation id="mc-CR51" publication-type="journal">Gauthier, I. What constrains the organization of the ventral temporal cortex? <italic>Trends in Cognitive Sciences</italic><bold>4</bold>, 1&#x02013;2 (2000).<pub-id pub-id-type="pmid">10637614</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><citation-alternatives><element-citation id="ec-CR52" publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name></person-group><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nat Rev Neurosci</source><year>2014</year><volume>15</volume><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id><pub-id pub-id-type="pmid">24962370</pub-id>
</element-citation><mixed-citation id="mc-CR52" publication-type="journal">Grill-Spector, K. &#x00026; Weiner, K. S. The functional architecture of the ventral temporal cortex and its role in categorization. <italic>Nat Rev Neurosci</italic><bold>15</bold>, 536&#x02013;548 (2014).<pub-id pub-id-type="pmid">24962370</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR53"><label>53.</label><citation-alternatives><element-citation id="ec-CR53" publication-type="journal"><person-group person-group-type="author"><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Sayres</surname><given-names>R</given-names></name><name><surname>Vinberg</surname><given-names>J</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><article-title>fMRI-Adaptation and Category Selectivity in Human Ventral Temporal Cortex: Regional Differences Across Time Scales</article-title><source>Journal of Neurophysiology</source><year>2010</year><volume>103</volume><fpage>3349</fpage><lpage>3365</lpage><pub-id pub-id-type="doi">10.1152/jn.01108.2009</pub-id><pub-id pub-id-type="pmid">20375251</pub-id>
</element-citation><mixed-citation id="mc-CR53" publication-type="journal">Weiner, K. S., Sayres, R., Vinberg, J. &#x00026; Grill-Spector, K. fMRI-Adaptation and Category Selectivity in Human Ventral Temporal Cortex: Regional Differences Across Time Scales. <italic>Journal of Neurophysiology</italic><bold>103</bold>, 3349&#x02013;3365 (2010).<pub-id pub-id-type="pmid">20375251</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR54"><label>54.</label><citation-alternatives><element-citation id="ec-CR54" publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name><etal/></person-group><article-title>A multi-modal parcellation of human cerebral cortex</article-title><source>Nature</source><year>2016</year><volume>536</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/nature18933</pub-id><pub-id pub-id-type="pmid">27437579</pub-id>
</element-citation><mixed-citation id="mc-CR54" publication-type="journal">Glasser, M. F. <italic>et al</italic>. A multi-modal parcellation of human cerebral cortex. <italic>Nature</italic><bold>536</bold>, 171&#x02013;178 (2016).<pub-id pub-id-type="pmid">27437579</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Harel, A., Kravitz, D. J. &#x00026; Baker, C. I. Task context impacts visual object processing differentially across the cortex. <italic>Proc. Natl. Acad. Sci</italic>. USA. <bold>111</bold>, (2014).</mixed-citation></ref><ref id="CR56"><label>56.</label><citation-alternatives><element-citation id="ec-CR56" publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Daniels</surname><given-names>N</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><article-title>Task context overrules object- and category-related representational content in the human parietal cortex</article-title><source>Cereb. Cortex</source><year>2017</year><volume>27</volume><fpage>310</fpage><lpage>321</lpage><pub-id pub-id-type="pmid">28108492</pub-id>
</element-citation><mixed-citation id="mc-CR56" publication-type="journal">Bracci, S., Daniels, N. &#x00026; Op de Beeck, H. Task context overrules object- and category-related representational content in the human parietal cortex. <italic>Cereb. Cortex</italic><bold>27</bold>, 310&#x02013;321 (2017).<pub-id pub-id-type="pmid">28108492</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR57"><label>57.</label><citation-alternatives><element-citation id="ec-CR57" publication-type="journal"><person-group person-group-type="author"><name><surname>Soldan</surname><given-names>A</given-names></name><name><surname>Zarahn</surname><given-names>E</given-names></name><name><surname>Hilton</surname><given-names>HJ</given-names></name><name><surname>Stern</surname><given-names>Y</given-names></name></person-group><article-title>Global familiarity of visual stimuli affects repetition-related neural plasticity but not repetition priming</article-title><source>NeuroImage</source><year>2008</year><volume>39</volume><fpage>515</fpage><lpage>526</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.08.011</pub-id><pub-id pub-id-type="pmid">17913513</pub-id>
</element-citation><mixed-citation id="mc-CR57" publication-type="journal">Soldan, A., Zarahn, E., Hilton, H. J. &#x00026; Stern, Y. Global familiarity of visual stimuli affects repetition-related neural plasticity but not repetition priming. <italic>NeuroImage</italic><bold>39</bold>, 515&#x02013;526 (2008).<pub-id pub-id-type="pmid">17913513</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR58"><label>58.</label><citation-alternatives><element-citation id="ec-CR58" publication-type="journal"><person-group person-group-type="author"><name><surname>Henson</surname><given-names>RNA</given-names></name></person-group><article-title>Neuroimaging studies of priming</article-title><source>Progress in Neurobiology</source><year>2003</year><volume>70</volume><fpage>53</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/S0301-0082(03)00086-8</pub-id><pub-id pub-id-type="pmid">12927334</pub-id>
</element-citation><mixed-citation id="mc-CR58" publication-type="journal">Henson, R. N. A. Neuroimaging studies of priming. <italic>Progress in Neurobiology</italic><bold>70</bold>, 53&#x02013;81 (2003).Please center the image and enlarge the image so that its width matches the width of each line of text.<pub-id pub-id-type="pmid">12927334</pub-id>
</mixed-citation></citation-alternatives></ref></ref-list></back></article>