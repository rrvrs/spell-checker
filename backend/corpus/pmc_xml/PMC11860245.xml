<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Nat Comput Sci</journal-id><journal-id journal-id-type="iso-abbrev">Nat Comput Sci</journal-id><journal-title-group><journal-title>Nature Computational Science</journal-title></journal-title-group><issn pub-type="epub">2662-8457</issn><publisher><publisher-name>Nature Publishing Group US</publisher-name><publisher-loc>New York</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39706876</article-id><article-id pub-id-type="pmc">PMC11860245</article-id>
<article-id pub-id-type="publisher-id">746</article-id><article-id pub-id-type="doi">10.1038/s43588-024-00746-w</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A spatiotemporal style transfer algorithm for dynamic visual stimulus generation</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9502-4667</contrib-id><name><surname>Greco</surname><given-names>Antonino</given-names></name><address><email>antonino.greco@uni-tuebingen.de</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5115-936X</contrib-id><name><surname>Siegel</surname><given-names>Markus</given-names></name><address><email>markus.siegel@uni-tuebingen.de</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03a1kwz48</institution-id><institution-id institution-id-type="GRID">grid.10392.39</institution-id><institution-id institution-id-type="ISNI">0000 0001 2190 1447</institution-id><institution>Department of Neural Dynamics and Magnetoencephalography, Hertie Institute for Clinical Brain Research, </institution><institution>University of T&#x000fc;bingen, </institution></institution-wrap>T&#x000fc;bingen, Germany </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03a1kwz48</institution-id><institution-id institution-id-type="GRID">grid.10392.39</institution-id><institution-id institution-id-type="ISNI">0000 0001 2190 1447</institution-id><institution>Centre for Integrative Neuroscience, </institution><institution>University of T&#x000fc;bingen, </institution></institution-wrap>T&#x000fc;bingen, Germany </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03a1kwz48</institution-id><institution-id institution-id-type="GRID">grid.10392.39</institution-id><institution-id institution-id-type="ISNI">0000 0001 2190 1447</institution-id><institution>MEG Center, </institution><institution>University of T&#x000fc;bingen, </institution></institution-wrap>T&#x000fc;bingen, Germany </aff><aff id="Aff4"><label>4</label>German Center for Mental Health (DZPG), T&#x000fc;bingen, Germany </aff></contrib-group><pub-date pub-type="epub"><day>20</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="ppub"><year>2025</year></pub-date><volume>5</volume><issue>2</issue><fpage>155</fpage><lpage>169</lpage><history><date date-type="received"><day>24</day><month>4</month><year>2024</year></date><date date-type="accepted"><day>21</day><month>11</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Understanding how visual information is encoded in biological and artificial systems often requires the generation of appropriate stimuli to test specific hypotheses, but available methods for video generation are scarce. Here we introduce the spatiotemporal style transfer (STST) algorithm, a dynamic visual stimulus generation framework that allows the manipulation and synthesis of video stimuli for vision research. We show how stimuli can be generated that match the low-level spatiotemporal features of their natural counterparts, but lack their high-level semantic features, providing a useful tool to study object recognition. We used these stimuli to probe PredNet, a predictive coding deep network, and found that its next-frame predictions were not disrupted by the omission of high-level information, with human observers also confirming the preservation of low-level features and lack of high-level information in the generated stimuli. We also introduce a procedure for the independent spatiotemporal factorization of dynamic stimuli. Testing such factorized stimuli on humans and deep vision models suggests a spatial bias in how humans and deep vision models encode dynamic visual information. These results showcase potential applications of the STST algorithm as a versatile tool for dynamic stimulus generation in vision science.</p></abstract><abstract id="Abs2" abstract-type="web-summary"><p id="Par2">The spatiotemporal style transfer (STST) algorithm enables video generation by selectively manipulating the spatial and temporal features of natural videos, fostering vision science research in both biological and artificial systems.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Sensory processing</kwd><kwd>Computer science</kwd><kwd>Computational neuroscience</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/100010663</institution-id><institution>EC | EU Framework Programme for Research and Innovation H2020 | H2020 Priority Excellent Science | H2020 European Research Council (H2020 Excellent Science - European Research Council)</institution></institution-wrap></funding-source><award-id>CoG 864491</award-id><principal-award-recipient><name><surname>Siegel</surname><given-names>Markus</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft (German Research Foundation)</institution></institution-wrap></funding-source><award-id>276693517 (SFB 1233)</award-id><award-id>SI 1332/6-1 (SPP 2041)</award-id><principal-award-recipient><name><surname>Siegel</surname><given-names>Markus</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature America, Inc. 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Main</title><p id="Par3">Understanding how visual information is encoded in the brain has been a longstanding goal of neuroscience and vision science. Recently, there has been increasing interest to also study hidden representations of computer vision models and to compare biological and artificial vision<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. A fundamental component of this research is the generation of controlled visual stimuli. Traditionally, stimulus manipulation in vision research has been performed for low-level features of static visual stimuli such as the matching of pixel contrast<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, or manipulating the speed of dynamic visual stimuli<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>. Recently, computer graphics approaches have led to a proliferation of methods that can generate parametrically designed images for both machine learning and neuroscience research. Examples include the parametric generation of facial expressions<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> or three-dimensional (3D) visual scenes<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>.</p><p id="Par4">Although these methods have progressed the field of stimulus generation to a remarkable degree, they often fall short in terms of flexibility and tend to diverge from the natural statistics of our visual environment<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, often resulting in outputs that can appear artificial. In contrast, deep neural network models (DNNs) trained for computer vision tasks<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup> have revolutionized the approach of image synthesis in a variety of ways, affording researchers novel paradigms with which to investigate visual processing within artificial and biological neural networks<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup>. One of the earliest examples of a DNN-based stimulus generation method is DeepDream<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR14">14</xref></sup>, an algorithm that accentuates patterns in images in a manner that can be conceived as &#x02018;algorithmic pareidolia&#x02019;, using a pretrained deep convolutional neural network (CNN)<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. This method has been predominantly used in vision research to simulate the visual hallucination patterns commonly found in altered states of consciousness induced by psychedelic drugs<sup><xref ref-type="bibr" rid="CR16">16</xref>&#x02013;<xref ref-type="bibr" rid="CR18">18</xref></sup>. An alternative to this method has also been proposed to generate visual stimuli that maximally activate specific visual cortical regions across species<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup>. Another influential family of DNN-based methods for image synthesis comes from the seminal work of Gatys and colleagues<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, in which they proposed the &#x02018;neural style transfer&#x02019; (NST) algorithm to extract and recombine the texture and shape of natural images to generate novel stimuli<sup><xref ref-type="bibr" rid="CR21">21</xref>&#x02013;<xref ref-type="bibr" rid="CR24">24</xref></sup>. NST has been applied in vision research to compare visual representations of images in humans and machines<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup> and to understand how spatial texture information is encoded in the visual cortex of the mammalian brain<sup><xref ref-type="bibr" rid="CR27">27</xref>&#x02013;<xref ref-type="bibr" rid="CR29">29</xref></sup>.</p><p id="Par5">Despite the abundance of methods for generating static visual stimuli, the algorithms proposed for dynamic visual stimuli generation, namely videos, are scarce. Unlike images, videos also have a temporal dimension and, in natural videos, the relationship between spatial and temporal features is often non-trivial<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup>. Here we introduce a method for generating dynamic visual stimuli that can serve different purposes in vision research. The method is based on the NST algorithm but extends its capabilities to videos. We thus refer to it as &#x02018;spatiotemporal style transfer&#x02019; (STST). We capitalized on recent texturization algorithms that extended the NST capabilities to generate dynamic textures, that is, video clips with stationary spatiotemporal patterns such as flames or sea waves<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>. The general idea is to synthesize &#x02018;model metamers&#x02019;<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, which are dynamic stimuli whose layer activations within a DNN model are matched to those of natural videos. To create these stimuli, we perform optimization in a two-stream model architecture<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup>. This model is designed to replicate the relative segregation of the neural pathways responsible for processing spatial and temporal features in the brain<sup><xref ref-type="bibr" rid="CR37">37</xref>&#x02013;<xref ref-type="bibr" rid="CR39">39</xref></sup>.</p><p id="Par6">After elucidating the procedural steps of the algorithm, we demonstrate its use in generating model metamers from natural videos. These metamers retain low-level spatiotemporal features but lack high-level object semantics, and thus offer a tool to study object recognition in biological and artificial visual systems<sup><xref ref-type="bibr" rid="CR40">40</xref>&#x02013;<xref ref-type="bibr" rid="CR42">42</xref></sup>. Using state-of-the-art deep vision models, we tested early and late layer activations for differences between natural and metamer stimuli. We then examined predictive coding networks&#x02019; internal representations during next-frame prediction and assessed human object recognition on metamer stimuli. Finally, we illustrate how our STST algorithm enables spatiotemporal factorization, blending spatial and temporal features from different videos for discrimination tasks in both humans and deep vision models.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>The STST algorithm</title><p id="Par7">The proposed algorithm is based on a two-stream neural network model. which we outline here. Details of the model description are presented in the <xref rid="Sec11" ref-type="sec">Methods</xref>. One module (or &#x02018;stream&#x02019;) processes spatial features in each frame, and the other module captures temporal features across consecutive frames (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). We adopted VGG-19<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> as the spatial module and the multiscale spacetime-oriented energy model (MSOE)<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup> as the temporal module. The optimization procedure generates &#x02018;model metamers&#x02019; that match the content and/or texture between generated and target videos by minimizing spatial and temporal losses. This yields four distinct loss components that, when weighted and combined, form a flexible overall loss function, enabling diverse combinations for dynamic stimulus generation.<fig id="Fig1"><label>Fig. 1</label><caption><title>Graphical representation of the STST algorithm.</title><p>The current frame as well as the current and previous frames are forward passed to the spatial and temporal stream, respectively, up to a predefined set of layers. Content (green) and texture (orange) loss are defined for both streams as the difference of layer activations (here, green-highlighted layers for the content loss and orange-highlighted layers for the texture loss) and their Gram matrices between the target and generated frames, respectively. These are weighted and summed to form the overall STST loss. The partial derivative of this loss with respect to the current generated frame is used to optimize it by gradient descent. Natural images adapted from Google LLC under a Creative Commons license <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</ext-link>.</p></caption><graphic xlink:href="43588_2024_746_Fig1_HTML" id="d33e388"/></fig></p><p id="Par8">Crucially, we employed several preconditioning techniques to enhance the perceptual stability and prevent artifacts: the addition of a total variation loss<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, the multiscale nature of the optimization processing<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>, a color transfer postprocessing<sup><xref ref-type="bibr" rid="CR47">47</xref>,<xref ref-type="bibr" rid="CR48">48</xref></sup> and a frame blending operation. These techniques enable the algorithm to generate dynamic stimuli that maintain a consistent appearance over time, particularly for complex natural videos.</p></sec><sec id="Sec4"><title>Model metamers for object recognition</title><p id="Par9">We now describe an example application of STST for investigating object recognition in visual systems. We generated dynamic metamer stimuli that had low-level spatiotemporal features similar to those of their natural counterparts, but that lacked high-level object information. We collected three high-quality video clips from the YouTube-8M dataset<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> and optimized dynamic stimuli to match both their spatial and temporal textures (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). For comparison, we generated dynamic stimuli starting from the same target videos but using another algorithm<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> that was previously proposed to generate dynamic metamers<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. This alternate algorithm basically consists of randomizing the phase in the spatiotemporal frequency domain<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. In the following, we will refer to this alternate algorithm as spatiotemporal phase scrambling (STPS). The natural videos, as well as their STST and STPS counterparts, are available in Supplementary Video <xref rid="MOESM5" ref-type="media">1</xref>.<fig id="Fig2"><label>Fig. 2</label><caption><title>Low-level features in metamers and their effects on layer representations of deep vision models.</title><p><bold>a</bold>, Example frames from the original target videos as well as from STST- and STPS-generated videos. <bold>b</bold>, Time courses of four spatiotemporal features in original (gray), STST (azure) and STPS (pink) stimuli. Features include pixel intensity and contrast as spatial low-level features and pixel change and optical flow (magnitude and angle) as temporal low-level features. The <italic>x</italic> axis for all plots is the frame number. On the <italic>y</italic> axis, a.u. stands for arbitrary units while rad stands for radiants. <bold>c</bold>, We used STST stimuli and their natural counterparts as inputs to deep vision models and extracted early (orange) and late (blue) layer activations. <bold>d</bold>, Time courses of the similarity metric (CKA) between natural and generated stimuli for image classification models in early (orange) and late (blue) layer activations. <bold>e</bold>, Time courses of the similarity metric (CKA) for video classification models in early (orange) and late (blue) layer activations. Natural images in <bold>a</bold>, <bold>c</bold> and <bold>d</bold> adapted from Google LLC under a Creative Commons license <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</ext-link>.</p><p><xref rid="MOESM8" ref-type="media">Source data</xref></p></caption><graphic xlink:href="43588_2024_746_Fig2_HTML" id="d33e481"/></fig></p><p id="Par10">By qualitatively inspecting the generated frames in Fig. <xref rid="Fig2" ref-type="fig">2a</xref>, it can be noticed how our STST algorithm and STPS produce very dissimilar stimuli, both in terms of appearance and motion dynamics. We computed four basic spatiotemporal features for the original, STST and STPS videos to quantitatively assess both methods, namely pixel intensity and contrast as spatial features, and pixel change and optical flow (magnitude and angle) as temporal features (Fig. <xref rid="Fig2" ref-type="fig">2b</xref>). We found that both methods excelled at matching the spatial features, but the temporal features were less preserved. Critically, STST performed substantially better than STPS at matching the optical flow of the natural videos, which is crucial for dynamic stimuli.</p><p id="Par11">We validated these results on a large sample of 100 videos from the Kinetics400 dataset<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, which, in the following, we refer to as the validation dataset. We computed the Pearson correlation coefficient (PCC; Supplementary Fig. <xref rid="MOESM1" ref-type="media">1a</xref>) and Euclidean distance (ED; Supplementary Fig. <xref rid="MOESM1" ref-type="media">1b</xref>) between the low-level features of the original and STST or STPS videos to quantitatively measure to what extent these features matched. All four spatial and temporal features were significantly better preserved by STST than by STPS (all temporal features: PCC, <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, Cohen&#x02019;s <italic>d</italic>&#x02009;&#x0003e;&#x02009;0.76; ED, <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001, <italic>d</italic>&#x02009;&#x0003e;&#x02009;0.34; all spatial features: PCC, <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.015, <italic>d</italic>&#x02009;&#x0003e;&#x02009;0.35; ED, <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, <italic>d</italic>&#x02009;&#x0003e;&#x02009;1.08). The strongest effect was found for the optical flow angle (PCC, <italic>d</italic>&#x02009;=&#x02009;4.25; ED, <italic>d</italic>&#x02009;=&#x02009;1.62). These results show how the STST algorithm can be used to dismantle high-level image information while preserving low-level spatiotemporal statistics.</p><p id="Par12">We performed an ablation study on several components of the STST algorithm to assess their contribution to this performance of the algorithm. Specifically, we repeated the generation of the same three example videos while deleting either the color-matching step or total variation loss. We found that both ablations substantially deteriorated the quality of the generated metamers (Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>). By inspecting the generated frames and the time course of the low-level statistics (Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>), it can be seen that the color-matching steps have a strong impact on contrast and luminosity, as well as on the full color distribution, whereas the absence of the total variation loss induces excessive high-frequency noise. Thus, color matching and total variation loss substantially contribute to the performance of the algorithm.</p></sec><sec id="Sec5"><title>Metamer effects on layer representations of vision models</title><p id="Par13">We next tested the generated stimuli using state-of-the-art deep learning models for image and video classification. We used these models because they exhibit a hierarchical structure for extracting visual features that is similar to the mammalian visual cortex<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR52">52</xref></sup>. We thus hypothesized that the similarity between natural and STST stimuli in layer activations of these models should be higher in early layers than in late layers (Fig. <xref rid="Fig2" ref-type="fig">2c</xref>). To compute the similarity between layer activations, we used the center kernel alignment (CKA) score<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. Notably, we did not use the same models we deployed for the spatial and temporal streams in the STST model.</p><p id="Par14">We found almost perfect matching of early-layer representations between natural and metamer stimuli across all example videos in the image classification models (Fig. <xref rid="Fig2" ref-type="fig">2d</xref>) (ResNet50 (ref. <sup><xref ref-type="bibr" rid="CR54">54</xref></sup>) and ConvNeXt-T<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>). Conversely, late layer activations differed substantially between natural and metamer stimuli. This pattern of decreased similarity was consistent across all videos. Although these results were in line with our predictions, the employed models were applied on a frame-by-frame basis as they were conceived for image classification. Thus, we extended our analyses to video classification models using ResNet18-3D<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> and ResNet18-MC3 (ref. <sup><xref ref-type="bibr" rid="CR57">57</xref></sup>). We used the same procedure, but this time using snippets of videos in a sliding-window approach. Again, we observed early-layer activations being very similar across all frames and example videos, whereas late layers showed consistently decreased similarity. Crucially, we found the same pattern as in image classification models, although the dynamic range of the effects was smaller (Fig. <xref rid="Fig2" ref-type="fig">2e</xref>).</p><p id="Par15">We validated these findings on the validation dataset (Supplementary Fig. <xref rid="MOESM1" ref-type="media">3a,b</xref>). We found that the CKA score was significantly higher in early compared to late layers (for image models, all <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, all <italic>d</italic>&#x02009;&#x0003e;&#x02009;3.14; for video models, all <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, all <italic>d</italic>&#x02009;&#x0003e;&#x02009;1.34). We computed the CKA score between hidden layer activations of natural and STPS stimuli, finding that for STPS also, early layers had a significantly higher score than late layer representations (for image models, all <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, all <italic>d</italic>&#x02009;&#x0003e;&#x02009;4.64; for video models, all <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, all <italic>d</italic>&#x02009;&#x0003e;&#x02009;2.43). We also compared the CKA score of layer activations between STST and STPS. For all models and layers, the CKA score was significantly higher in STST than in STPS (all <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, all <italic>d</italic>&#x02009;&#x0003e;&#x02009;0.94). Thus, the better preservation of low-level features in STST as compared to STPS may lead to a higher similarity between natural and STST as compared to STPS stimuli, even at later model stages. Together, these findings confirmed that STST allows us to generate dynamic stimuli with high similarity to natural videos in terms of low-level spatiotemporal statistics, but without preserving high-level features that are critical for object recognition in biological and artificial vision systems.</p></sec><sec id="Sec6"><title>Evaluating semantic understanding in PredNet with metamers</title><p id="Par16">We used the metamer stimuli generated with STST to investigate the representational capabilities of DNNs inspired by predictive coding theories<sup><xref ref-type="bibr" rid="CR58">58</xref>,<xref ref-type="bibr" rid="CR59">59</xref></sup>. We focused on PredNet<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>, a deep convolutional recurrent neural network (Fig. <xref rid="Fig3" ref-type="fig">3a</xref>) that was trained with self-supervised learning to perform next-frame prediction<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>. We opted for this model because it has interesting properties that are aligned with a range of phenomena observed in the visual cortex<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>, probably due to its architectural structure, which involves recurrent and feedback connections<sup><xref ref-type="bibr" rid="CR60">60</xref></sup> that are absent in traditional feedforward-only CNN models<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR63">63</xref></sup>. Recently, it has been debated whether PredNet is able to extract robust high-level representations of its dynamic inputs or it merely acts as a flow filter predicting low-level optical flow<sup><xref ref-type="bibr" rid="CR58">58</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup>.<fig id="Fig3"><label>Fig. 3</label><caption><title>Probing the effect of high-level information in predictive coding networks using STST metamers.</title><p><bold>a</bold>, Graphical outline of the analysis pipeline. Left: PredNet architecture with input and prediction (blue), representation (green) and error (red) modules. Gray arrows denote information flow, with solid lines indicating forward and feedback flow, and dashed lines indicating recurrent connections. Middle: we passed as inputs to PredNet STST metamer stimuli and their natural counterparts (Original). We separately computed next-frame predictions for both inputs. <bold>b</bold>, Example ground-truth and predicted frames. <bold>c</bold>, SSIM between PredNet predictions and ground-truth frames for original (gray) and STST (azure) stimuli. Bar plots depict video averages. <bold>d</bold>, Raincloud plots showing SSIM between predicted and ground-truth frames for all original, STST and STPS videos of the validation dataset. <bold>e</bold>, SSIM between consecutive video frames for all original, STST and STPS videos of the validation dataset. <bold>f</bold>, cSSIM between predicted and ground-truth frames for all original, STST and STPS videos of the validation dataset. Circles indicate mean values of the distribution. Dots indicate individual videos (<italic>n</italic>&#x02009;=&#x02009;100). Natural images in <bold>a</bold> and <bold>b</bold> adapted from Google LLC under a Creative Commons license <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</ext-link>.</p><p><xref rid="MOESM9" ref-type="media">Source data</xref></p></caption><graphic xlink:href="43588_2024_746_Fig3_HTML" id="d33e708"/></fig></p><p id="Par17">To investigate this, we compared the next-frame predictions of PredNet for both original and STST-generated metamer stimuli (Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). We reasoned that if PredNet were able to exploit high-level visual representations, next-frame predictions should be better for natural as compared to metamer stimuli. Figure <xref rid="Fig3" ref-type="fig">3b</xref> shows frame predictions by the PredNet model for the three example videos that were also used in the previous experiments. Qualitatively, predictions had a high fidelity compared to their ground truth for both natural and metamer stimuli. We computed the structural similarity index measure (SSIM)<sup><xref ref-type="bibr" rid="CR65">65</xref></sup> to quantify how well PredNet performed next-frame prediction (Fig. <xref rid="Fig3" ref-type="fig">3c</xref>).</p><p id="Par18">We found that for the three example videos, SSIM was slightly higher for STST as compared to natural stimuli across frames and on average. Again, to statistically assess these results, we repeated the analysis on the larger validation dataset and conducted the same analysis for stimuli generated with the STPS algorithm (Fig. <xref rid="Fig3" ref-type="fig">3d</xref>). We found that SSIM was indeed significantly higher for STST predictions than for natural videos (<italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, <italic>d</italic>&#x02009;=&#x02009;0.29). Furthermore, for STPS stimuli, SSIM was even higher than for the original (<italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, <italic>d</italic>&#x02009;=&#x02009;1.25) and STST (<italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, <italic>d</italic>&#x02009;=&#x02009;0.92) videos. Thus, in contrast to what would be expected if PredNet exploited high-level content, the PredNet predictions were better for STPS stimuli and even better for STST stimuli as compared to natural videos.</p><p id="Par19">We reasoned that this effect could reflect the extent to which PredNet predictions are merely a copy of the previous frame, which could be more successful if consecutive frames were more alike in the metamer videos than in the original videos. To investigate this, we computed the average SSIM between all consecutive frames in all videos (Fig. <xref rid="Fig3" ref-type="fig">3e</xref>). This value was indeed significantly higher for STPS (<italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, <italic>d</italic>&#x02009;=&#x02009;1.62) and STST (<italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, <italic>d</italic>&#x02009;=&#x02009;0.35) videos than for natural ones. Importantly, the SSIM between consecutive frames for STST was significantly lower than for STPS (<italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, <italic>d</italic>&#x02009;=&#x02009;1.39), and thus closer to the natural videos, showing that STST stimuli are better suited to address the research questions at hand. To test whether the increased SSIM for metamer predictions was influenced by this stimulus statistic, we computed a dynamic version of SSIM conditional on the previous frame (cSSIM)<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>. cSSIM scales SSIM by the extent to which next-frame predictions diverge from merely copying the previous frame (Fig. <xref rid="Fig3" ref-type="fig">3f</xref> and <xref rid="Sec11" ref-type="sec">Methods</xref>). Indeed, cSSIM was significantly decreased for STST videos as compared to the original videos (<italic>P</italic>&#x02009;=&#x02009;0.0002, <italic>d</italic>&#x02009;=&#x02009;0.22), and was even lower for STPS compared to both original (<italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, <italic>d</italic>&#x02009;=&#x02009;0.51) and STST (<italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, <italic>d</italic>&#x02009;=&#x02009;0.28) videos. Together, these results show that PredNet predictions were not impaired by a lack of high-level content and that the quality of PredNet predictions reflected the similarity of consecutive frames.</p></sec><sec id="Sec7"><title>Evaluation of low-level feature preservation in humans</title><p id="Par20">After testing artificial systems, we also probed humans on these metamer stimuli to investigate the preservation of low-level and high-level features and how STST and STPS stimuli are perceived. First, we used a video-captioning task (Fig. <xref rid="Fig4" ref-type="fig">4a</xref>) in which human participants were asked to provide a text description of the presented videos. We used the same three high-quality videos as used in the previous analyses, alongside their scrambled counterparts. We used deep language models to extract sentence embedding from these captions and performed classification analysis (Fig. <xref rid="Fig4" ref-type="fig">4b</xref>) to investigate the discriminability of these descriptions between conditions.<fig id="Fig4"><label>Fig. 4</label><caption><title>Probing humans on STST and STPS stimuli.</title><p><bold>a</bold>, Graphical outline of the video-captioning task. <bold>b</bold>, Classification analyses on the sentence embeddings between original, STST and STPS video captions. The density plot represents the null distribution, and vertical dotted lines indicate the observed accuracy pooled across all stimuli. <bold>c</bold>, Cosine distance scores between all pairs of original, STST and STPS videos, as well as between the original video captions and the captions of the STST and STPS stimuli. Error bars indicate s.e.m. Dots indicate human participants (<italic>n</italic>&#x02009;=&#x02009;14). <bold>d</bold>, UMAP representation of the sentence embeddings from all participants and stimuli. Some actual text captions are provided next to the corresponding samples. <bold>e</bold>, Graphical description of the 2AFC perceptual similarity task and all five conditions used to investigate how humans rated the STST and STPS videos. <bold>f</bold>, Preference for STST videos compared to STPS. Error bars indicate s.e.m. Dots indicate individual participants (<italic>n</italic>&#x02009;=&#x02009;13). <bold>g</bold>, Accuracy of similarity judgments for the four control conditions. Error bars indicate s.e.m. Dots indicate human participants (<italic>n</italic>&#x02009;=&#x02009;13). Natural images in <bold>a</bold>, <bold>d</bold> and <bold>e</bold> adapted from Google LLC under a Creative Commons license <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</ext-link>.</p><p><xref rid="MOESM10" ref-type="media">Source data</xref></p></caption><graphic xlink:href="43588_2024_746_Fig4_HTML" id="d33e866"/></fig></p><p id="Par21">We found that for each of the three videos, the original video captions were significantly discriminable from both STST (mean accuracy&#x02009;=&#x02009;0.964, all <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001) and STPS (mean accuracy&#x02009;=&#x02009;0.976, all <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001) descriptions. No discriminability was observed between STST and STPS (mean accuracy&#x02009;=&#x02009;0.532, all <italic>P</italic>&#x02009;=&#x02009;0.11). Crucially, we also found that the cosine distance between sentence embeddings for all pairs of original videos were significantly larger (Fig. <xref rid="Fig4" ref-type="fig">4c</xref>) than the cosine distances for STST (<italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, <italic>d</italic>&#x02009;=&#x02009;2.84) and STPS (<italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001, <italic>d</italic>&#x02009;=&#x02009;3.22) video pairs. In other words, the captions of the three original videos were consistent across subjects and different between videos, whereas captions were significantly less distinct for STST and STPS (compare also Fig. <xref rid="Fig4" ref-type="fig">4d</xref>). We also observed that the cosine distance between the original and STST embeddings was significantly smaller than between the original and STPS (Fig. <xref rid="Fig4" ref-type="fig">4c</xref>, <italic>P</italic>&#x02009;=&#x02009;0.0072, <italic>d</italic>&#x02009;=&#x02009;0.83).</p><p id="Par22">A projection of the sentence embedding structure via the uniform manifold approximation and projection (UMAP) dimensionality reduction<sup><xref ref-type="bibr" rid="CR66">66</xref></sup> method (Fig. <xref rid="Fig4" ref-type="fig">4d</xref>) unraveled how the metamer stimuli were perceived substantially differently from the original videos. Furthermore, the actual text captions of the metamers lacked all the semantic references to the object-level features that were present in the original video captions. Together, these results suggest that no high-level content was detected by humans in the STST videos and that the STST videos were described in a more similar way to the natural videos than to the STPS videos.</p><p id="Par23">To further compare the STST and STPS videos, we collected data on human participants in a two alternative forced choice (2AFC) perceptual similarity task (Fig. <xref rid="Fig4" ref-type="fig">4e</xref>). Participants were instructed to report which of two sequentially presented option videos (either STST or STPS) were more similar to a reference video (original), shown at the start of each trial. We used the same three high-quality videos as were used in the previous analyses and tasks, alongside their manipulated counterparts. Crucially, when confronted with the decision to judge the similarity of the STST and STPS videos to the natural videos, human observers significantly preferred the STST videos (Fig. <xref rid="Fig4" ref-type="fig">4f</xref>; condition 1: preference score for STST&#x02009;=&#x02009;0.974, <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001). We also conducted control analyses on whether humans recognized that the STST or STPS videos generated from the reference (termed &#x02018;A&#x02019; videos) were more similar to the reference than videos generated from another video (termed &#x02018;B&#x02019; videos) (conditions 2 to 5). We found that, in all control conditions, humans were able to appropriately select the correct video significantly more often than by chance (Fig. <xref rid="Fig4" ref-type="fig">4g</xref>, all accuracies&#x02009;&#x0003e;&#x02009;0.948, all <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001). Together, these findings show that STST-generated metamers are perceived by humans as perceptually more similar to the original videos than the STPS metamers, which probably reflects the better preservation of low-level spatiotemporal features.</p></sec><sec id="Sec8"><title>Model metamers for spatiotemporal factorization</title><p id="Par24">The two-stream architecture of the network implementing the STST algorithm allows us to independently synthesize spatial and temporal features, a process that can be conceived as spatiotemporal factorization. This provides a unique facility to generate stimuli that are specifically tailored to investigate how spatial and temporal features are processes in visual systems. Here we showcase a version of this application in which we match texture loss in both spatial and temporal streams, but using two separate videos (Fig. <xref rid="Fig5" ref-type="fig">5a</xref>). The idea is to generate a video that is matched in space to one video and in time to another. All other hyperparameters were kept the same as for the previously generated model metamers.<fig id="Fig5"><label>Fig. 5</label><caption><title>Metamer stimuli for spatiotemporal factorization.</title><p><bold>a</bold>, Graphical illustration of the STST algorithm for generating a video that is matched with two target videos that are independently given as input to the spatial and temporal streams. <bold>b</bold>, Example of a video generated with this procedure. At the top are frames from the spatial (orange) and temporal (green) target, as well as from the generated STST (azure) video. Below are the low-level statistics of the three videos. <bold>c</bold>, Stimulus properties from stimulus sets 1 and 2. On the bottom left of each video are the 3D color distribution and its palette. On the bottom right, a radar plot shows the distribution of optical flow across the frames. <bold>d</bold>, Distances between all pairs of stimuli in each set for all low-level statistics. For the color distribution, the distance metric is the KL divergence. For the other low-level statistics we used the Euclidean distance. The legend on the left reports the order of the bar plots on the right. <bold>e</bold>, Pearson correlation between the original video from the spatial (orange) or temporal (green) stream and the corresponding spatiotemporally factorized metamer video for each low-level statistic. Notably, the metamer videos have higher correlation with the spatial targets in the spatial statistics and higher correlation with the temporal targets in the temporal statistics. The legend on the left reports the order of the bar plots on the right. Mag., magnitude of the optical flow. Natural images in <bold>a</bold>&#x02013;<bold>e</bold> adapted from Google LLC under a Creative Commons license <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</ext-link>.</p><p><xref rid="MOESM11" ref-type="media">Source data</xref></p></caption><graphic xlink:href="43588_2024_746_Fig5_HTML" id="d33e976"/></fig></p><p id="Par25">Figure <xref rid="Fig5" ref-type="fig">5b</xref> presents an example of generated frames alongside their low-level statistics. The spatiotemporally factorized metamer video (blue) is following the spatial statistics of the spatial target video (red) and the temporal statistics of the temporal target video (green). We used this approach in a proof-of-principle study to investigate how biological and artificial visual systems represent spatial and temporal features in these mixed stimuli compared to their natural counterparts. Specifically, we asked to what extent similarities can be independently judged in the temporal and spatial domains. We hypothesized that, if this were the case, the same spatiotemporally mixed metamer video should be perceived as more similar to either the spatial or temporal natural target video depending on the relevant feature domain.</p><p id="Par26">We performed this experiment on two sets of three videos (see Supplementary Video <xref rid="MOESM6" ref-type="media">2</xref> for the first set and Supplementary Video <xref rid="MOESM7" ref-type="media">3</xref> for the second set). The first stimulus set were the three example videos we also used in all the other experiments (Fig. <xref rid="Fig5" ref-type="fig">5c</xref>, top row). We quantified the videos&#x02019; color distribution with their palette and the optical flow distribution (Fig. <xref rid="Fig5" ref-type="fig">5c</xref>). It turned out that, by chance, the three videos in stimulus set 1 had a similar temporal profile and a dissimilar spatial profile. To quantify this, we computed distance measures between each pair of videos along all the low-level statistics we had used so far, with the addition of the full color distribution (Fig. <xref rid="Fig5" ref-type="fig">5d</xref>). Although the spatiotemporal similarities were not concerning in all the previous investigations, for the current proof-of-principle experiment this could bias similarity judgments towards the more distinct spatial domain.</p><p id="Par27">We thus implemented a second stimulus set with three videos that showed similarity patterns opposite those of set 1 (Fig. <xref rid="Fig5" ref-type="fig">5c</xref>, bottom row). These videos consisted of a quasi-static monkey, a rugby game with the camera following players moving all to the left, and a pole vault athlete running to the right followed by the camera. All videos had a green background. This stimulus set was characterized by more different temporal features and more similar spatial features when compared to set 1 (Fig. <xref rid="Fig5" ref-type="fig">5d</xref>). The STST algorithm allowed us to spatiotemporally mix the target videos of both stimulus sets. For both stimulus sets, the spatial and temporal features of the six mixed metamer videos were correlated with either the spatial (average correlation in stimulus sets 1/2, <italic>r</italic>&#x02009;=&#x02009;0.98/0.95) or temporal (average correlation in stimulus sets 1/2, <italic>r</italic>&#x02009;=&#x02009;0.90/0.78) natural target video, respectively (Fig. <xref rid="Fig5" ref-type="fig">5e</xref>).</p></sec><sec id="Sec9"><title>Spatial bias in humans and models with factorized metamers</title><p id="Par28">Based on these stimuli, we collected data from human participants using a 2AFC spatiotemporal perceptual similarity task (Fig. <xref rid="Fig6" ref-type="fig">6a</xref>). Participants were instructed to report which of two sequentially presented option videos (either the original spatial or temporal target) appeared more similar to the previously presented reference video (one of the mixing combinations). Before the start of each trial, a text cue instructed participants to base their similarity judgments on either spatial or temporal features. We quantified the results as the accuracy of human observers in detecting the correct spatial or temporal target video for the feature domain at hand.<fig id="Fig6"><label>Fig. 6</label><caption><title>Testing humans and deep video models on spatiotemporally factorized metamer stimuli.</title><p><bold>a</bold>, Graphical description of the 2AFC spatiotemporal perceptual similarity task performed by human participants. <bold>b</bold>, Spatiotemporal discriminability for stimulus set 1 (<italic>n</italic>&#x02009;=&#x02009;13). Mixing matrices show the accuracy of participants in discriminating either space (orange) or time (green) for all combinations. Bars show average accuracies across on-diagonal and off-diagonal entries. Error bars represent s.e.m. Dashed horizontal lines indicate chance level. Dots indicate individual participants. <bold>c</bold>, Spatiotemporal discriminability for stimulus set 2 (<italic>n</italic>&#x02009;=&#x02009;12). The conventions are the same as in <bold>b</bold>. <bold>d</bold>, Graphical illustration of the methodology employed to test deep video models in analogy to the behavioral task for humans. All off-diagonal STST mixing combinations were given as input to the models alongside their respective spatial and temporal targets. We then computed the CKA score in early and late layers between the STST and original videos to assess either the spatial (orange) or temporal (green) representational similarity. <bold>e</bold>, CKA results for stimulus set 1. Bars show the average CKA scores across all off-diagonal combinations. Error bars denote s.e.m. Dots indicate individual combinations (<italic>n</italic>&#x02009;=&#x02009;6). The corresponding mixing matrices are shown above the bars. <bold>f</bold>, CKA results for stimulus set 2. The conventions are the same as in <bold>e</bold>. Natural images in <bold>a</bold>&#x02013;<bold>d</bold> adapted from Google LLC under a Creative Commons license <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</ext-link>.</p><p><xref rid="MOESM12" ref-type="media">Source data</xref></p></caption><graphic xlink:href="43588_2024_746_Fig6_HTML" id="d33e1078"/></fig></p><p id="Par29">As expected, for videos with the same spatial and temporal target videos (matrix diagonal in Fig. <xref rid="Fig6" ref-type="fig">6b,c</xref>), participants were significantly better than chance at discriminating spatial and temporal similarity (all <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0002, all <italic>d</italic>&#x02009;&#x0003e;&#x02009;2.29). However, these trials were comparatively easy, as the comparison was made between the original target video and another original video. Accordingly, we next focused on the mixed metamer stimuli, which matched both options in either feature dimension (off-diagonal in Fig. <xref rid="Fig6" ref-type="fig">6b,c</xref>). We found that for these challenging trials too, the spatial accuracy was higher than chance for both stimulus sets (stimulus set 1/2: accuracy&#x02009;=&#x02009;0.88/0.89, <italic>P</italic>&#x02009;&#x0003c;&#x02009;0.0001/0.0001, <italic>d</italic>&#x02009;=&#x02009;3.82/3.72). Also, for these trials, the temporal accuracy was higher than chance for the temporally more distinct stimulus set 2, but not for set 1 (stimulus set 1/2: accuracy&#x02009;=&#x02009;0.42/0.67, <italic>P</italic>&#x02009;=&#x02009;0.387/0.039, <italic>d</italic>&#x02009;=&#x02009;&#x02212;0.35/0.96). For both sets, the spatial accuracy was significantly higher than the temporal one (stimulus set 1/2: <italic>P</italic>&#x02009;=&#x02009;0.001/0.013, <italic>d</italic>&#x02009;=&#x02009;1.92/1.09). These results show that humans can independently judge spatial and temporal video similarities, and suggest a similarity judgment bias towards spatial features.</p><p id="Par30">Finally, we also tested deep vision models trained for video classification (ResNet18-3D and ResNet18-MC3) using the spatiotemporally factorized videos (Fig. <xref rid="Fig6" ref-type="fig">6d</xref>). To compare their behavior with the human data, we passed as inputs all off-diagonal mixed metamer videos and the corresponding spatial and temporal target videos. We extracted the models&#x02019; early and late layer representations and computed the CKA score between the STST videos and their original videos to obtain a spatial and temporal similarity score. Across both stimulus sets (Fig. <xref rid="Fig6" ref-type="fig">6e,f</xref>), we found that the models&#x02019; representations of the mixed metamer videos were more similar to the spatial target than to the temporal target. We observed this pattern for both early and late layer representations and across both tested models. Overall, these results suggest that human similarity judgments and representational similarities in deep vision models trained for video classification are biased towards spatial over temporal features, providing insights into the representational capabilities of biological and artificial systems for the spatiotemporal integration of dynamic visual information.</p></sec></sec><sec id="Sec10" sec-type="discussion"><title>Discussion</title><p id="Par31">Here we propose an STST algorithm as a flexible framework for dynamic visual stimulus generation. The proposed framework may facilitate vision science beyond the applications shown here. For instance, in the present examples we only manipulate the texture (style) of the spatiotemporal targets, either from the same natural video or by factorizing space and time with two different videos. Future applications of STST may explore all possible combinations of content and texture loss along the spatial and temporal dimensions, with up to four distinct target videos. For example, one other application is the so-called video style transfer<sup><xref ref-type="bibr" rid="CR67">67</xref>&#x02013;<xref ref-type="bibr" rid="CR69">69</xref></sup>, in which the content from one target video is combined with the style from another video or image. For this application, other algorithms have been proposed, some of which also exploit optical flow information to improve perceptual stability<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>. The proposed STST algorithm differs from these video style transfer alternatives<sup><xref ref-type="bibr" rid="CR67">67</xref>&#x02013;<xref ref-type="bibr" rid="CR69">69</xref></sup> in terms of model architecture (two-stream approach), flexibility and its ability to match spatiotemporal stimulus statistics at various levels. These aspects are especially relevant when conceiving our approach as a dynamic visual stimulus generation framework. This is particularly the case in the field of NeuroAI, where the comparison of biological and artificial systems is increasingly evaluated by means of out-of-distribution stimuli<sup><xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR70">70</xref>&#x02013;<xref ref-type="bibr" rid="CR72">72</xref></sup>, that is, stimuli that diverge from the training set of artificial systems and that are faced by biological systems during their lifetimes. The STST framework thus opens an intriguing out-of-distribution regime for video stimuli.</p><p id="Par32">Our specific choice of spatial and temporal stream models was motivated by previous studies on image style transfer and dynamic texture synthesis<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>. Beyond these specific models, the STST algorithm provides a general framework for spatiotemporal factorization. Future research may explore different settings of the proposed framework, for example by using different spatial models or temporal stream models that allow the consideration of longer temporal scales than with the model implemented here. Nevertheless, using the described parameters and models, the proposed framework may already provide a suitable and robust solution for many applications, including the showcased examples.</p><p id="Par33">As such examples, we generated metamer stimuli that were invariant to the spatial and temporal texture of natural videos. We benchmarked our method to the only other currently available method (to the best of our knowledge), STPS<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, to generate dynamic stimuli preserving low-level spatiotemporal statistics. STST outperformed STPS, especially in preserving the optical flow of videos. This may be ascribed to the fact that STPS randomizes the phase of the spatiotemporal frequency spectrum, whereas our method uses, as part of its objective, the Gram matrix of the layer activations of an optical flow estimation model. These findings highlight the potential of our algorithm for the study of object recognition in dynamic natural vision.</p><p id="Par34">We tested the effectiveness of STST-generated stimuli in preserving low-level features and ablating high-level features using state-of-the-art deep vision models<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup> for image and video classification. These models parallel the hierarchical organization of the mammalian visual cortex in the sequential extraction and integration of visual features<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR52">52</xref>,<xref ref-type="bibr" rid="CR73">73</xref></sup>. Notably, we employed convolutional-only models in our analysis, as it has been shown that vision transformer architectures<sup><xref ref-type="bibr" rid="CR74">74</xref>,<xref ref-type="bibr" rid="CR75">75</xref></sup> do not exhibit a hierarchical structure in the visual feature extraction process<sup><xref ref-type="bibr" rid="CR76">76</xref></sup>.</p><p id="Par35">Our results show that not only image, but also video classification models exhibit similar early-layer activation for metamer and natural stimuli, whereas later layers diverge. Notably, there is less research on the effective nature of hidden representations of video-processing models and how they relate to biological visual systems<sup><xref ref-type="bibr" rid="CR77">77</xref>,<xref ref-type="bibr" rid="CR78">78</xref></sup>. Our results confirm that for video models and low-level temporal features also, early layers similarly encoded metamer and natural videos, and late layers are also disrupted in metamers when integrating motion.</p><p id="Par36">We also investigated the representational capabilities of PredNet, a DNN inspired by the concepts of predictive coding<sup><xref ref-type="bibr" rid="CR58">58</xref>&#x02013;<xref ref-type="bibr" rid="CR60">60</xref></sup>. The measures of predictiveness quantified its predictive ability beyond static image quality or simple frame-copying<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>, uncovering limitations in exploiting the object-related motion patterns present in natural videos<sup><xref ref-type="bibr" rid="CR58">58</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup>. We speculate that this effect may be due to the fact that we used the version of PredNet trained to minimize only the lowest-level prediction error, as this version has been shown to perform best on next-frame prediction tasks<sup><xref ref-type="bibr" rid="CR60">60</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup>. However, this architectural implementation differs from the original theoretical proposition of predictive coding<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. Furthermore, our analyses revealed that STST better preserved consecutive frame dependencies than STPS, which renders STST particularly well suited for this application.</p><p id="Par37">Complementarily to artificial systems, we also investigated the effectiveness of our algorithm in human observers. Human participants described metamer stimuli less consistently than natural videos, reflecting their lack of high-level content. Crucially, descriptions and similarity ratings indicated that STST metamer videos better preserved low-level features compared to STPS, with a strong preference for STST in direct comparisons to natural videos. This preference probably stems from STST&#x02019;s better match with natural optical flow compared to STPS, suggesting that optical flow is important for perceived naturalness in dynamic stimuli. Consistently, deep vision models showed greater similarity between STST and natural videos than with STPS, highlighting a notable parallel between biological and artificial systems in the role of optical flow in shaping representations<sup><xref ref-type="bibr" rid="CR79">79</xref></sup>, even in models trained solely on image classification.</p><p id="Par38">Finally, we introduced a procedure for the spatiotemporal factorization of dynamic stimuli by matching the model layer activations of the spatial and temporal streams to two different target videos. Our results suggest a spatial bias in similarity judgments, which may stem from the additional computational effort due to temporal integration associated with temporal information<sup><xref ref-type="bibr" rid="CR80">80</xref></sup>. Alternately, and well compatible with our findings in deep vision models, this bias may reflect a quantitative bias in the number of computational units for spatial as compared to temporal features. Further investigations are required to investigate these alternatives. A possible confounding factor of these findings is the relative discrepancy between metamer and target videos for spatial and temporal features. Specifically, in factorized metamers, spatial features were slightly better matched with their natural targets than temporal features. Moreover, the average correlation of temporal features with their targets was higher in stimulus set 1 (0.90) than in stimulus set 2 (0.78). Nevertheless, in the temporal task, both humans and the deep vision model had higher accuracy and CKA scores for stimulus set 2 than for set 1. This provides evidence against the hypothesis that the discrepancy between metamer and target features explains the results.</p><p id="Par39">In conclusion, the proposed STST algorithm allows the powerful manipulation and synthesis of video stimuli for vision research. Our results shed light on potential applications of STST and promote it as a versatile tool for dynamic stimulus generation.</p></sec><sec id="Sec11"><title>Methods</title><sec id="Sec12"><title>Model architecture</title><p id="Par40">Our proposed algorithm is based on a two-stream model architecture consisting of one module (or &#x02018;stream&#x02019;) acting as a spatial feature detector on each frame of the target video, and another module performing feature detection in the temporal domain across consecutive pairs of frames (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). The model is agnostic to the specific implementations of both spatial and temporal modules, so any pretrained differentiable model can be used. Thus, in the following, we formally denote the spatial module as a differentiable function <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{S}}\left({{x}}_{t}\right)$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi mathvariant="script">S</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq1.gif"/></alternatives></inline-formula> that takes as input the current frame defined as a 3D tensor <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{x}}_{t}\in {{\mathbb{R}}}^{H\times W\times C}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq2.gif"/></alternatives></inline-formula>, where <italic>H</italic> and <italic>W</italic> are the height and width of the image in pixels, <italic>C</italic> is the number of channels in the frame (in our case <italic>C</italic>&#x02009;=&#x02009;3, as we are working with RGB-encoded frames) and the subscript <italic>t</italic> denotes the time step. Each element of the tensor <italic>x</italic><sub><italic>t</italic></sub> represents the intensity of the pixel at position (<italic>H</italic>,&#x02009;<italic>W</italic>) in channel <italic>C</italic>, with a value in the range [0,&#x02009;1]. We also denote the temporal module as a differentiable function <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{T}}\left({{{x}}}_{t},\,{{{x}}}_{t-1}\right)$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq3.gif"/></alternatives></inline-formula> that takes as input both the current (<italic>x</italic><sub><italic>t</italic></sub>) and previous (<inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{x}}}_{t-1}$$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq4.gif"/></alternatives></inline-formula>) frame.</p><p id="Par41">We opted to maintain consistency with previous works on NST and its extension to dynamic textures. Accordingly, we adopted VGG-19 (ref. <sup><xref ref-type="bibr" rid="CR43">43</xref></sup>) as the spatial module and the MSOE model<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup> as the temporal module. VGG-19 is a pretrained CNN model<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> that was trained on the ImageNet1k dataset<sup><xref ref-type="bibr" rid="CR81">81</xref>,<xref ref-type="bibr" rid="CR82">82</xref></sup> for object recognition, and it was used originally in both ref. <sup><xref ref-type="bibr" rid="CR21">21</xref></sup> and ref. <sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. We also opted for this model because recent evidence has shown that this type of architecture outperforms many other models in the task of style transfer or texturization<sup><xref ref-type="bibr" rid="CR83">83</xref></sup>, although the reasons for this remain debated<sup><xref ref-type="bibr" rid="CR46">46</xref>,<xref ref-type="bibr" rid="CR83">83</xref>,<xref ref-type="bibr" rid="CR84">84</xref></sup>. MSOE is a pretrained CNN model that was trained on the UCF101 dataset<sup><xref ref-type="bibr" rid="CR85">85</xref></sup> to predict optical flow and was similarly used in ref. <sup><xref ref-type="bibr" rid="CR33">33</xref></sup> as the temporal module. We opted for this model because it is generally invariant to spatial features<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> and thus relies entirely on the temporal dynamics of the video data to estimate optical flow.</p><p id="Par42">We denoted the layer activations of the CNN models as <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathscr{A}}}^{{\ell}}\in {{\mathbb{R}}}^{{N}_{{\ell}}\times {M}_{{\ell}}}$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq5.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${N}_{{\ell}}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${M}_{{\ell}}$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq7.gif"/></alternatives></inline-formula> refer to the number of filters and the number of spatial locations (the height times the width of the feature map) in convolutional layer <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}$$\end{document}</tex-math><mml:math id="M16"><mml:mi>&#x02113;</mml:mi></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq8.gif"/></alternatives></inline-formula>. The layer activation <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathscr{A}}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M18"><mml:msup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq9.gif"/></alternatives></inline-formula> is defined as the results of the forward pass to that specific layer <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}$$\end{document}</tex-math><mml:math id="M20"><mml:mi>&#x02113;</mml:mi></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq10.gif"/></alternatives></inline-formula> after both the convolutional operator and the nonlinear activation function. Here, note that we refer to the layer activation <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathscr{A}}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M22"><mml:msup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq11.gif"/></alternatives></inline-formula> for both the forward pass of <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{S}}\left({{{x}}}_{t}\right)$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mi mathvariant="script">S</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq12.gif"/></alternatives></inline-formula> and <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{T}}\left({{{x}}}_{t},\,{{{x}}}_{t-1}\right)$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq13.gif"/></alternatives></inline-formula>, indiscriminately.</p></sec><sec id="Sec13"><title>Optimization procedure</title><p id="Par43">We now define the optimization procedure to generate what we refer to as &#x02018;model metamers&#x02019;, which are generated dynamic stimuli possessing certain spatiotemporal features that are indistinguishable from their natural counterparts for the two-stream model<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. The STST algorithm can generate metamers that are matched in terms of content (shape) and style (texture) along both spatial and temporal streams. Following the work of Gatys and colleagues<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, we define the loss function of the content-matching procedure as the squared difference between the layer activations <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathscr{A}}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M28"><mml:msup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq14.gif"/></alternatives></inline-formula> of the target frame <inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{x}}}_{t}$$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq15.gif"/></alternatives></inline-formula> and the layer activations <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{{\mathscr{A}}}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M32"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq16.gif"/></alternatives></inline-formula> of the generated frame <inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{g}}}_{t}\in {{\mathbb{R}}}^{H\times W\times C}$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq17.gif"/></alternatives></inline-formula>:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal L}}_{\rm{content}}^{{\ell}}\left({{{x}}}_{t},\,{{{g}}}_{t}\right)=\frac{1}{2{N}_{{\ell}}{M}_{{\ell}}}\mathop{\sum }\limits_{i=1}^{{N}_{{\ell}}}\mathop{\sum }\limits_{j=1}^{{M}_{{\ell}}}{\left({{\mathscr{A}}}_{i,\,j}^{{\ell}}-{\hat{{\mathscr{A}}}}_{i,\,j}^{{\ell}}\right)}^{2}$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">content</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x0005e;</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><graphic xlink:href="43588_2024_746_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par44">Optimizing this loss function for all frames allows the algorithm to match the spatial or temporal structure of the target video. In other words, by using merely the content-matching procedure, the algorithm generates approximately the same video that was used as target. Note that, for the temporal module <inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{T}}\left({{{x}}}_{t},\,{{{x}}}_{t-1}\right)$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq18.gif"/></alternatives></inline-formula>, the input is both the current <inline-formula id="IEq19"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{x}}}_{t}$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq19.gif"/></alternatives></inline-formula> and previous <inline-formula id="IEq20"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{x}}}_{t-1}$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq20.gif"/></alternatives></inline-formula> target frames, so the current generated frame <inline-formula id="IEq21"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{g}}}_{t}$$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq21.gif"/></alternatives></inline-formula> and the previous generated frame <inline-formula id="IEq22"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{g}}}_{t-1}$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq22.gif"/></alternatives></inline-formula> also need to be provided.</p><p id="Par45">Conversely, to perform texture matching we need to first obtain a representation of the texture information<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Here we refer to texture as either spatial or temporal recurrent patterns (namely, a set of stationary statistics). We define this texture representation as the correlation between different filter responses within a layer activation<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>, encapsulated by the so-called Gram matrix <inline-formula id="IEq23"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}^{{\ell}}\in {{\mathbb{R}}}^{{N}_{{\ell}}\times {N}_{{\ell}}}$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq23.gif"/></alternatives></inline-formula>, whose entries are given by<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{i,\,j}^{{\ell}}={\frac{1}{{N}_{{\ell}}{M}_{{\ell}}}\mathop{\sum }\limits_{k=1}^{{M}_{{\ell}}}{{\mathscr{A}}}_{i,\,k}^{{\ell}}{{\mathscr{A}}}_{j,\,k}^{{\ell}}}$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math><graphic xlink:href="43588_2024_746_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par46">Thus, the loss function of the texture-matching procedure is defined as<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{texture}}^{{\ell}}\left({{{x}}}_{t},\,{{{g}}}_{t}\right)={\frac{1}{2{{N}_{{\ell}}}^{2}}\mathop{\sum }\limits_{i=1}^{{N}_{{\ell}}}\mathop{\sum }\limits_{j=1}^{{N}_{{\ell}}}{\left({G}_{i,\,j}^{{\ell}}-{\hat{G}}_{i,\,j}^{{\ell}}\right)}^{2}}$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">texture</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x0005e;</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><graphic xlink:href="43588_2024_746_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq24"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}^{{\ell}}$$\end{document}</tex-math><mml:math id="M54"><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq24.gif"/></alternatives></inline-formula> is the Gram matrix computed from layer activations <inline-formula id="IEq25"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathscr{A}}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M56"><mml:msup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq25.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq26"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{G}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M58"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq26.gif"/></alternatives></inline-formula> is computed from <inline-formula id="IEq27"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{{\mathscr{A}}}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M60"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq27.gif"/></alternatives></inline-formula>. Generating metamers with the <inline-formula id="IEq28"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{texture}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M62"><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">texture</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq28.gif"/></alternatives></inline-formula> loss results in videos that no longer maintain a retinotopic correspondence with the original natural video. However, a variety of low-level statistical properties are preserved, depending on which dimension it is applied to. For example, in the spatial stream, the luminosity and contrast are well preserved, as is the magnitude of the optical flow or the global motion in the temporal stream. We can finally define the general form of the loss function for our STST algorithm as<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{spatial}}\left({{{x}}}_{t},\,{{{g}}}_{t}\right)=\mathop{\sum }\limits_{{\ell}{{=}}1}^{{L}_{\rm{s}}}\left(\alpha {{\mathcal{L}}}_{\rm{sc}}^{{\ell}}\left({{{x}}}_{t},\,{{{g}}}_{t}\right)+\beta {{\mathcal{L}}}_{\rm{st}}^{{\ell}}\left({{{x}}}_{t},\,{{{g}}}_{t}\right)\right)$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">spatial</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mfenced close=")" open="("><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">sc</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">st</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="43588_2024_746_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{l}{{\mathcal{L}}}_{\rm{temporal}}\left({{{x}}}_{t},\,{{{g}}}_{t},\,{{{x}}}_{t-1},\,{{{g}}}_{t-1}\right)\\=\mathop{\sum }\limits_{{\ell}{{=}}1}^{{L}_{\rm{t}}}\left(\theta {{\mathcal{L}}}_{\rm{tc}}^{{\ell}}\left({{{x}}}_{t},\,{{{g}}}_{t},\,{{{x}}}_{t-1},\,{{{g}}}_{t-1}\right)+\lambda {{\mathcal{L}}}_{\rm{tt}}^{{\ell}}\left({{{x}}}_{t},\,{{{g}}}_{t},\,{{{x}}}_{t-1},\,{{{g}}}_{t-1}\right)\right)\end{array}$$\end{document}</tex-math><mml:math id="M66"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">temporal</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mfenced close=")" open="("><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">tc</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">tt</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="43588_2024_746_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{STST}}\left({{{x}}}_{t},\,{{{g}}}_{t},\,{{{x}}}_{t-1},\,{{{g}}}_{t-1}\right)={{\mathcal{L}}}_{\rm{spatial}}\left({{{x}}}_{t},\,{{{g}}}_{t}\right)+{{\mathcal{L}}}_{\rm{temporal}}\left({{{x}}}_{t},\,{{{g}}}_{t},\,{{{x}}}_{t-1},\,{{{g}}}_{t-1}\right)$$\end{document}</tex-math><mml:math id="M68"><mml:mrow><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">STST</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">spatial</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">temporal</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="43588_2024_746_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <italic>L</italic><sub>s</sub> and <italic>L</italic><sub>t</sub> are the number of selected layers in the spatial and temporal modules, respectively. Moreover, <italic>&#x003b1;</italic> and <italic>&#x003b2;</italic> represent the weights associated with the content <inline-formula id="IEq29"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{sc}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M70"><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">sc</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq29.gif"/></alternatives></inline-formula> and texture <inline-formula id="IEq30"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{st}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M72"><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">st</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq30.gif"/></alternatives></inline-formula> loss in the spatial loss <inline-formula id="IEq31"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{spatial}}$$\end{document}</tex-math><mml:math id="M74"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">spatial</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq31.gif"/></alternatives></inline-formula>, and <italic>&#x003b8;</italic> and <italic>&#x003bb;</italic> are the weights for the content <inline-formula id="IEq32"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{tc}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M76"><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">tc</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq32.gif"/></alternatives></inline-formula> and texture <inline-formula id="IEq33"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{tt}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M78"><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">tt</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq33.gif"/></alternatives></inline-formula> loss in the temporal loss <inline-formula id="IEq34"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{temporal}}$$\end{document}</tex-math><mml:math id="M80"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">temporal</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq34.gif"/></alternatives></inline-formula>. These weights are set as hyperparameters that modulate the relative contribution of each of the four terms to the general loss function <inline-formula id="IEq35"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{STST}}$$\end{document}</tex-math><mml:math id="M82"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">STST</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq35.gif"/></alternatives></inline-formula>. Furthermore, the selection of a restricted number of terms (for instance, selecting only the spatial and temporal texture losses) can be seen as setting the undesired terms&#x02019; weights to zero. This provides a versatile tool for dynamic stimulus generation, as one can perform many combinations of the four losses that compose the general loss function. Crucially, one can select more than one target video and even assign a different target video for each of the four loss terms.</p><p id="Par47">Once the total loss for the optimization procedure is defined, the actual update of the parameter space <inline-formula id="IEq36"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{g}}}_{t}$$\end{document}</tex-math><mml:math id="M84"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq36.gif"/></alternatives></inline-formula> consists of applying a gradient-based optimization method such as gradient descent, which iteratively adjusts the parameters in the direction that minimizes the loss:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{g}}}_{t}:={{{{g}}}_{t}-\eta \frac{\partial {{\mathcal{L}}}_{\rm{STST}}}{\partial {{{g}}}_{t}}}$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:mfrac><mml:mrow><mml:mi>&#x02202;</mml:mi><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">STST</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>&#x02202;</mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="43588_2024_746_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where <italic>&#x003b7;</italic> is the learning rate that weights the gradient of the loss with respect to the parameters and is set as a hyperparameter. The number of iterations needed to update the parameters <inline-formula id="IEq37"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{g}}}_{t}$$\end{document}</tex-math><mml:math id="M88"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq37.gif"/></alternatives></inline-formula> is also a hyperparameter of STST. Importantly, the gradients are computed and applied only with respect to the current generated frame, even in the case in which the temporal module is performing the forward pass with both current and previous frames. This is because optimizing the full pair of consecutive frames would lead to instabilities in the optimization. The generation of the dynamic stimulus is finished when the optimization procedure has been iterated across all frames.</p></sec><sec id="Sec14"><title>Perceptual stabilization via preconditioning</title><p id="Par48">Although the optimization procedure described above performs well for videos with simple spatiotemporal dynamics, as similarly shown by Tesfaldet and colleagues<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> for the generation of dynamic textures, it encounters substantial limitations when applied to natural video sequences. Empirically, it tends to manifest considerable perceptual instabilities both in the space and time domains, so we incorporate a family of techniques for image and video synthesis that substantially improve its robustness and perceptual stability. The employed techniques can be generally described as a &#x02018;preconditioning&#x02019; of the optimization procedure. In mathematical optimization, preconditioning refers to a transformation of an optimization problem to condition it to a form that is more suitable for finding optimal solutions. Intuitively, preconditioning changes the basins of attraction, allowing the optimization process to approach preferred solutions more easily<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>.</p><p id="Par49">The first preconditioning technique that we introduced is the addition of the total variation (TV) loss<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> to our loss function. The TV loss is a regularization term often used in image-processing algorithms to encourage smoothness in the output while preserving edges, and it was introduced in the literature of image generation from early works on the DeepDream algorithm<sup><xref ref-type="bibr" rid="CR46">46</xref>,<xref ref-type="bibr" rid="CR86">86</xref></sup>. Intuitively, the idea is that in natural images, pixel values tend to change gradually except at the edges, where there are abrupt changes. The TV loss penalizes the sum of the absolute differences between neighboring pixel values, as described by the following anisotropic 2D version of the formula:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{TV}}\left({{{g}}}_{t}\right)=\frac{1}{{HW}C}\mathop{\sum }\limits_{k=1}^{C}\mathop{\sum }\limits_{i,\,j}^{H,\,W}\left|{{{g}}}_{t}^{i+1,\,j,\,k}-{{{g}}}_{t}^{i,\,j,\,k}\right|+\left|{{{g}}}_{t}^{i,\,j+1,\,k}-{{{g}}}_{t}^{i,\,j,\,k}\right|$$\end{document}</tex-math><mml:math id="M90"><mml:mrow><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TV</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:munderover><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>W</mml:mi></mml:mrow></mml:munderover><mml:mfenced close="&#x02223;" open="&#x02223;"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close="&#x02223;" open="&#x02223;"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="43588_2024_746_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par50">This additional loss helps to prevent the excessive high-frequency noise that results from the optimization procedure described above. Specifically, this arises from the spatial module and most probably the convolutional nature of the selected model, as it has been shown that strided convolutions and pooling operations can create high-frequency patterns in the gradients<sup><xref ref-type="bibr" rid="CR46">46</xref>,<xref ref-type="bibr" rid="CR84">84</xref></sup>. Note that this loss term is applied on the current generated frame during the optimization process, so there is an additional hyperparameter <italic>&#x003c9;</italic> to weight the contribution of the <inline-formula id="IEq38"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{TV}}$$\end{document}</tex-math><mml:math id="M92"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TV</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq38.gif"/></alternatives></inline-formula> term to the total loss.</p><p id="Par51">Another technique we borrowed from the DeepDream field is the spatial multiscale approach<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>. It basically consists of starting the optimization with an image at a lower resolution and iteratively enlarging it to the original size. This is done to inject spatial frequency biases into the parameter space, by initially providing low-frequency patterns and then refining high-frequency ones. We implemented this approach by resizing both the targets and generated frames using bilinear interpolation and a hyperparameter <italic>&#x003c3;</italic> to control the so-called octave scale. The octave values <inline-formula id="IEq39"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{o}}$$\end{document}</tex-math><mml:math id="M94"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">o</mml:mi></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq39.gif"/></alternatives></inline-formula> are the exponent power of the octave scale, used for multiplying them with the original frame size <italic>H</italic>&#x02009;&#x000d7;&#x02009;<italic>W</italic>, for example, <inline-formula id="IEq40"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${H}_{{\mathcal{o}}}={H{\sigma }^{{\mathcal{o}}}}$$\end{document}</tex-math><mml:math id="M96"><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">o</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq40.gif"/></alternatives></inline-formula> and <inline-formula id="IEq41"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${W}_{{\mathcal{o}}}={W{\sigma }^{{\mathcal{o}}}}$$\end{document}</tex-math><mml:math id="M98"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">o</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq41.gif"/></alternatives></inline-formula>. Note that <inline-formula id="IEq42"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{o}}={0}$$\end{document}</tex-math><mml:math id="M100"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">o</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq42.gif"/></alternatives></inline-formula> yields the original resolution. The aforementioned optimization procedure is applied on each octave, possibly even with different hyperparameters, such as different learning rates for different octaves. We followed the authors that introduced the multiscale approach and normalized the gradients of the total loss with respect to <inline-formula id="IEq43"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{g}}}_{t}$$\end{document}</tex-math><mml:math id="M102"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq43.gif"/></alternatives></inline-formula> by dividing them with their standard deviation across the height <inline-formula id="IEq44"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${H}_{{\mathcal{o}}}$$\end{document}</tex-math><mml:math id="M104"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">o</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq44.gif"/></alternatives></inline-formula> and width <inline-formula id="IEq45"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${W}_{{\mathcal{o}}}$$\end{document}</tex-math><mml:math id="M106"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">o</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq45.gif"/></alternatives></inline-formula> dimensions<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>.</p><p id="Par52">In the STST algorithm we also implemented a method to match the color distribution between <inline-formula id="IEq46"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{x}}}_{t}$$\end{document}</tex-math><mml:math id="M108"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq46.gif"/></alternatives></inline-formula> and <inline-formula id="IEq47"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{g}}}_{t}$$\end{document}</tex-math><mml:math id="M110"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq47.gif"/></alternatives></inline-formula>. We noticed that this substantially improved the similarity between the targeted and generated color distributions compared to only applying the optimization. Thus, following the multiscale procedure, we applied a color transfer algorithm as a postprocessing step<sup><xref ref-type="bibr" rid="CR47">47</xref>,<xref ref-type="bibr" rid="CR48">48</xref></sup>. Instead of using a na&#x000ef;ve approach such as histogram matching for each channel individually<sup><xref ref-type="bibr" rid="CR87">87</xref></sup>, the employed algorithm finds a transformation from the full 3D probability density function of the pixel intensity of the target frame to the generated frame, and also reduces the grain artifacts by preserving the gradient field of the target frame<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>.</p><p id="Par53">Finally, we applied a blending operation on the frame transitions to reduce flickering artifacts. This turned out to be a critical step that improved the perceptual stability by preconditioning the initial conditions of the optimization procedure at the current frame with the post-processed frames at the previous time point and the addition of uniform noise <inline-formula id="IEq48"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mu \in {{\mathbb{R}}}^{H\times W\times C}}$$\end{document}</tex-math><mml:math id="M112"><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq48.gif"/></alternatives></inline-formula>, as follows:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{g}}}_{t}\leftarrow \varphi {{{g}}}_{t-1}+\left(1-\varphi \right)\mu ,\,\mu {\mathscr{ \sim }}{\mathscr{U}}\left(0,\,1\right)$$\end{document}</tex-math><mml:math id="M114"><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02190;</mml:mo><mml:mi>&#x003c6;</mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:mfenced><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>&#x003bc;</mml:mi><mml:mi mathvariant="script"> \sim</mml:mi><mml:mi mathvariant="script">U</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="43588_2024_746_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>where <italic>&#x003c6;</italic> is the hyperparameter that controls the blending ratio and &#x02190; stands for initialization. For instance, setting <italic>&#x003c6;</italic>&#x02009;=&#x02009;0.9 results in the current frame being initialized with 90% the pixel intensity of the previous frame, with the rest being uniform noise. Although this blending operation substantially improved the temporal stability of our method, the initial frames of the generated stimuli had slightly different low-level statistics compared to the rest of the frames. In other words, the relative stationarity of the low-level statistics in the generated stimuli with respect to the target stimuli started to be consistent only after a few initial frames. We assumed that this was because the very first frame is initialized only as noise, because there is no previous frame with which to precondition. Also, it is difficult to increase the speed of the optimization process as our algorithm needs to have a relatively low value of <italic>&#x003b7;</italic> to maintain numerical stability. This can be interpreted as the search for stable local minima not just within each frame, but across them, as the parameter space of the next frames is preconditioned on the previous ones by the blending operation. Thus, to remedy the first frame effect, we mirror-padded the target and the generated stimuli for the first <italic>&#x003be;</italic> frames. In other words, we took the first <italic>&#x003be;</italic> frames of the target stimuli, flipped their order, and concatenated them before the first frame. We then discarded the initial padded frames after the optimization.</p></sec><sec id="Sec15"><title>Stimulus generation for object-recognition studies</title><p id="Par54">We generated dynamic stimuli by matching spatiotemporal features to their natural counterparts. We optimized metamer stimuli to match both the spatial and temporal textures of three high-quality video clips collected from the YouTube-8M dataset<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>. The target videos consisted of 120 frames at 60 frames per second (FPS), with a frame resolution of 360&#x02009;&#x000d7;&#x02009;640. We also performed validation analyses on 100 video clips collected from the validation set of the Kinetics400 dataset<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. We selected one random video from 100 different action labels to maximize the diversity in the sample, analyzing the first 60 frames from each video, corresponding on average to 2&#x02009;s, because the majority of the videos were recorded at 30&#x02009;FPS. The identity code of each video in the dataset is provided in Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>. All statistical comparisons performed on this validation dataset were conducted using two-tailed paired <italic>t</italic>-tests and Cohen&#x02019;s <italic>d</italic> as the effect size metric.</p><p id="Par55">We included in our total loss function only the spatial <inline-formula id="IEq49"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{st}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M116"><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">st</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq49.gif"/></alternatives></inline-formula> and temporal <inline-formula id="IEq50"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{tt}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M118"><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">tt</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq50.gif"/></alternatives></inline-formula> texture loss components to eliminate any high-order regularity. For the spatial stream <inline-formula id="IEq51"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{S}}\left({{{x}}}_{t}\right)$$\end{document}</tex-math><mml:math id="M120"><mml:mrow><mml:mi mathvariant="script">S</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq51.gif"/></alternatives></inline-formula>, we used VGG-19 (ref. <sup><xref ref-type="bibr" rid="CR43">43</xref></sup>) and selected the layers conv1_1, conv2_1, conv3_1, conv4_1 and conv5_1 for the texture loss (as in ref. <sup><xref ref-type="bibr" rid="CR21">21</xref></sup>), and for the temporal stream <inline-formula id="IEq52"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{T}}\left({{{x}}}_{t},\,{{{x}}}_{t-1}\right)$$\end{document}</tex-math><mml:math id="M122"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq52.gif"/></alternatives></inline-formula> we used MSOE<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> and selected the concatenation layer (as in ref. <sup><xref ref-type="bibr" rid="CR33">33</xref></sup>), where the multiscale feature maps of orientations are stored, for the texture loss. For each frame, we selected three octaves <inline-formula id="IEq53"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{o}}{{\in }}{\left[-2,\,-{1,\,0}\right]}$$\end{document}</tex-math><mml:math id="M124"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">o</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq53.gif"/></alternatives></inline-formula> with an octave scale <italic>&#x003c3;</italic>&#x02009;=&#x02009;1.5, resulting in frame resolutions of 160&#x02009;&#x000d7;&#x02009;284, 240&#x02009;&#x000d7;&#x02009;426 and the original one. The weights associated to both texture losses (<italic>&#x003b2;</italic> and <italic>&#x003bb;</italic>) were always set to 1, and the weight <italic>&#x003c9;</italic> for the <inline-formula id="IEq54"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{TV}}$$\end{document}</tex-math><mml:math id="M126"><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TV</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq54.gif"/></alternatives></inline-formula> loss was set in dependence on the octaves, namely having values of 0.05, 0.1 and 0.5, respectively. The hyperparameters of the optimization process were octave-dependent, with iterations of 250, 750 and 1,000 and the learning rate <italic>&#x003b7;</italic> set as 0.001, 0.003 and 0.005, respectively. We set the blending ratio <italic>&#x003c6;</italic> to 0.95 and the number of padding frames <italic>&#x003be;</italic> to 5. Using an NVIDIA A40 graphical processing unit, this optimization process for the three high-quality videos took on average 6&#x02009;h.</p></sec><sec id="Sec16"><title>Comparison to existing methods</title><p id="Par56">We generated dynamic metamer stimuli from the same target videos with a different algorithm<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, STSP. STSP consists of three steps, starting with randomization of the phase spectrum of each frame separately using a 2D fast Fourier transform (FFT). The random phase angles were sampled from a uniform distribution with range [&#x02212;&#x003c0;,&#x02009;&#x003c0;], then applied to all frames. Next we used a 3D FFT to also randomize the phase spectrum of the full spatiotemporal data. Finally, we used the same color transfer algorithm<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> as for STST to enable a fair comparison of the methods. We applied the first step to each channel of the frames separately, the second to all frames together but for each channel separately, and the third step to each frame, because it handles the full 3D distribution of pixel intensity.</p></sec><sec id="Sec17"><title>Spatiotemporal feature analysis</title><p id="Par57">We computed four basic spatiotemporal features from the original videos, our STST stimuli and the STPS stimuli. For the spatial low-level features, for each frame, we computed the average pixel value across all <italic>H</italic>, <italic>W</italic> and <italic>C</italic> dimensions and the luminance contrast as the standard deviation across both <italic>H</italic> and <italic>W</italic> (ref. <sup><xref ref-type="bibr" rid="CR50">50</xref></sup>), after transforming the image from the RGB color space to grayscale using the dot product between the channel values and the following vector [0.299,&#x02009;0.587,&#x02009;0.114]. For the temporal low-level features, we computed the pixel change as the average across all <italic>H</italic>, <italic>W</italic> and <italic>C</italic> dimensions of the absolute difference between consecutive frames<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, and the optical flow feature as the average across <italic>H</italic> and <italic>W</italic> of the magnitude and the angle of the dense optical flow estimation between consecutive frames (grayscaled as above), using the Farneb&#x000e4;ck algorithm<sup><xref ref-type="bibr" rid="CR88">88</xref></sup> with the pyramid scale set to 0.5 with five levels, an averaging window size of 13, ten iterations per level, five pixel neighbors used to find the polynomial expansion in each pixel, and a Gaussian kernel with a standard deviation of 1.1 for smoothing the derivatives used as a basis for the polynomial expansion.</p></sec><sec id="Sec18"><title>Testing vision models on metamer stimuli</title><p id="Par58">We tested the effects of STST stimuli on hidden activations of current state-of-the-art deep learning models for image and video classification. Data analyses were performed using Python 3.9, TensorFlow 2.4 and PyTorch 2.2. We used ResNet50 (ref. <sup><xref ref-type="bibr" rid="CR54">54</xref></sup>) and ConvNeXt-T<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>, trained on the Imagenet1K dataset<sup><xref ref-type="bibr" rid="CR81">81</xref></sup>, as image classification models, and ResNet18-3D<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> and ResNet18-MC3 (ref. <sup><xref ref-type="bibr" rid="CR57">57</xref></sup>), trained on the Kinetics400 dataset<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, as video classification models. For the video classification models, we used five consecutive frames as input and progressed throughout the video with a one-frame step size. For image classification models, we processed individual frames. As early layers, we selected the &#x02018;maxpool&#x02019; layer for ResNet50, &#x02018;features.0&#x02019; for ConvNeXt-T and &#x02018;layer1.0.conv1.1&#x02019; for both ResNet18-3D and ResNet18-MC3. As late layers, we selected &#x02018;layer4.2.relu_2&#x02019; for ResNet50, &#x02018;features.7.2.add&#x02019; for ConvNeXt-T and &#x02018;layer4.1.relu&#x02019; for both ResNet18-3D and ResNet18-MC3. This layer nomenclature was adopted from the torchvision<sup><xref ref-type="bibr" rid="CR89">89</xref></sup> implementation of these models. CKA<sup><xref ref-type="bibr" rid="CR53">53</xref></sup> with a linear kernel was used as a similarity score of the models&#x02019; layer activations between natural and STST stimuli.</p></sec><sec id="Sec19"><title>Testing high-level representations in predictive coding networks</title><p id="Par59">We used metamer stimuli to investigate the role of high-level representations in PredNet<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>, a deep convolutional recurrent neural network with four hierarchical levels that was trained with self-supervised learning methods with the KITTI dataset<sup><xref ref-type="bibr" rid="CR90">90</xref></sup> to perform next-frame prediction<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>. We used the same implementation as ref. <sup><xref ref-type="bibr" rid="CR60">60</xref></sup> and focused our analyses on the model that had the highest predictive performance<sup><xref ref-type="bibr" rid="CR60">60</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup>, namely the one that was trained to minimize the prediction error from the lowest level of the hierarchical structure of PredNet (in the original paper this was referred to as PredNet <italic>L</italic><sub>0</sub>). We passed to the model the metamer stimuli alongside their natural counterparts in a sliding-window fashion, using as context the last ten frames, in line with the original training hyperparameter<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>. The inputs were preprocessed by rescaling them to a frame resolution of 128&#x02009;&#x000d7;&#x02009;160 as required by the model. We extracted from the model the predicted next frame and the average prediction error coming from the output of the error module across all levels of the hierarchy. We then computed the SSIM<sup><xref ref-type="bibr" rid="CR65">65</xref></sup> and its dynamic version conditional on the previous frame (cSSIM)<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>. cSSIM measures how different the predictions are from the previous frame, quantifying how risky the prediction of the model is in comparison to simply performing a copying of the previous frame as a prediction for the current one:<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{cSSIM}}\left({f}_{t},\,{f}_{t-1},\,{\hat{f}}_{t}\right)={\left(1-{\rm{SSIM}}\left({f}_{t-1},\,{\hat{f}}_{t}\right)\right) {\rm{SSIM}}\left({f}_{t},\,{\hat{f}}_{t}\right)}$$\end{document}</tex-math><mml:math id="M128"><mml:mrow><mml:mi mathvariant="normal">cSSIM</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x0005e;</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">SSIM</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x0005e;</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">SSIM</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x0005e;</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="43588_2024_746_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq55"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}_{t}$$\end{document}</tex-math><mml:math id="M130"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq55.gif"/></alternatives></inline-formula> denotes the current frame (either from the original or STST video) and <inline-formula id="IEq56"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{f}}_{t}$$\end{document}</tex-math><mml:math id="M132"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq56.gif"/></alternatives></inline-formula> represents PredNet&#x02019;s prediction of the current frame.</p></sec><sec id="Sec20"><title>Testing model metamers for object recognition in the human visual system</title><p id="Par60">We conducted behavioral experiments to investigate the effects of our metamer stimuli for object recognition on human participants. The experiments were conducted in accordance with the Declaration of Helsinki and approved by the ethics committee of the University of T&#x000fc;bingen. All subjects gave informed consent. No statistical method was used to predetermine the sample sizes of the human experiments. Sample sizes were chosen in accordance with typical sample sizes used in similar studies within the field, ensuring consistency and comparability with established research practices. No data were excluded from the analyses.</p><p id="Par61">First, we collected data for the video-captioning task (<italic>n</italic>&#x02009;=&#x02009;14, mean age&#x02009;=&#x02009;30.4&#x02009;years (3.1&#x02009;s.d.), six females). We asked participants to type on a keyboard an English text description of the presented videos, which had a visual angle of 15&#x000b0;. We instructed them to describe the video as they would describe it to a person that had not seen the video. We restricted them to use between 2 and 30 words and use only letters and spaces, without punctuation. Each trial consisted of the video playing followed by a box in which participants inserted their caption. After an inter-trial interval (ITI) of 2&#x02009;s, the next trial started. We used the same three high-quality videos we used in the previous analyses, alongside their STST and STPS counterparts, for a total of nine videos. The order of appearance was pseudo-randomized by counterbalancing across participants whether they started to watch the STST followed by the STPS and original or starting with STPS and then the STST and original. We opted for this design to avoid the presentation of the original videos first, which could have biased the subsequent trials.</p><p id="Par62">We used the Sentence Transformers library (version 3.0.1)<sup><xref ref-type="bibr" rid="CR91">91</xref></sup> with the paraphrase-mpnet-base-v2 deep transformer model, a Siamese BERT model pretrained for the specific purpose of clustering and semantic search, to extract 768D sentence embeddings from these captions. These embeddings were computed as the mean pooling of the last layer output, taking into account the attention matrix for correct averaging. Next, we performed classification analysis using the support vector machine model with the radial basis function as kernel (rbf-SVM), a regularization value <italic>C</italic> of 1 and a three-fold-stratified cross-validation scheme. The classification was performed for every combination of original, STST and STPS videos on each stimulus, then the results were averaged across stimuli. The statistical significance of the results was tested by randomly shuffling the class labels and generating a null distribution of 10,000 samples of model performance, which was evaluated with the accuracy score. We also computed the cosine distance between the sentence embeddings of all pairs of original, STST and STPS video descriptions and between the original videos and either their respective STST or STPS counterpart videos. Statistical comparisons were evaluated using two-tailed paired <italic>t</italic>-tests and Cohen&#x02019;s <italic>d</italic> as the effect size metric. We also visualized the sentence embeddings using UMAP, with 15 neighbors, cosine distance as a metric, 200 epochs, a learning rate of 1, a minimum distance parameter of 0.1, the spread set to 1 and spectral initialization.</p><p id="Par63">Second, we collected data for the 2AFC perceptual similarity task (<italic>n</italic>&#x02009;=&#x02009;13, mean age&#x02009;=&#x02009;30.0 (2.8&#x02009;s.d.), six females). Six participants completed the task after the previous video-captioning task. Each trial consisted of three videos, interleaved by 300&#x02009;ms and presented with 15&#x000b0; visual angle. The first video was the reference, and the second and third the options of the 2AFC. The videos were the same as the videos used in the previous task, so nine in total (three originals and their STST and STPS counterparts). Participants were instructed to report which of the two option videos (either STST or STPS) were more similar to the reference video (original) by pressing the D and L keys to select the first and second option, respectively. To provide a definition for video similarity, the subjects were instructed as follows: &#x02018;There are no right or wrong answers in this task. We are interested in your personal judgment. By similarity, we mean your perception of one video being visually close to another one.&#x02019;</p><p id="Par64">For each reference video whose stimulus identity was described as stimulus A, we presented as options five different combinations: STST-A versus STPS-A, STST-A versus STST-B, STPS-A versus STPS-B, STPS-A versus STST-B and STST-A versus STPS-B. We denoted as B the STST and STPS videos having as target original videos not the one being presented as the reference. As there were three original videos, the B stimulus could be one of the two remaining ones. So, each of the combinations having the type B stimulus to be presented was shown twice with the two remaining ones. The order of appearance of the two options was randomized, as well as the trial order. We computed the preference score for the STST videos in the combination STST-A versus STPS-A as the number of times each participant selected the STST-A option divided by the number of trials of that combination. All other combinations were evaluated as the accuracy in selecting the option having the type A stimulus. Statistical comparisons were evaluated using two-tailed paired <italic>t</italic>-tests and Cohen&#x02019;s <italic>d</italic> as the effect size metric.</p></sec><sec id="Sec21"><title>Stimulus generation for spatiotemporal factorization studies</title><p id="Par65">We generated a metamer stimulus that was matched in space to one video and in time to another, a process that can be conceived of as spatiotemporal factorization. We used the same three videos used to generate dynamic metamer stimuli for object-recognition studies (stimulus set 1) and a new set of three videos with resolution 320&#x02009;&#x000d7;&#x02009;180 at 30&#x02009;FPS (stimulus set 2).</p><p id="Par66">To quantitatively assess the spatiotemporal similarities, we plotted the full 3D color distribution across the frames, as well as the palette, defined with the <italic>k</italic>-means clustering algorithm with <italic>k</italic>&#x02009;=&#x02009;5, and the Kullback&#x02013;Leibler (KL) divergence between each pair of stimuli in each stimulus set. We also plotted the distribution of the magnitude and angle of the optical flow across the frames, computed with the same method as above (section &#x02018;Spatiotemporal feature analysis&#x02019;). We used the Euclidean distance to measure the differences between each pair of stimuli within each stimulus set for all the low-level spatial and temporal features we analyzed.</p><p id="Par67">Thus, for each combination of these natural videos, we passed one video to the spatial stream <inline-formula id="IEq57"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{S}}\left({{{x}}}_{t}\right)$$\end{document}</tex-math><mml:math id="M134"><mml:mrow><mml:mi mathvariant="script">S</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq57.gif"/></alternatives></inline-formula> and the other to the temporal stream <inline-formula id="IEq58"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{T}}\left({{{x}}}_{t},\,{{{x}}}_{t-1}\right)$$\end{document}</tex-math><mml:math id="M136"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq58.gif"/></alternatives></inline-formula>. Unlike the stimulus-generation procedure above, we computed the spatial <inline-formula id="IEq59"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{st}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M138"><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">st</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq59.gif"/></alternatives></inline-formula> and temporal <inline-formula id="IEq60"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{\rm{tt}}^{{\ell}}$$\end{document}</tex-math><mml:math id="M140"><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">tt</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02113;</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="43588_2024_746_Article_IEq60.gif"/></alternatives></inline-formula> texture loss components with respect to these two targets. All the hyperparameters were set the same as the previous stimulus generation procedure (section &#x02018;Stimulus generation for object-recognition studies&#x02019;). To evaluate this generative procedure, we computed the Pearson correlation between the low-level features of the spatial and temporal target videos and the low-level features of the spatiotemporally factorized STST video.</p></sec><sec id="Sec22"><title>Testing spatiotemporally factorized metamers in humans and deep visual models</title><p id="Par68">We performed a proof-of-principle study on the potential application of spatiotemporal factorization in both humans and deep visual models. We collected data in human participants for the 2AFC spatiotemporal perceptual similarity task, for both stimulus set 1 (<italic>n</italic>&#x02009;=&#x02009;13, mean age&#x02009;=&#x02009;30.3 (3.8&#x02009;s.d.), four females) and stimulus set 2 (<italic>n</italic>&#x02009;=&#x02009;12, mean age&#x02009;=&#x02009;29.3 (3.7&#x02009;s.d.), six females). Among both data collections, five participants completed the task after the previous video-captioning task and the 2AFC perceptual similarity task. As in the previous 2AFC task, each trial consisted of three videos (the reference and the remaining options), interleaved by 300&#x02009;ms and presented with a 15&#x000b0; visual angle. The stimuli were the natural videos from each stimulus set and all the combinations from mixing them for the spatial or temporal stream, with a total of 15 stimuli. Participants were instructed to report which of the two option videos (either the spatial or temporal original target video) was spatially or temporally more similar to the reference video (one of the mixed combinations), by pressing the D and L keys to select the first or second options, respectively. At the beginning of each trial, a visual cue signaled whether similarity was to be judged in space or time. For the trials in which the reference was a video with only one target video (no mixing), we showed as options the spatial-temporal target and another natural video from the stimulus set. Because there were two alternate natural videos, we repeated this type of trial twice. To provide a consistent definition of video similarity, we gave the same instruction as for the previous 2AFC task. For spatial trials, we asked to focus on the shapes, patterns, colors and spatial arrangements in the videos. For the temporal trials, we asked to focus on the motion, timing, rhythm and sequence of the events in the videos. Subjects were also explicitly instructed to ignore temporal aspects during the spatial trials and vice versa. Behavior was quantified as the accuracy of the participants in selecting the target option corresponding to the cued feature dimension. We compared accuracies against chance level (0.5) and between spatial and temporal accuracies across participants using two-tailed paired <italic>t</italic>-tests and Cohen&#x02019;s <italic>d</italic> as the effect size metric.</p><p id="Par69">To test our spatiotemporally factorized videos on deep vision models trained for video classification in a comparable way to humans, we used the same ResNet18-3D and ResNet18-MC3 models used in the previous analysis (section &#x02018;Testing vision models on metamer stimuli&#x02019;). We extracted early and late layer activations, as defined above, from each combination of mixing video as well as from their spatial and temporal targets. We included in the analyses only the mixed video combinations (off-diagonal), as the unmixed (on-diagonal) videos corresponded to the STST videos generated for object recognition above. We computed the CKA score between the mixed video and its respective spatial and temporal targets for each model, layer stage and stimulus set.</p></sec><sec id="Sec23"><title>Reporting summary</title><p id="Par70">Further information on research design is available in the <xref rid="MOESM2" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this Article.</p></sec></sec><sec id="Sec24" sec-type="supplementary-material"><title>Supplementary information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="43588_2024_746_MOESM1_ESM.pdf"><label>Supplementary Information</label><caption><p>Supplementary Table 1 and Figs. 1&#x02013;3.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="43588_2024_746_MOESM2_ESM.pdf"><caption><p>Reporting Summary</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM3"><media xlink:href="43588_2024_746_MOESM3_ESM.pdf"><caption><p>Peer Review File</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM4"><media xlink:href="43588_2024_746_MOESM4_ESM.json"><label>Supplementary Data 1</label><caption><p>Dataset for the human psychophysical experiments.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM5"><media xlink:href="43588_2024_746_MOESM5_ESM.mp4"><label>Supplementary Video 1</label><caption><p>Model metamers for object recognition.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM6"><media xlink:href="43588_2024_746_MOESM6_ESM.mp4"><label>Supplementary Video 2</label><caption><p>Spatiotemporally factorized model metamers for stimulus set 1.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM7"><media xlink:href="43588_2024_746_MOESM7_ESM.mp4"><label>Supplementary Video 3</label><caption><p>Spatiotemporally factorized model metamers for stimulus set 2.</p></caption></media></supplementary-material>
</p></sec><sec id="Sec25" sec-type="supplementary-material"><title>Source data</title><p>
<supplementary-material content-type="local-data" id="MOESM8"><media xlink:href="43588_2024_746_MOESM8_ESM.zip"><label>Source Data Fig. 2</label><caption><p>Statistical source data.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM9"><media xlink:href="43588_2024_746_MOESM9_ESM.zip"><label>Source Data Fig. 3</label><caption><p>Statistical source data.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM10"><media xlink:href="43588_2024_746_MOESM10_ESM.zip"><label>Source Data Fig. 4</label><caption><p>Statistical source data.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM11"><media xlink:href="43588_2024_746_MOESM11_ESM.zip"><label>Source Data Fig. 5</label><caption><p>Statistical source data.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM12"><media xlink:href="43588_2024_746_MOESM12_ESM.zip"><label>Source Data Fig. 6</label><caption><p>Statistical source data.</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1038/s43588-024-00746-w.</p></sec><ack><title>Acknowledgements</title><p>This study was supported by the European Research Council (ERC; <ext-link ext-link-type="uri" xlink:href="https://erc.europa.eu/">https://erc.europa.eu/</ext-link>, CoG 864491 to M.S.) and by the German Research Foundation (DFG; <ext-link ext-link-type="uri" xlink:href="https://www.dfg.de/">https://www.dfg.de/</ext-link>, projects 276693517 (SFB 1233; to M.S.) and SI 1332/6-1 (SPP 2041; to M.S.)).</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>A.G. provided conceptualization, software, methodology, investigation, formal analysis, visualization, writing of the original draft, and review and editing. M.S. provided conceptualization, supervision, resources, project administration, funding acquisition, and review and editing of the paper.</p></notes><notes notes-type="peer-review"><title>Peer review</title><sec id="FPar1"><title>Peer review information</title><p id="Par71"><italic>Nature Computational Science</italic> thanks Fangcheng Zhong and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. <xref rid="MOESM3" ref-type="media">Peer reviewer reports</xref> are available. Primary Handling Editor: Fernando Chirigati, in collaboration with the <italic>Nature Computational Science</italic> team.</p></sec></notes><notes notes-type="data-availability"><title>Data availability</title><p>The Kinetics400 dataset is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/cvdfoundation/kinetics-dataset">https://github.com/cvdfoundation/kinetics-dataset</ext-link>. Generated video data are available as Supplementary Videos <xref rid="MOESM5" ref-type="media">1</xref>&#x02013;<xref rid="MOESM7" ref-type="media">3</xref>. Behavioral data for the human experiments are available as Supplementary Data <xref rid="MOESM4" ref-type="media">1</xref>. <xref ref-type="sec" rid="Sec25">Source data</xref> are provided with this paper.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>Code for our algorithm is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/antoninogreco/STST">https://github.com/antoninogreco/STST</ext-link> (ref. <sup><xref ref-type="bibr" rid="CR92">92</xref></sup>).</p></notes><notes id="FPar2" notes-type="COI-statement"><title>Competing interests</title><p id="Par72">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Marr, D. <italic>Vision</italic>: <italic>A Computational Approach</italic> (MIT Press, 1982).</mixed-citation></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>MEG sensor patterns reflect perceptual but not categorical similarity of animate and inanimate objects</article-title><source>Neuroimage</source><year>2019</year><volume>193</volume><fpage>167</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.028</pub-id><pub-id pub-id-type="pmid">30885785</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Proklova, D., Kaiser, D. &#x00026; Peelen, M. V. MEG sensor patterns reflect perceptual but not categorical similarity of animate and inanimate objects. <italic>Neuroimage</italic><bold>193</bold>, 167&#x02013;177 (2019).<pub-id pub-id-type="pmid">30885785</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Stocker</surname><given-names>AA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Noise characteristics and prior expectations in human visual speed perception</article-title><source>Nat. Neurosci.</source><year>2006</year><volume>9</volume><fpage>578</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1038/nn1669</pub-id><pub-id pub-id-type="pmid">16547513</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Stocker, A. A. &#x00026; Simoncelli, E. P. Noise characteristics and prior expectations in human visual speed perception. <italic>Nat. Neurosci.</italic><bold>9</bold>, 578&#x02013;585 (2006).<pub-id pub-id-type="pmid">16547513</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Davies</surname><given-names>AJ</given-names></name><name><surname>Chaplin</surname><given-names>TA</given-names></name><name><surname>Rosa</surname><given-names>MGP</given-names></name><name><surname>Yu</surname><given-names>H-H</given-names></name></person-group><article-title>Natural motion trajectory enhances the coding of speed in primate extrastriate cortex</article-title><source>Sci. Rep.</source><year>2016</year><volume>6</volume><fpage>19739</fpage><pub-id pub-id-type="doi">10.1038/srep19739</pub-id><pub-id pub-id-type="pmid">26813361</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Davies, A. J., Chaplin, T. A., Rosa, M. G. P. &#x00026; Yu, H.-H. Natural motion trajectory enhances the coding of speed in primate extrastriate cortex. <italic>Sci. Rep.</italic><bold>6</bold>, 19739 (2016).<pub-id pub-id-type="pmid">26813361</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>AP</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name></person-group><article-title>A parameterized digital 3D model of the Rhesus macaque face for investigating the visual processing of social cues</article-title><source>J. Neurosci. Methods</source><year>2019</year><volume>324</volume><fpage>108309</fpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.06.001</pub-id><pub-id pub-id-type="pmid">31229584</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Murphy, A. P. &#x00026; Leopold, D. A. A parameterized digital 3D model of the Rhesus macaque face for investigating the visual processing of social cues. <italic>J. Neurosci. Methods</italic><bold>324</bold>, 108309 (2019).<pub-id pub-id-type="pmid">31229584</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Raistrick, A. et al. <italic>Proc. 2023 IEEE</italic>/<italic>CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (IEEE, 2023).</mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Greff, K. et al. <italic>Proc. 2022 IEEE</italic>/<italic>CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (IEEE, 2022).</mixed-citation></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Torralba</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Statistics of natural image categories</article-title><source>Network</source><year>2003</year><volume>14</volume><fpage>391</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_14_3_302</pub-id><pub-id pub-id-type="pmid">12938764</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Torralba, A. &#x00026; Oliva, A. Statistics of natural image categories. <italic>Network</italic><bold>14</bold>, 391&#x02013;412 (2003).<pub-id pub-id-type="pmid">12938764</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">LeCun, Y., Bengio, Y. &#x00026; Hinton, G. Deep learning. <italic>Nature</italic><bold>521</bold>, 436&#x02013;444 (2015).<pub-id pub-id-type="pmid">26017442</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Krizhevsky, A., Sutskever, I. &#x00026; Hinton, G. E. ImageNet classification with deep convolutional neural networks. In <italic>Proc. Advances in Neural Information Processing Systems</italic> (eds Pereira, F. et al.) 1097&#x02013;1105 (Curran Associates, Inc., 2012).</mixed-citation></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Eight open questions in the computational modeling of higher sensory cortex</article-title><source>Curr. Opin. Neurobiol.</source><year>2016</year><volume>37</volume><fpage>114</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2016.02.001</pub-id><pub-id pub-id-type="pmid">26921828</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Yamins, D. L. K. &#x00026; DiCarlo, J. J. Eight open questions in the computational modeling of higher sensory cortex. <italic>Curr. Opin. Neurobiol.</italic><bold>37</bold>, 114&#x02013;120 (2016).<pub-id pub-id-type="pmid">26921828</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Neural population control via deep image synthesis</article-title><source>Science</source><year>2019</year><volume>364</volume><fpage>eaav9436</fpage><pub-id pub-id-type="doi">10.1126/science.aav9436</pub-id><pub-id pub-id-type="pmid">31048462</pub-id>
</element-citation><mixed-citation id="mc-CR12" publication-type="journal">Bashivan, P., Kar, K. &#x00026; DiCarlo, J. J. Neural population control via deep image synthesis. <italic>Science</italic><bold>364</bold>, eaav9436 (2019).<pub-id pub-id-type="pmid">31048462</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Simonyan, K., Vedaldi, A. &#x00026; Zisserman, A. <italic>Proc. 2nd International Conference on Learning Representations</italic> (ICLR, 2014).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Mordvintsev, A., Olah, C. &#x00026; Tyka, M. Inceptionism: going deeper into neural networks. <italic>Google Research Blog</italic><ext-link ext-link-type="uri" xlink:href="http://googleresearch.blogspot.co.uk/2015/06/inceptionism-going-deeper-into-neural.html">http://googleresearch.blogspot.co.uk/2015/06/inceptionism-going-deeper-into-neural.html</ext-link> (2015).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Szegedy, C. et al. <italic>Proc. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (IEEE, 2015).</mixed-citation></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname><given-names>K</given-names></name><name><surname>Roseboom</surname><given-names>W</given-names></name><name><surname>Schwartzman</surname><given-names>DJ</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><article-title>A deep-dream virtual reality platform for studying altered perceptual phenomenology</article-title><source>Sci. Rep.</source><year>2017</year><volume>7</volume><fpage>15982</fpage><pub-id pub-id-type="doi">10.1038/s41598-017-16316-2</pub-id><pub-id pub-id-type="pmid">29167538</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Suzuki, K., Roseboom, W., Schwartzman, D. J. &#x00026; Seth, A. K. A deep-dream virtual reality platform for studying altered perceptual phenomenology. <italic>Sci. Rep.</italic><bold>7</bold>, 15982 (2017).<pub-id pub-id-type="pmid">29167538</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Greco</surname><given-names>A</given-names></name><name><surname>Gallitto</surname><given-names>G</given-names></name><name><surname>D&#x02019;Alessandro</surname><given-names>M</given-names></name><name><surname>Rastelli</surname><given-names>C</given-names></name></person-group><article-title>Increased entropic brain dynamics during deepdream-induced altered perceptual phenomenology</article-title><source>Entropy</source><year>2021</year><volume>23</volume><fpage>839</fpage><pub-id pub-id-type="doi">10.3390/e23070839</pub-id><pub-id pub-id-type="pmid">34208923</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">Greco, A., Gallitto, G., D&#x02019;Alessandro, M. &#x00026; Rastelli, C. Increased entropic brain dynamics during deepdream-induced altered perceptual phenomenology. <italic>Entropy</italic><bold>23</bold>, 839 (2021).<pub-id pub-id-type="pmid">34208923</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Rastelli</surname><given-names>C</given-names></name><name><surname>Greco</surname><given-names>A</given-names></name><name><surname>Kenett</surname><given-names>YN</given-names></name><name><surname>Finocchiaro</surname><given-names>C</given-names></name><name><surname>De Pisapia</surname><given-names>N</given-names></name></person-group><article-title>Simulated visual hallucinations in virtual reality enhance cognitive flexibility</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>4027</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-08047-w</pub-id><pub-id pub-id-type="pmid">35256740</pub-id>
</element-citation><mixed-citation id="mc-CR18" publication-type="journal">Rastelli, C., Greco, A., Kenett, Y. N., Finocchiaro, C. &#x00026; De Pisapia, N. Simulated visual hallucinations in virtual reality enhance cognitive flexibility. <italic>Sci. Rep.</italic><bold>12</bold>, 4027 (2022).<pub-id pub-id-type="pmid">35256740</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>EY</given-names></name><etal/></person-group><article-title>Inception loops discover what excites neurons most using deep predictive models</article-title><source>Nat. Neurosci.</source><year>2019</year><volume>22</volume><fpage>2060</fpage><lpage>2065</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0517-x</pub-id><pub-id pub-id-type="pmid">31686023</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Walker, E. Y. et al. Inception loops discover what excites neurons most using deep predictive models. <italic>Nat. Neurosci.</italic><bold>22</bold>, 2060&#x02013;2065 (2019).<pub-id pub-id-type="pmid">31686023</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>W</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name></person-group><article-title>XDream: finding preferred stimuli for visual neurons using generative networks and gradient-free optimization</article-title><source>PLoS Comput. Biol.</source><year>2020</year><volume>16</volume><fpage>e1007973</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007973</pub-id><pub-id pub-id-type="pmid">32542056</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">Xiao, W. &#x00026; Kreiman, G. XDream: finding preferred stimuli for visual neurons using generative networks and gradient-free optimization. <italic>PLoS Comput. Biol.</italic><bold>16</bold>, e1007973 (2020).<pub-id pub-id-type="pmid">32542056</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Gatys, L. A., Ecker, A. S. &#x00026; Bethge, M. <italic>Proc. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (IEEE, 2016).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Gatys, L., Ecker, A. S. &#x00026; Bethge, M. Texture synthesis using convolutional neural networks. In <italic>Proc. Advances in Neural Information Processing Systems</italic> (eds Cortes, C. et al.) 262&#x02013;270 (Curran Associates, Inc., 2015).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Johnson, J., Alahi, A. &#x00026; Fei-Fei, L. Perceptual losses for real-time style transfer and super-resolution. In <italic>Computer Vision &#x02013; ECCV 2016</italic> (eds Leibe, B. et al.) 694&#x02013;711 (Springer, 2016).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Gatys, L. A., Ecker, A. S., Bethge, M., Hertzmann, A. &#x00026; Shechtman, E. <italic>Proc. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (IEEE, 2017).</mixed-citation></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname><given-names>TS</given-names></name><etal/></person-group><article-title>A parametric texture model based on deep convolutional features closely matches texture appearance for humans</article-title><source>J. Vis.</source><year>2017</year><volume>17</volume><fpage>5</fpage><pub-id pub-id-type="doi">10.1167/17.12.5</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Wallis, T. S. et al. A parametric texture model based on deep convolutional features closely matches texture appearance for humans. <italic>J. Vis.</italic><bold>17</bold>, 5 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Nadler</surname><given-names>EO</given-names></name><etal/></person-group><article-title>Divergences in color perception between deep neural networks and humans</article-title><source>Cognition</source><year>2023</year><volume>241</volume><fpage>105621</fpage><pub-id pub-id-type="doi">10.1016/j.cognition.2023.105621</pub-id><pub-id pub-id-type="pmid">37716312</pub-id>
</element-citation><mixed-citation id="mc-CR26" publication-type="journal">Nadler, E. O. et al. Divergences in color perception between deep neural networks and humans. <italic>Cognition</italic><bold>241</bold>, 105621 (2023).<pub-id pub-id-type="pmid">37716312</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>MH</given-names></name><name><surname>Sanchez Giraldo</surname><given-names>LG</given-names></name><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name></person-group><article-title>Stimulus- and goal-oriented frameworks for understanding natural vision</article-title><source>Nat. Neurosci.</source><year>2019</year><volume>22</volume><fpage>15</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0284-0</pub-id><pub-id pub-id-type="pmid">30531846</pub-id>
</element-citation><mixed-citation id="mc-CR27" publication-type="journal">Turner, M. H., Sanchez Giraldo, L. G., Schwartz, O. &#x00026; Rieke, F. Stimulus- and goal-oriented frameworks for understanding natural vision. <italic>Nat. Neurosci.</italic><bold>22</bold>, 15&#x02013;24 (2019).<pub-id pub-id-type="pmid">30531846</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Pasupathy</surname><given-names>A</given-names></name><name><surname>Kim</surname><given-names>T</given-names></name><name><surname>Popovkina</surname><given-names>DV</given-names></name></person-group><article-title>Object shape and surface properties are jointly encoded in mid-level ventral visual cortex</article-title><source>Curr. Opin. Neurobiol.</source><year>2019</year><volume>58</volume><fpage>199</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.09.009</pub-id><pub-id pub-id-type="pmid">31586749</pub-id>
</element-citation><mixed-citation id="mc-CR28" publication-type="journal">Pasupathy, A., Kim, T. &#x00026; Popovkina, D. V. Object shape and surface properties are jointly encoded in mid-level ventral visual cortex. <italic>Curr. Opin. Neurobiol.</italic><bold>58</bold>, 199&#x02013;208 (2019).<pub-id pub-id-type="pmid">31586749</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Jagadeesh</surname><given-names>AV</given-names></name><name><surname>Gardner</surname><given-names>JL</given-names></name></person-group><article-title>Texture-like representation of objects in human visual cortex</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2022</year><volume>119</volume><fpage>e2115302119</fpage><pub-id pub-id-type="doi">10.1073/pnas.2115302119</pub-id><pub-id pub-id-type="pmid">35439063</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Jagadeesh, A. V. &#x00026; Gardner, J. L. Texture-like representation of objects in human visual cortex. <italic>Proc. Natl Acad. Sci. USA</italic><bold>119</bold>, e2115302119 (2022).<pub-id pub-id-type="pmid">35439063</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Nitzany</surname><given-names>EI</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name></person-group><article-title>The statistics of local motion signals in naturalistic movies</article-title><source>J. Vis.</source><year>2014</year><volume>14</volume><fpage>10</fpage><pub-id pub-id-type="doi">10.1167/14.4.10</pub-id></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Nitzany, E. I. &#x00026; Victor, J. D. The statistics of local motion signals in naturalistic movies. <italic>J. Vis.</italic><bold>14</bold>, 10 (2014).</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Sinno, Z. &#x00026; Bovik, A. C. <italic>Proc. 2019 IEEE International Conference on Image Processing</italic> (<italic>ICIP</italic>) (IEEE, 2019).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Funke, C. M., Gatys, L. A., Ecker, A. S. &#x00026; Bethge, M. Synthesising dynamic textures using convolutional neural networks. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1702.07006v1">https://arxiv.org/abs/1702.07006v1</ext-link> (2017).</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Tesfaldet, M., Brubaker, M. A. &#x00026; Derpanis, K. G. <italic>Proc. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> (IEEE, 2018).</mixed-citation></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>Feather</surname><given-names>J</given-names></name><name><surname>Leclerc</surname><given-names>G</given-names></name><name><surname>M&#x00105;dry</surname><given-names>A</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><article-title>Model metamers reveal divergent invariances between biological and artificial neural networks</article-title><source>Nat. Neurosci.</source><year>2023</year><volume>26</volume><fpage>2017</fpage><lpage>2034</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01442-0</pub-id><pub-id pub-id-type="pmid">37845543</pub-id>
</element-citation><mixed-citation id="mc-CR34" publication-type="journal">Feather, J., Leclerc, G., M&#x00105;dry, A. &#x00026; McDermott, J. H. Model metamers reveal divergent invariances between biological and artificial neural networks. <italic>Nat. Neurosci.</italic><bold>26</bold>, 2017&#x02013;2034 (2023).<pub-id pub-id-type="pmid">37845543</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Simonyan, K. &#x00026; Zisserman, A. Two-stream convolutional networks for action recognition in videos. In <italic>Proc. Advances in Neural Information Processing Systems</italic> (eds Ghahramani, Z. et al.) 568&#x02013;576 (Curran Associates, Inc., 2014).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Feichtenhofer, C., Pinz, A. &#x00026; Wildes, R. <italic>Proc. Advances in Neural Information Processing Systems</italic> (Curran Associates, Inc., 2016).</mixed-citation></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Milner</surname><given-names>AD</given-names></name><name><surname>Jakobson</surname><given-names>LS</given-names></name><name><surname>Carey</surname><given-names>DP</given-names></name></person-group><article-title>A neurological dissociation between perceiving objects and grasping them</article-title><source>Nature</source><year>1991</year><volume>349</volume><fpage>154</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1038/349154a0</pub-id><pub-id pub-id-type="pmid">1986306</pub-id>
</element-citation><mixed-citation id="mc-CR37" publication-type="journal">Goodale, M. A., Milner, A. D., Jakobson, L. S. &#x00026; Carey, D. P. A neurological dissociation between perceiving objects and grasping them. <italic>Nature</italic><bold>349</bold>, 154&#x02013;156 (1991).<pub-id pub-id-type="pmid">1986306</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Milner</surname><given-names>AD</given-names></name></person-group><article-title>Separate visual pathways for perception and action</article-title><source>Trends Neurosci.</source><year>1992</year><volume>15</volume><fpage>20</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(92)90344-8</pub-id><pub-id pub-id-type="pmid">1374953</pub-id>
</element-citation><mixed-citation id="mc-CR38" publication-type="journal">Goodale, M. A. &#x00026; Milner, A. D. Separate visual pathways for perception and action. <italic>Trends Neurosci.</italic><bold>15</bold>, 20&#x02013;25 (1992).<pub-id pub-id-type="pmid">1374953</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VA</given-names></name><name><surname>Sup&#x000e8;r</surname><given-names>H</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name></person-group><article-title>Feedforward, horizontal and feedback processing in the visual cortex</article-title><source>Curr. Opin. Neurobiol.</source><year>1998</year><volume>8</volume><fpage>529</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(98)80042-1</pub-id><pub-id pub-id-type="pmid">9751656</pub-id>
</element-citation><mixed-citation id="mc-CR39" publication-type="journal">Lamme, V. A., Sup&#x000e8;r, H. &#x00026; Spekreijse, H. Feedforward, horizontal and feedback processing in the visual cortex. <italic>Curr. Opin. Neurobiol.</italic><bold>8</bold>, 529&#x02013;535 (1998).<pub-id pub-id-type="pmid">9751656</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><article-title>Hierarchical models of object recognition in cortex</article-title><source>Nat. Neurosci.</source><year>1999</year><volume>2</volume><fpage>1019</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1038/14819</pub-id><pub-id pub-id-type="pmid">10526343</pub-id>
</element-citation><mixed-citation id="mc-CR40" publication-type="journal">Riesenhuber, M. &#x00026; Poggio, T. Hierarchical models of object recognition in cortex. <italic>Nat. Neurosci.</italic><bold>2</bold>, 1019&#x02013;1025 (1999).<pub-id pub-id-type="pmid">10526343</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><etal/></person-group><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2014</year><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id>
</element-citation><mixed-citation id="mc-CR41" publication-type="journal">Yamins, D. L. K. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. <italic>Proc. Natl Acad. Sci. USA</italic><bold>111</bold>, 8619&#x02013;8624 (2014).<pub-id pub-id-type="pmid">24812127</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DL</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nat. Neurosci.</source><year>2016</year><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id><pub-id pub-id-type="pmid">26906502</pub-id>
</element-citation><mixed-citation id="mc-CR42" publication-type="journal">Yamins, D. L. &#x00026; DiCarlo, J. J. Using goal-driven deep learning models to understand sensory cortex. <italic>Nat. Neurosci.</italic><bold>19</bold>, 356&#x02013;365 (2016).<pub-id pub-id-type="pmid">26906502</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Simonyan, K. &#x00026; Zisserman, A. <italic>Proc. 3rd International Conference on Learning Representations</italic> (ICLR, 2015).</mixed-citation></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name><surname>Derpanis</surname><given-names>KGP</given-names></name><name><surname>Wildes</surname><given-names>R</given-names></name></person-group><article-title>Spacetime texture representation and recognition based on a spatiotemporal orientation analysis</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2011</year><volume>34</volume><fpage>1193</fpage><lpage>1205</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2011.221</pub-id></element-citation><mixed-citation id="mc-CR44" publication-type="journal">Derpanis, K. G. P. &#x00026; Wildes, R. Spacetime texture representation and recognition based on a spatiotemporal orientation analysis. <italic>IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>34</bold>, 1193&#x02013;1205 (2011).</mixed-citation></citation-alternatives></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Rudin</surname><given-names>LI</given-names></name><name><surname>Osher</surname><given-names>S</given-names></name><name><surname>Fatemi</surname><given-names>E</given-names></name></person-group><article-title>Nonlinear total variation based noise removal algorithms</article-title><source>Physica D</source><year>1992</year><volume>60</volume><fpage>259</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1016/0167-2789(92)90242-F</pub-id></element-citation><mixed-citation id="mc-CR45" publication-type="journal">Rudin, L. I., Osher, S. &#x00026; Fatemi, E. Nonlinear total variation based noise removal algorithms. <italic>Physica D</italic><bold>60</bold>, 259&#x02013;268 (1992).</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name><surname>Olah</surname><given-names>C</given-names></name><name><surname>Mordvintsev</surname><given-names>A</given-names></name><name><surname>Schubert</surname><given-names>L</given-names></name></person-group><article-title>Feature visualization</article-title><source>Distill</source><year>2017</year><volume>2</volume><fpage>e7</fpage><pub-id pub-id-type="doi">10.23915/distill.00007</pub-id></element-citation><mixed-citation id="mc-CR46" publication-type="journal">Olah, C., Mordvintsev, A. &#x00026; Schubert, L. Feature visualization. <italic>Distill</italic><bold>2</bold>, e7 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Reinhard</surname><given-names>E</given-names></name><name><surname>Adhikhmin</surname><given-names>M</given-names></name><name><surname>Gooch</surname><given-names>B</given-names></name><name><surname>Shirley</surname><given-names>P</given-names></name></person-group><article-title>Color transfer between images</article-title><source>IEEE Comput. Graph. Appl.</source><year>2001</year><volume>21</volume><fpage>34</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1109/38.946629</pub-id></element-citation><mixed-citation id="mc-CR47" publication-type="journal">Reinhard, E., Adhikhmin, M., Gooch, B. &#x00026; Shirley, P. Color transfer between images. <italic>IEEE Comput. Graph. Appl.</italic><bold>21</bold>, 34&#x02013;41 (2001).</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><citation-alternatives><element-citation id="ec-CR48" publication-type="journal"><person-group person-group-type="author"><name><surname>Piti&#x000e9;</surname><given-names>F</given-names></name><name><surname>Kokaram</surname><given-names>AC</given-names></name><name><surname>Dahyot</surname><given-names>R</given-names></name></person-group><article-title>Automated colour grading using colour distribution transfer</article-title><source>Comput. Vis. Image Underst.</source><year>2007</year><volume>107</volume><fpage>123</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1016/j.cviu.2006.11.011</pub-id></element-citation><mixed-citation id="mc-CR48" publication-type="journal">Piti&#x000e9;, F., Kokaram, A. C. &#x00026; Dahyot, R. Automated colour grading using colour distribution transfer. <italic>Comput. Vis. Image Underst.</italic><bold>107</bold>, 123&#x02013;137 (2007).</mixed-citation></citation-alternatives></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Abu-El-Haija, S. et al. YouTube-8M: a large-scale video classification benchmark. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.08675">https://arxiv.org/abs/1609.08675</ext-link> (2016).</mixed-citation></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name><surname>Vinken</surname><given-names>K</given-names></name><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Op De Beeck</surname><given-names>HP</given-names></name></person-group><article-title>Visual categorization of natural movies by rats</article-title><source>J. Neurosci.</source><year>2014</year><volume>34</volume><fpage>10645</fpage><lpage>10658</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3663-13.2014</pub-id><pub-id pub-id-type="pmid">25100598</pub-id>
</element-citation><mixed-citation id="mc-CR50" publication-type="journal">Vinken, K., Vermaercke, B. &#x00026; Op De Beeck, H. P. Visual categorization of natural movies by rats. <italic>J. Neurosci.</italic><bold>34</bold>, 10645&#x02013;10658 (2014).<pub-id pub-id-type="pmid">25100598</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Kay, W. et al. The kinetics human action video dataset. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.06950">https://arxiv.org/abs/1705.06950</ext-link> (2017).</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Zeiler, M. D. &#x00026; Fergus, R. Visualizing and understanding convolutional networks. In <italic>Proc. 13th European Conference on Computer Vision</italic> (eds Fleet, D. et al.) 818&#x02013;833 (Springer, 2014).</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Kornblith, S., Norouzi, M., Lee, H. &#x00026; Hinton, G. <italic>Proc. 36th International Conference on Machine Learning</italic> (PMLR, 2019).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &#x00026; Sun, J. <italic>Proc. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (IEEE, 2016).</mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Liu, Z. et al. <italic>Proc. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (IEEE, 2022).</mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Tran, D. et al. <italic>Proc. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> (IEEE, 2018).</mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Xie, S., Sun, C., Huang, J., Tu, Z. &#x00026; Murphy, K. Rethinking spatiotemporal feature learning: speed-accuracy trade-offs in video classification. In <italic>Proc. European Conference on Computer Vision (ECCV)</italic> (eds Ferrari, V. et al.) 318&#x02013;335 (Springer, 2018).</mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Millidge, B., Seth, A. &#x00026; Buckley, C. L. Predictive coding: a theoretical and experimental review. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2107.12979">https://arxiv.org/abs/2107.12979</ext-link> (2022).</mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">Salvatori, T. et al. Brain-inspired computational intelligence via predictive coding. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2308.07870">https://arxiv.org/abs/2308.07870</ext-link> (2023).</mixed-citation></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="other">Lotter, W., Kreiman, G. &#x00026; Cox, D. <italic>Proc.</italic><italic>International Conference on Learning Representations</italic> (ICLR, 2017).</mixed-citation></ref><ref id="CR61"><label>61.</label><citation-alternatives><element-citation id="ec-CR61" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Dong</surname><given-names>H</given-names></name><name><surname>El Saddik</surname><given-names>A</given-names></name></person-group><article-title>Deep learning in next-frame prediction: a benchmark review</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>69273</fpage><lpage>69283</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.2987281</pub-id></element-citation><mixed-citation id="mc-CR61" publication-type="journal">Zhou, Y., Dong, H. &#x00026; El Saddik, A. Deep learning in next-frame prediction: a benchmark review. <italic>IEEE Access</italic><bold>8</bold>, 69273&#x02013;69283 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR62"><label>62.</label><citation-alternatives><element-citation id="ec-CR62" publication-type="journal"><person-group person-group-type="author"><name><surname>Lotter</surname><given-names>W</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Cox</surname><given-names>D</given-names></name></person-group><article-title>A neural network trained for prediction mimics diverse features of biological neurons and perception</article-title><source>Nat. Mach. Intell.</source><year>2020</year><volume>2</volume><fpage>210</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1038/s42256-020-0170-9</pub-id><pub-id pub-id-type="pmid">34291193</pub-id>
</element-citation><mixed-citation id="mc-CR62" publication-type="journal">Lotter, W., Kreiman, G. &#x00026; Cox, D. A neural network trained for prediction mimics diverse features of biological neurons and perception. <italic>Nat. Mach. Intell.</italic><bold>2</bold>, 210&#x02013;219 (2020).<pub-id pub-id-type="pmid">34291193</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR63"><label>63.</label><citation-alternatives><element-citation id="ec-CR63" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>C</given-names></name><etal/></person-group><article-title>Unsupervised neural network models of the ventral visual stream</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2021</year><volume>118</volume><fpage>e2014196118</fpage><pub-id pub-id-type="doi">10.1073/pnas.2014196118</pub-id><pub-id pub-id-type="pmid">33431673</pub-id>
</element-citation><mixed-citation id="mc-CR63" publication-type="journal">Zhuang, C. et al. Unsupervised neural network models of the ventral visual stream. <italic>Proc. Natl Acad. Sci. USA</italic><bold>118</bold>, e2014196118 (2021).<pub-id pub-id-type="pmid">33431673</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR64"><label>64.</label><mixed-citation publication-type="other">Rane, R. P. Sz&#x000fc;gyi, E., Saxena, V., Ofner, A. &#x00026; Stober, S. <italic>Proc. 2020 International Conference on Multimedia Retrieval</italic> (ACM, 2020).</mixed-citation></ref><ref id="CR65"><label>65.</label><citation-alternatives><element-citation id="ec-CR65" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Bovik</surname><given-names>AC</given-names></name><name><surname>Sheikh</surname><given-names>HR</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Image quality assessment: from error visibility to structural similarity</article-title><source>IEEE Trans. Image Process.</source><year>2004</year><volume>13</volume><fpage>600</fpage><lpage>612</lpage><pub-id pub-id-type="doi">10.1109/TIP.2003.819861</pub-id><pub-id pub-id-type="pmid">15376593</pub-id>
</element-citation><mixed-citation id="mc-CR65" publication-type="journal">Wang, Z., Bovik, A. C., Sheikh, H. R. &#x00026; Simoncelli, E. P. Image quality assessment: from error visibility to structural similarity. <italic>IEEE Trans. Image Process.</italic><bold>13</bold>, 600&#x02013;612 (2004).<pub-id pub-id-type="pmid">15376593</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR66"><label>66.</label><citation-alternatives><element-citation id="ec-CR66" publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Saul</surname><given-names>N</given-names></name><name><surname>Gro&#x000df;berger</surname><given-names>L</given-names></name></person-group><article-title>UMAP: Uniform Manifold Approximation and Projection</article-title><source>J. Open Source Softw.</source><year>2018</year><volume>3</volume><fpage>861</fpage><pub-id pub-id-type="doi">10.21105/joss.00861</pub-id></element-citation><mixed-citation id="mc-CR66" publication-type="journal">McInnes, L., Healy, J., Saul, N. &#x00026; Gro&#x000df;berger, L. UMAP: Uniform Manifold Approximation and Projection. <italic>J. Open Source Softw.</italic><bold>3</bold>, 861 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="other">Huang, H. et al. <italic>Proc. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (IEEE, 2017).</mixed-citation></ref><ref id="CR68"><label>68.</label><citation-alternatives><element-citation id="ec-CR68" publication-type="journal"><person-group person-group-type="author"><name><surname>Ruder</surname><given-names>M</given-names></name><name><surname>Dosovitskiy</surname><given-names>A</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><article-title>Artistic style transfer for videos and spherical images</article-title><source>Int. J. Comput. Vis.</source><year>2018</year><volume>126</volume><fpage>1199</fpage><lpage>1219</lpage><pub-id pub-id-type="doi">10.1007/s11263-018-1089-z</pub-id></element-citation><mixed-citation id="mc-CR68" publication-type="journal">Ruder, M., Dosovitskiy, A. &#x00026; Brox, T. Artistic style transfer for videos and spherical images. <italic>Int. J. Comput. Vis.</italic><bold>126</bold>, 1199&#x02013;1219 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="other">Gao, W., Li, Y., Yin, Y. &#x00026; Yang, M.-H. <italic>Proc. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</italic> (IEEE, 2020).</mixed-citation></ref><ref id="CR70"><label>70.</label><citation-alternatives><element-citation id="ec-CR70" publication-type="journal"><person-group person-group-type="author"><name><surname>Golan</surname><given-names>T</given-names></name><name><surname>Raju</surname><given-names>PC</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Controversial stimuli: pitting neural networks against each other as models of human cognition</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2020</year><volume>117</volume><fpage>29330</fpage><lpage>29337</lpage><pub-id pub-id-type="doi">10.1073/pnas.1912334117</pub-id><pub-id pub-id-type="pmid">33229549</pub-id>
</element-citation><mixed-citation id="mc-CR70" publication-type="journal">Golan, T., Raju, P. C. &#x00026; Kriegeskorte, N. Controversial stimuli: pitting neural networks against each other as models of human cognition. <italic>Proc. Natl Acad. Sci. USA</italic><bold>117</bold>, 29330&#x02013;29337 (2020).<pub-id pub-id-type="pmid">33229549</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR71"><label>71.</label><mixed-citation publication-type="other">Golan, T., Guo, W., Sch&#x000fc;tt, H. H. &#x00026; Kriegeskorte, N. <italic>Proc. SVRHM 2022 Workshop at NeurIPS</italic> (International Conference on Neural Information Processing Systems, 2022); <ext-link ext-link-type="uri" xlink:href="https://neurips.cc/virtual/2022/65923">https://neurips.cc/virtual/2022/65923</ext-link></mixed-citation></ref><ref id="CR72"><label>72.</label><mixed-citation publication-type="other">Gaziv, G., Lee, M. J. &#x00026; DiCarlo, J. J. <italic>Proc. 37th International Conference on Neural Information Processing Systems</italic> (Curran Associates, Inc., 2024).</mixed-citation></ref><ref id="CR73"><label>73.</label><citation-alternatives><element-citation id="ec-CR73" publication-type="journal"><person-group person-group-type="author"><name><surname>Cadieu</surname><given-names>CF</given-names></name><etal/></person-group><article-title>Deep neural networks rival the representation of primate IT cortex for core visual object recognition</article-title><source>PLoS Comput. Biol.</source><year>2014</year><volume>10</volume><fpage>e1003963</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003963</pub-id><pub-id pub-id-type="pmid">25521294</pub-id>
</element-citation><mixed-citation id="mc-CR73" publication-type="journal">Cadieu, C. F. et al. Deep neural networks rival the representation of primate IT cortex for core visual object recognition. <italic>PLoS Comput. Biol.</italic><bold>10</bold>, e1003963 (2014).<pub-id pub-id-type="pmid">25521294</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR74"><label>74.</label><mixed-citation publication-type="other">Vaswani, A. et al. Attention is all you need. In <italic>Proc. Advances in Neural Information Processing Systems</italic> (eds Guyon, I. et al.) 6000&#x02013;6010 (Curran Associates, Inc., 2017).</mixed-citation></ref><ref id="CR75"><label>75.</label><mixed-citation publication-type="other">Dosovitskiy, A. et al. <italic>Proc. International Conference on Learning Representations</italic> (ICLR, 2021).</mixed-citation></ref><ref id="CR76"><label>76.</label><mixed-citation publication-type="other">Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C. &#x00026; Dosovitskiy, A. Do vision transformers see like convolutional neural networks? In <italic>Proc. Advances in Neural Information Processing Systems</italic> (eds Ranzato, M. et al.) 12116&#x02013;12128 (Curran Associates, Inc., 2021).</mixed-citation></ref><ref id="CR77"><label>77.</label><mixed-citation publication-type="other">Bakhtiari, S., Mineault, P., Lillicrap, T., Pack, C. &#x00026; Richards, B. The functional specialization of visual cortex emerges from training parallel pathways with self-supervised predictive learning. In <italic>Proc. Advances in Neural Information Processing Systems</italic> (eds Ranzato, M. et al.) 25164&#x02013;25178 (Curran Associates, Inc., 2021).</mixed-citation></ref><ref id="CR78"><label>78.</label><mixed-citation publication-type="other">Mineault, P., Bakhtiari, S., Richards, B. &#x00026; Pack, C. Your head is there to move you around: goal-driven models of the primate dorsal pathway. In <italic>Proc. Advances in Neural Information Processing Systems</italic> (eds Ranzato, M. e al.) 28757&#x02013;28771 (Curran Associates, Inc., 2021).</mixed-citation></ref><ref id="CR79"><label>79.</label><citation-alternatives><element-citation id="ec-CR79" publication-type="journal"><person-group person-group-type="author"><name><surname>Verri</surname><given-names>A</given-names></name><name><surname>Straforini</surname><given-names>M</given-names></name><name><surname>Torre</surname><given-names>V</given-names></name></person-group><article-title>Computational aspects of motion perception in natural and artificial vision systems</article-title><source>Phil. Trans. R. Soc. B</source><year>1992</year><volume>337</volume><fpage>429</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1098/rstb.1992.0119</pub-id><pub-id pub-id-type="pmid">1359590</pub-id>
</element-citation><mixed-citation id="mc-CR79" publication-type="journal">Verri, A., Straforini, M. &#x00026; Torre, V. Computational aspects of motion perception in natural and artificial vision systems. <italic>Phil. Trans. R. Soc. B</italic><bold>337</bold>, 429&#x02013;443 (1992).<pub-id pub-id-type="pmid">1359590</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR80"><label>80.</label><citation-alternatives><element-citation id="ec-CR80" publication-type="journal"><person-group person-group-type="author"><name><surname>Giese</surname><given-names>MA</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><article-title>Neural mechanisms for the recognition of biological movements</article-title><source>Nat. Rev. Neurosci.</source><year>2003</year><volume>4</volume><fpage>179</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1038/nrn1057</pub-id><pub-id pub-id-type="pmid">12612631</pub-id>
</element-citation><mixed-citation id="mc-CR80" publication-type="journal">Giese, M. A. &#x00026; Poggio, T. Neural mechanisms for the recognition of biological movements. <italic>Nat. Rev. Neurosci.</italic><bold>4</bold>, 179&#x02013;192 (2003).<pub-id pub-id-type="pmid">12612631</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR81"><label>81.</label><mixed-citation publication-type="other">Deng, J. et al. <italic>Proc. 2009 IEEE Conference on Computer Vision and Pattern Recognition</italic> (IEEE, 2009).</mixed-citation></ref><ref id="CR82"><label>82.</label><citation-alternatives><element-citation id="ec-CR82" publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><etal/></person-group><article-title>ImageNet large scale visual recognition challenge</article-title><source>Int. J. Comput. Vis.</source><year>2015</year><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation><mixed-citation id="mc-CR82" publication-type="journal">Russakovsky, O. et al. ImageNet large scale visual recognition challenge. <italic>Int. J. Comput. Vis.</italic><bold>115</bold>, 211&#x02013;252 (2015).</mixed-citation></citation-alternatives></ref><ref id="CR83"><label>83.</label><citation-alternatives><element-citation id="ec-CR83" publication-type="journal"><person-group person-group-type="author"><name><surname>Mordvintsev</surname><given-names>A</given-names></name><name><surname>Pezzotti</surname><given-names>N</given-names></name><name><surname>Schubert</surname><given-names>L</given-names></name><name><surname>Olah</surname><given-names>C</given-names></name></person-group><article-title>Differentiable image parameterizations</article-title><source>Distill</source><year>2018</year><volume>3</volume><fpage>e12</fpage><pub-id pub-id-type="doi">10.23915/distill.00012</pub-id></element-citation><mixed-citation id="mc-CR83" publication-type="journal">Mordvintsev, A., Pezzotti, N., Schubert, L. &#x00026; Olah, C. Differentiable image parameterizations. <italic>Distill</italic><bold>3</bold>, e12 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR84"><label>84.</label><citation-alternatives><element-citation id="ec-CR84" publication-type="journal"><person-group person-group-type="author"><name><surname>Odena</surname><given-names>A</given-names></name><name><surname>Dumoulin</surname><given-names>V</given-names></name><name><surname>Olah</surname><given-names>C</given-names></name></person-group><article-title>Deconvolution and checkerboard artifacts</article-title><source>Distill</source><year>2016</year><volume>1</volume><fpage>e3</fpage><pub-id pub-id-type="doi">10.23915/distill.00003</pub-id></element-citation><mixed-citation id="mc-CR84" publication-type="journal">Odena, A., Dumoulin, V. &#x00026; Olah, C. Deconvolution and checkerboard artifacts. <italic>Distill</italic><bold>1</bold>, e3 (2016).</mixed-citation></citation-alternatives></ref><ref id="CR85"><label>85.</label><mixed-citation publication-type="other">Soomro, K., Zamir, A. R. &#x00026; Shah, M. UCF101: a dataset of 101 human actions classes from videos in the wild. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1212.0402">https://arxiv.org/abs/1212.0402</ext-link> (2012).</mixed-citation></ref><ref id="CR86"><label>86.</label><mixed-citation publication-type="other">Mahendran, A. &#x00026; Vedaldi, A. <italic>Proc. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (IEEE, 2015).</mixed-citation></ref><ref id="CR87"><label>87.</label><citation-alternatives><element-citation id="ec-CR87" publication-type="journal"><person-group person-group-type="author"><name><surname>Coltuc</surname><given-names>D</given-names></name><name><surname>Bolon</surname><given-names>P</given-names></name><name><surname>Chassery</surname><given-names>J-M</given-names></name></person-group><article-title>Exact histogram specification</article-title><source>IEEE Trans. Image Process.</source><year>2006</year><volume>15</volume><fpage>1143</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1109/TIP.2005.864170</pub-id><pub-id pub-id-type="pmid">16671295</pub-id>
</element-citation><mixed-citation id="mc-CR87" publication-type="journal">Coltuc, D., Bolon, P. &#x00026; Chassery, J.-M. Exact histogram specification. <italic>IEEE Trans. Image Process.</italic><bold>15</bold>, 1143&#x02013;1152 (2006).<pub-id pub-id-type="pmid">16671295</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR88"><label>88.</label><mixed-citation publication-type="other">Farneb&#x000e4;ck, G. in <italic>Image Analysis</italic> Vol. 2749 (eds Bigun, J. &#x00026; Gustavsson, T.) 363&#x02013;370 (Springer, 2003).</mixed-citation></ref><ref id="CR89"><label>89.</label><mixed-citation publication-type="other">torchvision: PyTorch&#x02019;s Computer Vision library. <italic>GitHub</italic><ext-link ext-link-type="uri" xlink:href="https://github.com/pytorch/vision">https://github.com/pytorch/vision</ext-link> (2016).</mixed-citation></ref><ref id="CR90"><label>90.</label><citation-alternatives><element-citation id="ec-CR90" publication-type="journal"><person-group person-group-type="author"><name><surname>Geiger</surname><given-names>A</given-names></name><name><surname>Lenz</surname><given-names>P</given-names></name><name><surname>Stiller</surname><given-names>C</given-names></name><name><surname>Urtasun</surname><given-names>R</given-names></name></person-group><article-title>Vision meets robotics: the KITTI dataset</article-title><source>Int. J. Robot. Res.</source><year>2013</year><volume>32</volume><fpage>1231</fpage><lpage>1237</lpage><pub-id pub-id-type="doi">10.1177/0278364913491297</pub-id></element-citation><mixed-citation id="mc-CR90" publication-type="journal">Geiger, A., Lenz, P., Stiller, C. &#x00026; Urtasun, R. Vision meets robotics: the KITTI dataset. <italic>Int. J. Robot. Res.</italic><bold>32</bold>, 1231&#x02013;1237 (2013).</mixed-citation></citation-alternatives></ref><ref id="CR91"><label>91.</label><mixed-citation publication-type="other">Reimers, N. &#x00026; Gurevych, I. Sentence-BERT: sentence embeddings using Siamese BERT-Networks. In <italic>Proc. 2019 Conference on Empirical Methods in Natural Language Processing</italic><italic>and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</italic> (eds Inui, K. et al.) 3982&#x02013;3992 (ACL, 2019).</mixed-citation></ref><ref id="CR92"><label>92.</label><mixed-citation publication-type="other">Greco, A. antoninogreco/STST: STST v1.0. <italic>Zenodo</italic>10.5281/zenodo.14168471 (2024).</mixed-citation></ref></ref-list></back></article>